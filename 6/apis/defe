def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)text)text)text
#regex_quote_left = re.compile('"(?=\w)')
#regex_quote_right = re.compile('(?=\w\.)"')

def latex_escape(text,pound=True):
    text=text.replace('\\','{\\textbackslash}')
    for c in '^_&$%{}': text=text.replace(c,'\\'+c)
    text=text.replace('\\{\\textbackslash\\}','{\\textbackslash}')
    if pound: text=text.replace('#','\\#')
    return text

def render(text,
           extra={},
           allowed={},
           sep='p',
--
    # deal with images, videos, audios and links
    #############################################################

    def sub(x):
        f=image_mapper(x.group('k'))
        if not f: return None
        return '\n\\begin{center}\\includegraphics[width=8cm]{%s}\\end{center}\n' % (f)
--
\\usepackage{graphicx}
\\usepackage{grffile}
\\usepackage[utf8x]{inputenc}
\\definecolor{lg}{rgb}{0.9,0.9,0.9}
\\definecolor{dg}{rgb}{0.3,0.3,0.3}
\\def\\ft{\\small\\tt}
\\lstset{
   basicstyle=\\footnotesize,
   breaklines=true, basicstyle=\\ttfamily\\color{black}\\footnotesize,
--
\\end{document}
"""

def markmin2latex(data, image_mapper=lambda x:x, extra={},
                  wrapper=WRAPPER):
    body, title, authors = render(data, extra=extra, image_mapper=image_mapper)
    author = '\n\\and\n'.join(a.replace('\n','\\\\\n\\footnotesize ') for a in authors)
--
    parser.add_option("-i", "--info", dest="info",
                      help="markmin help")
    parser.add_option("-t", "--test", dest="test", action="store_true",
                      default=False)
    parser.add_option("-n", "--no_wrapper", dest="no_wrapper",
                      action="store_true",default=False)
    parser.add_option("-c", "--chapters", dest="chapters",action="store_true",
                      default=False,help="switch section for chapter")
    parser.add_option("-w", "--wrapper", dest="wrapper", default=False,
                      help="latex file containing header and footer")

    (options, args) = parser.parse_args()
--
# not this:
if d.has_key(key):
    ...do something with d[key]
    def good_append(new_item, a_list=None):
    if a_list is None:
        a_list = []
    a_list.append(new_item)
    return a_list
Filter out blank rows from a CSV reader (or items from a list):

def filter_rows(row_iterator):
    for row in row_iterator:
        if row:
            yield row
--
import glob
import os
import re
from collections import defaultdict

# Note to get more than 10 at a time, would need to learn how to set the cookie that gets set here:
# http://www.scirus.com/srsapp/preferences
--
# docsum_iter = get_iter("<DocSum>.*?</DocSum>", summary_xml)
# for docsum in docsum_iter:
#        record_text = docsum.group()
def get_iter(pattern, text):
    # Then for i in iter, use i.group() as the zone
    docsum_pattern = re.compile(pattern, re.DOTALL)
    docsum_iter = docsum_pattern.finditer(text)
--
year_pattern = Word(nums, exact=4)
citation_pattern = Suppress('<span class=gs_a>') + first_author_initials_pattern("initials") + first_author_lastname_pattern("lastname") + Suppress(SkipTo('- ')) + Suppress('- ') + journal_pattern("journal") + Suppress(',') + year_pattern("year") + Suppress("-")

def get_citations_from_page(page_raw):
    # Replace ampersands
    page_noamps = re.sub(r"&amp;", "&", page_raw)
    # "Remove accents"
--
        items.append(new_items)
    return(items)

def get_citations_from_directories(dir_name=".", key_prefix="", output_filename="output.txt"):
    files = glob.glob(dir_name)
    citations = []
    out_file = open(output_filename, "w")
--
    out_file.close()
    return(citations)

def get_name_of_inner_subdirectory(filename):
    the_dirname = os.path.dirname(filename)
    the_lowest_subdir = os.path.basename(the_dirname)
    return(the_lowest_subdir)

def convert_items_to_lookup_strings(items, keyname="test"):
    lookup_strings = ["|".join([item['journal'], item['year'], "", "", item["lastname"] + " " + item["initials"], keyname]) for item in items]

    #for line in lookup_strings:
--
    return(lookup_strings)

@TimedCache(timeout_in_seconds=60*60*24*7)
def get_list_of_hits(pattern, text):
    items = pattern.searchString(text)
    flat_list = [item for [item] in items.asList()]
    return(flat_list)

@TimedCache(timeout_in_seconds=60*60*24*7)
def get_dict_of_hits(pattern, text):
    default_items = defaultdict(str)
    items = pattern.searchString(text)
    if items:
        for key in items[0].keys():
            default_items[key] = items[0][key]
    else:
        print "\n\nPARSING ERROR:\n" + text
        default_items['id'] = text.split(",")[0]
    return(default_items)

def write_unique_strings(strings, filename):
    unique_strings = list(set(strings))
    fh = open(filename, "w")
    for st in unique_strings:
--
response.subtitle = 'Full Stack Web Framework, 6th Ed (pre-release).\nwritten by Massimo Di Pierro in English'
response.menu = []

def splitter(x):
    a,b = x.split(':',1)
    return a.strip(),b.strip()

def splitter_urlify(x):
    a,b = x.split(':',1)
    return a.strip(),b.strip(), urlify(b)


@cache('folders',None)
def get_folders(dummy=None):
    folder = os.path.join(request.folder,'sources')
    return folder, [f for f in os.listdir(folder)
                    if os.path.isdir(os.path.join(folder,f))]
FOLDER, FOLDERS = get_folders()

def get_subfolder(book_id):
    if not book_id:
        redirect(URL('index'))
    for f in FOLDERS:
--
            return f
    redirect(URL('index'))

def get_info(subfolder):
    infofile = os.path.join(FOLDER,subfolder,'info.txt')
    if os.path.exists(infofile):
        info = dict(splitter(line)
--
        return info
    return {}

def get_chapters(subfolder):
    filename = os.path.join(FOLDER,subfolder,'chapters.txt')
    chapters = [splitter_urlify(line)
                for line in open(filename).readlines()
--
    return chapters

@cache('menu',None)
def build_menu(dummy=None):
    menu = []
    submenu = []
    for subfolder in FOLDERS:
--

response.menu = build_menu()

def convert2html(book_id,text):
    extra = {}
    def url2(*a,**b):
        b['args'] = [book_id]+b.get('args',[])
        return URL(*a,**b)
    def truncate(x): return x[:70]+'...' if len(x)>70 else x
    extra['verbatim'] = lambda code: cgi.escape(code)
    extra['cite'] = lambda key: TAG.sup(
        '[',A(key,_href=URL('reference',args=(book_id,key)),
--
    rtn = MARKMIN(text.replace('\r',''),extra=extra,url=url2)
    return rtn

def index():
    books = {}
    for subfolder in FOLDERS:
        books[subfolder] = cache.ram('info_%s' % subfolder, lambda: get_info(subfolder), time_expire=TIME_EXPIRE)
    return locals()

def calc_date(now=request.utcnow.date()):
    # if you are changing sources often remove the
    # comment from the next 2 lines
    # import datetime
--
    format = '%a, %d %b %Y 23:59:59 GMT'
    return now.strftime(format)

def chapter():
    book_id, chapter_id = request.args(0), request.args(1, cast=int, default=0)
    subfolder = get_subfolder(book_id)
    info = cache.ram('info_%s' % subfolder, lambda: get_info(subfolder), time_expire=TIME_EXPIRE)
    chapters = cache.ram('chapters_%s' % subfolder, lambda: get_chapters(subfolder), time_expire=TIME_EXPIRE)
--
        content = XML(open(dest).read())
        return locals()

def search():
    book_id = request.args(0) or redirect(URL('index'))
    search = request.vars.search or redirect(URL('chapter',args=book_id))
    subfolder = get_subfolder(book_id)
--
                                               vars=dict(search=search),
                                               args=(book_id,chapter[0],chapter[2])),_class="btn"))
                        for chapter in results])
    response.view = 'default/chapter.html'
    return locals()

def image():
    book_id = request.args(0)
    key = request.args(1)
    subfolder = get_subfolder(book_id)
--
    return response.stream(filename)


def reference():
    book_id = request.args(0)
    key = request.args(1)
    subfolder = get_subfolder(book_id)
--
    headers = {'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) '
            'AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.168 Safari/535.19')}

    def translate(self, source, from_lang='en', to_lang='en', host=None, type_=None):
        '''Translate the source text from one language to another.'''
        if PY2:
            source = source.encode('utf-8')
--
        json5 = self._get_json5(url, host=host, type_=type_)
        return self._unescape(self._get_translation_from_json5(json5))

    def detect(self, source, host=None, type_=None):
        '''Detect the source text's language.'''
        if PY2:
            source = source.encode('utf-8')
--
        lang = self._get_language_from_json5(json5)
        return lang

    def _get_language_from_json5(self, content):
        match = self.detection_pattern.match(content)
        if not match:
            return None
        return match.group(1)

    def _get_translation_from_json5(self, content):
        result = ""
        pos = 2
        while True:
--
            pos = m.end()
        return result

    def _get_json5(self, url, host=None, type_=None):
        req = request.Request(url=url, headers=self.headers)
        if host or type_:
            req.set_proxy(host=host, type=type_)
--
        content = r.read()
        return content.decode('utf-8')

    def _unescape(self, text):
        return re.sub(r"\\.?", lambda x:eval('"%s"'%x.group(0)), text)
--
 from PySide.QtCore import *
 from PySide.QtGui import *
  
def index(): 
    app = QApplication(sys.argv)
    button = QPushButton(u"".join("wwww"))
    button.clicked.connect(u"".join("wwwwww"))
--
import os.path
import sys

def create_checksum(path):
    ''' Read a file, and creates a cumulative checksum, line by line. The final
        total checksum is returned.'''

--
    return checksum


def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
       of files.'''

--
    return path_collection


def find_duplicates(path='.'):
    '''Determine duplicate files based on filesize, and checksum. We use a
       dictionary to avoid n*n lookup times.'''
    duplicates = []
--


if __name__ == '__main__':
    # Default to the current directory
    path = '.'
    dup_log = 'duplicates.log'

--
import os
import fnmatch

def gen_find(filepat,top):
    for path, dirlist, filelist in os.walk(top):
        for name in fnmatch.filter(filelist,filepat):
            yield os.path.join(path,name)
--
import re
pat = re.compile(r"(\S+)|(<[^>]*>)")

def scanner(text):
	for m in pat.finditer(text):
		token = m.group(0)
		print("Feeding:", repr(token))
--
text = "<foo> This is a <b> foo file </b> you know. </foo>"
token_stream = scanner(text)

def parse_elem(opening_tag):
	name = opening_tag[1:-1]
	closing_tag = "</%s>" % name
	items = parse_items(closing_tag)
	return (name, items)

def parse_items(closing_tag = None):
	elems = []
	while 1:
		token = next(token_stream)
--
			elems.append(token)
	return elems

def is_opening_tag(token):
	return token.startswith("<") and not token.startswith("</")

tree = parse_items()
--
def step(ext, dirname, names):
    ext = ext.lower()

    for name in names:
--

exten = '.txt'

def step(ext, dirname, names):
    ext = ext.lower()

    for name in names:
--

logname = 'findfiletype.log'

def step((ext, logpath), dirname, names):
    ext = ext.lower()

    for name in names:
--

#get tags

def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#  post  tags

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(0.1)

--
stext = clipboard.get_selection()


def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#s = ','.join(wctext)

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(1.0)

--

options = ['p','td','=']

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[""])



--
#Enter script code
# dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs)
#Enter script code
#dialog.combo_menu(options, title="Choose an option", message="Choose an option", **kwargs)

options = ["something", "something else", "a third thing"]

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=["w"])

choice = [xp for xp in  choice]

--
stext = clipboard.get_selection()
open_wrap = '"'
wwoptions = stext.split()
retCode, choice = dialog.list_menu_multi(wwoptions, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(1.0)

--
import time
import re

def getwords(text, splitchars=' \t|!?.;:"#'):
    #()+=,/[]{}_
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
--

#get tags

def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#  post  tags

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(0.1)

--
 def screenshot(self, id):
       i = model.Screenshot.get(id)
       if i:
           im = Image.open(StringIO(i.image))
--

_a,_f=request.application, request.function
response.menu=[]
response.menu.append(('wproc',_f=='wproc','/%s/default/wproc'%_a))
response.menu.append(('proc',_f=='proc','/%s/default/proc'%_a))
response.menu.append(('net',_f=='net','/%s/default/net'%_a))
response.menu.append(('etc',_f=='etc','/%s/default/etc'%_a))
response.menu.append(('var',_f=='var','/%s/default/var'%_a))
response.menu.append(('logout',False,'/%s/default/logout'%_a))
if not session.authorized: response.menu=[('login',True,'')]

############################################################
### exposed functions
############################################################

def wproc():
    os.system('ag "w" /home/ahmed > applications/sysadmin/cache/conflict.tmp')
    table=TABLE()
    table.components.append(TR(TH('USER'),TH('PID'),TH('%CPU'),TH('%MEM')))
--



def windex():
    redirect(URL('wproc'))
    
def index():
    redirect(URL('proc'))

def logout():
    session.authorized=None
    redirect(URL('index'))

def markup(text):
    text=cgi.escape(text)
    items=re.compile('\W(?P<name>[\w\-]+@[\w\-]+(\.[\w\-]+)+)[^\w\-]').findall(text)
    done={}
--



def proc():
    os.system('ps auxw > applications/sysadmin/cache/ps.tmp')
    table=TABLE()
    table.components.append(TR(TH('USER'),TH('PID'),TH(),TH('%CPU'),TH('%MEM'), 
--
        table.components.append(TR(*items[:12]))
    return dict(table=table)

def wfinger():
    os.system('finger %s > applications/sysadmin/cache/finger.tmp' % request.args[0])
    return dict(finger=open('applications/sysadmin/cache/finger.tmp','r').read())

def finger():
    os.system('finger %s > applications/sysadmin/cache/finger.tmp' % request.args[0])
    return dict(finger=open('applications/sysadmin/cache/finger.tmp','r').read())

def wkill():
    os.system('kill -9 %s' % request.args[0])
    session.flash='process %s killed' % request.args[0]
    redirect(URL('proc'))
def kill():
    os.system('kill -9 %s' % request.args[0])
    session.flash='process %s killed' % request.args[0]
    redirect(URL('proc'))
#def man():
 #   command=request.args[-1]
    #os.system('info %s > applications/sysadmin/cache/man.tmp' % command)
   # man=open('applications/sysadmin/cache/man.tmp','r').read()
  #  return dict(command=command,man=man)

def net():
    os.system('netstat -na > applications/sysadmin/cache/netstat.tmp')
    table=TABLE()
    table.components.append(TR(TH('Local'),TH('Foreign'),TH('State')))
--
    arp=open('applications/sysadmin/cache/arp.tmp','r').read()
    return dict(table=table,ifconfig=ifconfig,arp=arp)

#def whois():
 #   os.system('whois %s > applications/sysadmin/cache/whois.tmp' % request.args[0])
  #  return dict(whois=markup(open('applications/sysadmin/cache/whois.tmp','r').read()))

def etc():
    os.system('users > applications/sysadmin/cache/users.tmp')
    users=open('applications/sysadmin/cache/users.tmp','r').readlines()
    os.system('service --list  > applications/sysadmin/cache/service.tmp')
--
        else: services_on[service]=True
    return dict(users=users, services=services_on)

#def var():
 #   return dict()

#def edit():
 #   return HTML(BODY(H1('not implemented'))).xml()
--
                     ('q', 'cite'),
                     ('script', 'src')])

    def __init__(self, baseuri, encoding, _type):
        _BaseHTMLProcessor.__init__(self, encoding, _type)
        self.baseuri = baseuri

    def resolveURI(self, uri):
        return _makeSafeAbsoluteURI(self.baseuri, uri.strip())

    def unknown_starttag(self, tag, attrs):
        attrs = self.normalize_attrs(attrs)
        attrs = [(key, ((tag, key) in self.relative_uris) and self.resolveURI(value) or value) for key, value in attrs]
        _BaseHTMLProcessor.unknown_starttag(self, tag, attrs)

def _resolveRelativeURIs(htmlSource, baseURI, encoding, _type):
    if not _SGML_AVAILABLE:
        return htmlSource

--
    p.feed(htmlSource)
    return p.output()

def _makeSafeAbsoluteURI(base, rel=None):
    # bail if ACCEPTABLE_URI_SCHEMES is empty
    if not ACCEPTABLE_URI_SCHEMES:
        try:
--
      'cellpadding', 'cellspacing', 'ch', 'challenge', 'char', 'charoff',
      'choff', 'charset', 'checked', 'cite', 'class', 'clear', 'color', 'cols',
      'colspan', 'compact', 'contenteditable', 'controls', 'coords', 'data',
      'datafld', 'datapagesize', 'datasrc', 'datetime', 'default', 'delay',
      'dir', 'disabled', 'draggable', 'dynsrc', 'enctype', 'end', 'face', 'for',
      'form', 'frame', 'galleryimg', 'gutter', 'headers', 'height', 'hidefocus',
      'hidden', 'high', 'href', 'hreflang', 'hspace', 'icon', 'id', 'inputmode',
      'ismap', 'keytype', 'label', 'leftspacing', 'lang', 'list', 'longdesc',
      'loop', 'loopcount', 'loopend', 'loopstart', 'low', 'lowsrc', 'max',
--

    # svgtiny - foreignObject + linearGradient + radialGradient + stop
    svg_elements = set(['a', 'animate', 'animateColor', 'animateMotion',
      'animateTransform', 'circle', 'defs', 'desc', 'ellipse', 'foreignObject',
      'font-face', 'font-face-name', 'font-face-src', 'g', 'glyph', 'hkern',
      'linearGradient', 'line', 'marker', 'metadata', 'missing-glyph', 'mpath',
      'path', 'polygon', 'polyline', 'radialGradient', 'rect', 'set', 'stop',
--



def definite_article(word):
    """ Returns the definite article for a given word.
    """
    return "the"

def indefinite_article(word):
    """ Returns the indefinite article for a given word.
    """
    return "a"

DEFINITE, INDEFINITE = \
    "definite", "indefinite"


#### PLURALIZE ######################################################################################

def pluralize(word, pos=NOUN, custom={}):
    """ Returns the plural of a given word.
    """
    return word + "s"
--
      'cellpadding', 'cellspacing', 'ch', 'challenge', 'char', 'charoff',
      'choff', 'charset', 'checked', 'cite', 'class', 'clear', 'color', 'cols',
      'colspan', 'compact', 'contenteditable', 'controls', 'coords', 'data',
      'datafld', 'datapagesize', 'datasrc', 'datetime', 'default', 'delay',
      'dir', 'disabled', 'draggable', 'dynsrc', 'enctype', 'end', 'face', 'for',
      'form', 'frame', 'galleryimg', 'gutter', 'headers', 'height', 'hidefocus',
      'hidden', 'high', 'href', 'hreflang', 'hspace', 'icon', 'id', 'inputmode',
      'ismap', 'keytype', 'label', 'leftspacing', 'lang', 'list', 'longdesc',
      'loop', 'loopcount', 'loopend', 'loopstart', 'low', 'lowsrc', 'max',
--

    # svgtiny - foreignObject + linearGradient + radialGradient + stop
    svg_elements = set(['a', 'animate', 'animateColor', 'animateMotion',
      'animateTransform', 'circle', 'defs', 'desc', 'ellipse', 'foreignObject',
      'font-face', 'font-face-name', 'font-face-src', 'g', 'glyph', 'hkern',
      'linearGradient', 'line', 'marker', 'metadata', 'missing-glyph', 'mpath',
      'path', 'polygon', 'polyline', 'radialGradient', 'rect', 'set', 'stop',
--
from cache_utils import cache_get, cache_set


def _standford_parser_cmd(format='oneline'):
    parser_dir = os.path.realpath(os.path.join("contrib", "stanford-parser"))
    cmd_parts = ('java', '-mx150m', '-cp', parser_dir + "/*:",
                 'edu.stanford.nlp.parser.lexparser.LexicalizedParser',
--
    return cmd_parts


def _exec_cmd(cmd, input):
    p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=STDOUT)
    response = p.communicate(input=input)[0]
    return response


def parse_coref(sentence, use_cache=True):
    cache_key = "coreferences"
    if use_cache:
        cache_attempt = cache_get(cache_key, sentence)
--
    return matches


def parse_malt(sentence, use_cache=True):
    malt_dir = os.path.realpath(os.path.join('contrib', 'malt-parser'))
    os.environ['MALTPARSERHOME'] = malt_dir
    parser = nltk.parse.malt.MaltParser(working_dir=malt_dir,
--
    return parser.parse(sentence)


def parse_stanford(sentence, use_cache=True):
    return _exec_cmd(_standford_parser_cmd(), sentence)


def parse(sentence, use_cache=True, parser='stanford'):

    cache_key = "parse_trees_{0}".format(parser)
    valid_lines = None
--
    return trees


def dependences(sentence, use_cache=True):
    import re

    if not hasattr(dependences, '_dep_regex'):
--

class DirWalker(object):

    def walk(self,dir,meth):
        """ walks a directory, and executes a callback on each file """
        dir = os.path.abspath(dir)
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
--
import os,sys

class DirWalkr(object):
    def walk(self,dir,meth, fileType=”.xml”):
        “”" walks a directory, and executes a callback on each file “”"
        dir = os.path.abspath(dir)
        for file in [file for file in os.listdir(dir) if not file in [".",".."]]:
--

#http://stackoverflow.com/questions/18938302/remove-duplicate-remove-certain-letters-from-line-if-found

def wdouble(filename):
    new_line = ''
    with open(filename) as fin:
        for line in fin.readlines():
--

import re

def ztokeniseContents(contents):

  # Lower case
  contents = contents.lower()
--



def main():
# If the openshot python code is found in the Python path, then
# we should be able to import openshot cli renderer and call the main() method

--
# The default ``config.py``


def set_prefs(prefs):
    """This function is called before opening the project."""

    # Specify which files and folders to ignore in the project.
--
    # Specifies which files should be considered python files.  It is
    # useful when you have scripts inside your project.  Only files
    # ending with ``.py`` are considered to be python files by
    # default.
    #prefs['python_files'] = ['*.py']

    # Custom source folders:  By default rope searches the project
    # for finding source folders (folders that should be searched
    # for finding modules).  You can add paths to that list.  Note
    # that rope guesses project source folders correctly most of the
--
    prefs['import_dynload_stdmods'] = True

    # If `True` modules with syntax errors are considered to be empty.
    # The default value is `False`; When `False` syntax errors raise
    # `rope.base.exceptions.ModuleSyntaxError` exception.
    prefs['ignore_syntax_errors'] = False

--
    prefs['ignore_bad_imports'] = False


def project_opened(project):
    """This function is called after opening the project."""
    # Do whatever you like here!
--
input="/home/ahmed/Dropbox/DCAR/References/mediationdictionary.pdf"
output="/home/ahmed/tst/pdf.txt"

def convert_pdf_txt(input,output):
    """docstring for convert_pdf_txt"""
    os.system(("ps2ascii %s %s") %( input , output))
    print('Done!')
--
import re
def tokeniseContents(contents):

  # Lower case
  contents = contents.lower()
--
import robot
import PorterStemmer
import time
socket.setdefaulttimeout(10)
# in a true webcrawler you should not use re instead use a DOM parser
import re
# for sorting a nested list
from operator import itemgetter

def get_page(url):
        try:
            f = urllib.urlopen(url)
            page = f.read()
--
        except:
            return ""
        return ""
def get_next_target(page):
        start_link = page.find('<a href=')
        if start_link == -1:
            return None, 0
--
        end_quote = page.find('"', start_quote + 1)
        url = page[start_quote + 1:end_quote]
        return url, end_quote
def union(p,q):
        cnt = 0
        for e in q:
            if e not in p:
                p.append(e)
                cnt += 1
        return cnt
def get_all_links(page):
        links = []
        while True:
            url,endpos = get_next_target(page)
--
            else:
                break
        return links
def add_to_index(index,keyword,url):
        #if keyword in index:
           # if url not in index[keyword]:
              #  index[keyword].append(url)
        index[0] += 1
       # else:
           # index[keyword] = [url]
def split_string(source,splitlist):
    return ''.join([ w if w not in splitlist else ' ' for w in source]).split()
def add_page_to_index_re(index,url,content):
        i = 0
        # it is not a good idea to use regular expression to parse html
        # i did this just to give a quick and dirty result
--
                        add_to_index(index,word,url)

        return i
def format_url(root,page):
        if page[0] == '/':
            return root + page
        return page
def crawl_web(seed,max_pages=10,max_depth=1):
        root = seed
        tocrawl = [seed]
        depth = [0]
--
                        crawled.append(page)
                        print "%s of %s pages sucessfully crawled" % (len(crawled), max_pages)
        return index, graph
def stem(word,p):
        output = ''
        output += p.stem(word, 0,len(word)-1)
        return output

def lookup(index, keyword):
        if keyword in index:
            return index[keyword]
        return None
def sort_by_score(l):
        get_score = itemgetter(0)
        map(get_score,l)
        l = sorted(l,key=get_score)
        l.reverse()
        return l
def lookup_best(index, keyword, ranks):
        result = []
        if keyword in index:
            for url in index[keyword]:
--
        if len(result) > 0:
            result = sort_by_score(result)
        return result
def get_inlinks(page,graph):
        il = {}
        for p in graph:
            for ol in graph[p]:
                if ol == page:
                    il[p] = graph[p]
        return il
def compute_ranks(graph):
        d = 0.8 # damping factor
        numloops = 10
        ranks = {}
--
        import pickle
        GLOBAL_NUM_SEARCHES = 8
        GLOBAL_TRENDING_INTERVAL = 4
        def calculate_trending(trending,now,before,interval,threshhold=0.5):
            for s in now:
                if s in before:
                    slope = float(now[s] - before[s])/interval
--
                        if s in trending:
                            trending.pop(s)
            return trending
        def trending(searches,interval):
            curr_searches = {}
            prev_searches = {}
            is_trending = {}
--
                    prev_searches = curr_searches.copy()
                    curr_searches.clear()
            return is_trending
        def print_cmds():
                return "    Welcome to the CS101 Web Crawler\n" + \
                       "    What do you want to do?\n" + \
                       "    Enter 1 - To start crawling a web page\n" + \
--
                       "    Enter 6 - Delete Index\n" + \
                       "    Enter q - Quit\n" + \
                       "    crawler:"
        def execute_start_crawl(index):
                maxdepth = int(raw_input("    Enter Max Depth:"))
                maxpages = int(raw_input("    Enter Max Pages:"))
                url = raw_input("    Enter Web Url:")
                return crawl_web(url,maxpages,maxdepth)
        def delete_file(path):
            ret = ""
            if os.path.exists(path):
                try:
--
                except:
                    ret += "        Failed to delete {0}\n".format(path)
                print ret
        def clear_data(data,data_str,path):
            ret = ''
            delete_file(path)
            length = len(data)
--
            ret += "        Cleared {0} entries from {1}\n".format(length,data_str)
            print ret
            return data
        def open_file(data,data_str,path):
            ret = ""
            file = open(path,"wb")
            if file:
--
            else:
                ret += "        Failed to open {0} at {1}\n".format(data_str,path)
            print ret
        def load_file(data,data_str,path):
            ret = ""
            if os.path.exists(path):
                file1 = open(path,'rb')
--
                ret += "        {0} does not exist\n".format(path)
            print ret
            return data
        def execute_cmd(c,index,graph, ranks, searches):
                if c == '1':
                    start = time.time()
                    index, graph = execute_start_crawl(index)
--
                    raw_input("    Press Enter")
                    print ""
                return index, graph, ranks, searches
        def main():

                index = {}
                graph = {}
--
import re

class Wiki2Plain:
    def __init__(self, wiki):
        self.wiki = wiki

        self.text = wiki
--
        self.text = self.unwiki(self.text)
        self.text = self.punctuate(self.text)

    def __str__(self):
        return self.text

    def unwiki(self, wiki):
        """
        Remove wiki markup from the text.
        """
--

        return wiki

    def unhtml(self, html):
        """
        Remove HTML from the text.
        """
--

        return html

    def punctuate(self, text):
        """
        Convert every text part into well-formed one-space
        separate paragraph.
--

        return '\n\n'.join(partsParsed)

    def image(self):
        """
        Retrieve the first image in the document.
        """
--
stext = clipboard.get_selection()


def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#s = ','.join(wctext)

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(1.0)

--
import os.path
import sys

def create_checksum(path):
    ''' Read a file, and creates a cumulative checksum, line by line. The final
        total checksum is returned.'''

--
    return checksum


def build_file_paths(path):
    '''Recursively traverse the filesystem, starting at path, and return a full list
       of files.'''

--
    return path_collection


def find_duplicates(path='.'):
    '''Determine duplicate files based on filesize, and checksum. We use a
       dictionary to avoid n*n lookup times.'''
    duplicates = []
--


if __name__ == '__main__':
    # Default to the current directory
    path = '.'
    dup_log = 'duplicates.log'

--
def load_data(data_path='/Users/maxlikely/data/economist/raw/debates'):
    '''

    Loads the economist data as dict of lists of debates. Each debate is a
--
    '''
    import json
    import path
    from collections import defaultdict
    from collections import Counter

    filenames = path.path(data_path).files('*.json')
    new_debates = defaultdict(list)
    for filename in sorted(filenames):
        with open(filename) as f:
            debate = json.load(f)
--
for word in oldlist:
    newlist.append(word.upper())

def iterwords(fh):
    for number, line in enumerate(fh):
        for word in re.split(r'\s+', line.strip()):
            yield number, word


def search(fh, query):
    query = re.split(r'\s+', query.strip().lower())
    matches = []
    words = iterwords(fh)
--

browser = "chromium-browser"

def nameSplit(name):
	(dirName, fileName) = os.path.split(name)
	(fileBaseName, fileExtension)=os.path.splitext(fileName)
	return dirName, fileName, fileBaseName, fileExtension
--
    # These are the HTML tags that we will leave intact
    valid_tags = ('a', 'b', 'i', 'u',)
    tolerate_missing_closing_tags = ('br',)
    from htmlentitydefs import entitydefs # replace entitydefs from sgmllib

    def __init__(self, mode):
        sgmllib.SGMLParser.__init__(self)
        self.result = []
        self.endTagList = []
        self.mode = mode

    def handle_data(self, data):
        self.result.append(data)

    def handle_charref(self, name):
        self.result.append("&#%s;" % name)

    def handle_entityref(self, name):
        x = ';' * self.entitydefs.has_key(name)
        self.result.append("&%s%s" % (name, x))

    def unknown_starttag(self, tag, attrs):
        """ Delete all tags except for legal ones. """
        if self.mode == "removeall":
            if tag in self.valid_tags:
--
                    endTag = '</%s>' % tag
                    self.endTagList.insert(0, endTag)

    def unknown_endtag(self, tag):
        if self.mode == "removeall":
            if tag in self.valid_tags:
                # We don't ensure proper nesting of opening/closing tags
--
                except ValueError:
                    pass

    def cleanup(self):
        """ Append missing closing tags. """
        self.result.extend(self.endTagList)

    def get_html(self):
        html = ' '.join(self.result)
        return html


def handle_pygments(pat, content, pygments_lexer):
    if IS_PYGMENTS:
        data = pat.search(content)
        while data:
--
        content = '[PYGMENTS_ERROR] - %s' % content
    return content

def wparse_content(content, mode="removeall"):
    """ Parse the messages """
    if content.strip():
        parser = wStrippingParser(mode)
--
print("============")
print("")

for Def in dir(policy_rules):
    Fun = getattr(policy_rules, Def)
    if callable(Fun) and Def.startswith("policy_"):
        print("%s: %s" % (Def, getattr(Fun, "__doc__")))
    sys.exit(0)

try:
--
sys.exit(-1)

Founds = []
for Def in dir(policy_rules):
    Fun = getattr(policy_rules, Def)
    if callable(Fun) and Def.startswith("policy_"):
        for RegExp in Fun():
            for Match in RegExp.finditer(FileContent):
                Line = FileContent.count("\n", 0, Match.start(0)) + 1
                Policy = Def.replace("policy_", "").replace("_", " ").capitalize()
                Pattern = RegExp.pattern
                Text = Match.group(0).replace(os.linesep, " ")
                Founds.append((Line, Policy, Pattern, Text))
--


# String functions
def decode_string(v, encoding="utf-8"):
    """ Returns the given value as a Unicode string (if possible).
    """
    if isinstance(encoding, basestring):
--
    return unicode(v)


def encode_string(v, encoding="utf-8"):
    """ Returns the given value as a Python byte string (if possible).
    """
    if isinstance(encoding, basestring):
--
encode_utf8 = encode_string


def isnumeric(strg):
    try:
        float(strg)
    except ValueError:
--

class lazydict(dict):

    def load(self):
        # Must be overridden in a subclass.
        # Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.
        pass

    def _lazy(self, method, *args):
        """ If the dictionary is empty, calls lazydict.load().
            Replaces lazydict.method() with dict.method() and calls it.
        """
--
            setattr(self, method, types.MethodType(getattr(dict, method), self))
        return getattr(dict, method)(self, *args)

    def __repr__(self):
        return self._lazy("__repr__")
    def __len__(self):
        return self._lazy("__len__")
    def __iter__(self):
        return self._lazy("__iter__")
    def __contains__(self, *args):
        return self._lazy("__contains__", *args)
    def __getitem__(self, *args):
        return self._lazy("__getitem__", *args)
    def __setitem__(self, *args):
        return self._lazy("__setitem__", *args)
    def setdefault(self, *args):
        return self._lazy("setdefault", *args)
    def get(self, *args, **kwargs):
        return self._lazy("get", *args)
    def items(self):
        return self._lazy("items")
    def keys(self):
        return self._lazy("keys")
    def values(self):
        return self._lazy("values")
    def update(self, *args):
        return self._lazy("update", *args)
    def pop(self, *args):
        return self._lazy("pop", *args)
    def popitem(self, *args):
        return self._lazy("popitem", *args)

class lazylist(list):

    def load(self):
        # Must be overridden in a subclass.
        # Must load data with list.append(self, v) instead of lazylist.append(v).
        pass

    def _lazy(self, method, *args):
        """ If the list is empty, calls lazylist.load().
            Replaces lazylist.method() with list.method() and calls it.
        """
--
            setattr(self, method, types.MethodType(getattr(list, method), self))
        return getattr(list, method)(self, *args)

    def __repr__(self):
        return self._lazy("__repr__")
    def __len__(self):
        return self._lazy("__len__")
    def __iter__(self):
        return self._lazy("__iter__")
    def __contains__(self, *args):
        return self._lazy("__contains__", *args)
    def insert(self, *args):
        return self._lazy("insert", *args)
    def append(self, *args):
        return self._lazy("append", *args)
    def extend(self, *args):
        return self._lazy("extend", *args)
    def remove(self, *args):
        return self._lazy("remove", *args)
    def pop(self, *args):
        return self._lazy("pop", *args)

#--- UNIVERSAL TAGSET ------------------------------------------------------------------------------
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# However, not all languages are well-suited to Penn Treebank (which was developed for English).
# As more languages are implemented, this is becoming more problematic.
#
--
NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X = \
    "NN", "VB", "JJ", "RB", "PR", "DT", "PP", "PP", "NO", "CJ", "UH", "PT", ".", "X"

def penntreebank2universal(token, tag):
    """ Returns a (token, tag)-tuple with a simplified universal part-of-speech tag.
    """
    if tag.startswith(("NNP-", "NNPS-")):
--

# Handle common abbreviations.
ABBREVIATIONS = abbreviations = set((
    "a.", "adj.", "adv.", "al.", "a.m.", "c.", "cf.", "comp.", "conf.", "def.",
    "ed.", "e.g.", "esp.", "etc.", "ex.", "f.", "fig.", "gen.", "id.", "i.e.",
    "int.", "l.", "m.", "Med.", "Mil.", "Mr.", "n.", "n.q.", "orig.", "pl.",
    "pred.", "pres.", "p.m.", "ref.", "v.", "vs.", "w/"
--
# Handle paragraph line breaks (\n\n marks end of sentence).
EOS = "END-OF-SENTENCE"

def find_tokens(string, punctuation=PUNCTUATION, abbreviations=ABBREVIATIONS, replace=replacements, linebreak=r"\n{2,}"):
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
        Handles common cases of abbreviations (e.g., etc., ...).
        Punctuation marks are split from other words. Periods (or ?!) mark the end of a sentence.
--
# Named entity rules are used to discover proper nouns (NNP's).


def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
        stripping comments and decoding each line to Unicode.
    """
--

class Lexicon(lazydict):

    def __init__(self, path="", morphology=None, context=None, entities=None, NNP="NNP", language=None):
        """ A dictionary of words and their part-of-speech tags.
            For unknown words, rules for word morphology, context and named entities can be used.
        """
--
        self.context    = Context(self, path=context)
        self.entities   = Entities(self, path=entities, tag=NNP)

    def load(self):
        # Arnold NNP x
        dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language


--

class Rules:

    def __init__(self, lexicon={}, cmd={}):
        self.lexicon, self.cmd = lexicon, cmd

    def apply(self, x):
        """ Applies the rule to the given token or list of tokens.
        """
        return x

class Morphology(lazylist, Rules):

    def __init__(self, lexicon={}, path=""):
        """ A list of rules based on word morphology (prefix, suffix).
        """
        cmd = ("char", # Word contains x.
--
        self._path = path

    @property
    def path(self):
        return self._path

    def load(self):
        # ["NN", "s", "fhassuf", "1", "NNS", "x"]
        list.extend(self, (x.split() for x in _read(self._path)))

    def apply(self, token, previous=(None, None), next=(None, None)):
        """ Applies lexical rules to the given token, which is a [word, tag] list.
        """
        w = token[0]
--
                token[1] = pos
        return token

    def insert(self, i, tag, affix, cmd="hassuf", tagged=None):
        """ Inserts a new rule that assigns the given tag to words with the given affix,
            e.g., Morphology.append("RB", "-ly").
        """
--
            r = [affix, cmd.lstrip("f"), tag, "x"]
        lazylist.insert(self, i, r)

    def append(self, *args, **kwargs):
        self.insert(len(self)-1, *args, **kwargs)

    def extend(self, rules=[]):
        for r in rules:
            self.append(*r)

--

class Context(lazylist, Rules):

    def __init__(self, lexicon={}, path=""):
        """ A list of rules based on context (preceding and following words).
        """
        cmd = ("prevtag", # Preceding word is tagged x.
--
        self._path = path

    @property
    def path(self):
        return self._path

    def load(self):
        # ["VBD", "VB", "PREVTAG", "TO"]
        list.extend(self, (x.split() for x in _read(self._path)))

    def apply(self, tokens):
        """ Applies contextual rules to the given list of tokens,
            where each token is a [word, tag] list.
        """
--
                    t[i] = [t[i][0], r[1]]
        return t[len(o):-len(o)]

    def insert(self, i, tag1, tag2, cmd="prevtag", x=None, y=None):
        """ Inserts a new rule that updates words with tag1 to tag2,
            given constraints x and y, e.g., Context.append("TO < NN", "VB")
        """
--
            x, tag1 = tag1.split(" > "); cmd="nexttag"
        lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])

    def append(self, *args, **kwargs):
        self.insert(len(self)-1, *args, **kwargs)

    def extend(self, rules=[]):
        for r in rules:
            self.append(*r)
#--- NAMED ENTITY RECOGNIZER -----------------------------------------------------------------------
--

class Entities(lazydict, Rules):

    def __init__(self, lexicon={}, path="", tag="NNP"):
        """ A dictionary of named entities and their labels.
            For domain names and e-mail adresses, regular expressions are used.
        """
--
        self.tag   = tag

    @property
    def path(self):
        return self._path

    def load(self):
        # ["Alexander", "the", "Great", "PERS"]
        # {"alexander": [["alexander", "the", "great", "pers"], ...]}
        for x in _read(self.path):
            x = [x.lower() for x in x.split()]
            dict.setdefault(self, x[0], []).append(x)

    def apply(self, tokens):
        """ Applies the named entity recognizer to the given list of tokens,
            where each token is a [word, tag] list.
        """
--
            i += 1
        return tokens

    def append(self, entity, name="pers"):
        """ Appends a named entity to the lexicon,
            e.g., Entities.append("Hooloovoo", "PERS")
        """
        e = [s.lower() for s in entity.split(" ") + [name]]
        self.setdefault(e[0], []).append(e)

    def extend(self, entities):
        for entity, name in entities:
            self.append(entity, name)

--

RE_SYNSET = re.compile(r"^[acdnrv][-_][0-9]+$")

def avg(list):
    return sum(list) / float(len(list) or 1)

class Score(tuple):

    def __new__(self, polarity, subjectivity, assessments=[]):
        """ A (polarity, subjectivity)-tuple with an assessments property.
        """
        return tuple.__new__(self, [polarity, subjectivity])

    def __init__(self, polarity, subjectivity, assessments=[]):
        self.assessments = assessments

class Sentiment(lazydict):

    def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
        """ A dictionary of words (adjectives) and polarity scores (positive/negative).
            The value for each word is a dictionary of part-of-speech tags.
            The value for each word POS-tag is a tuple with values for
--
        self.modifier    = kwargs.get("modifier" , lambda w: w.endswith("ly"))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language

    @property
    def confidence(self):
        return self._confidence

    def load(self, path=None):
        """ Loads the XML-file (with sentiment annotations) from the given path.
            By default, Sentiment.path is lazily loaded.
        """
        # <word form="great" wordnet_id="a-01123879" pos="JJ" polarity="1.0" subjectivity="1.0" intensity="1.0" />
        # <word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" />
--
                )
                psi = (float(p), float(s), float(i))
                if w:
                    words.setdefault(w, {}).setdefault(pos, []).append(psi)
                if w and label:
                    labels[w] = label
                if synset:
                    synsets.setdefault(synset, []).append(psi)
        self._language = xml.attrib.get("language", self._language)
        # Average scores of all word senses per part-of-speech tag.
        for w in words:
--
        dict.update(self.labeler, labels)
        dict.update(self._synsets, synsets)

    def synset(self, id, pos=ADJECTIVE):
        """ Returns a (polarity, subjectivity)-tuple for the given synset id.
            For example, the adjective "horrible" has id 193480 in WordNet:
            Sentiment.synset(193480, pos="JJ") => (-0.6, 1.0, 1.0).
--
            self.load()
        return tuple(self._synsets.get(id, (0.0, 0.0))[:2])

    def __call__(self, s, negation=True, **kwargs):
        """ Returns a (polarity, subjectivity)-tuple for the given sentence,
            with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0.
            The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector.
            An optional weight parameter can be given,
            as a function that takes a list of words and returns a weight.
        """
        def avg(assessments, weighted=lambda w: 1):
            s, n = 0, 0
            for words, score in assessments:
                w = weighted(words)
--
        # to stop assessments() from scanning for preceding negation & modifiers.
        elif hasattr(s, "terms"):
            a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation)
            kwargs.setdefault("weight", lambda w: s.terms[w[0]])
        # A dict of (word, weight)-items.
        elif isinstance(s, dict):
            a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation)
            kwargs.setdefault("weight", lambda w: s[w[0]])
        # A list of words.
        elif isinstance(s, list):
            a = self.assessments(((w, None) for w in s), negation)
--
                 subjectivity = avg([(w, s) for w, p, s, x in a], weight),
                  assessments = a)

    def assessments(self, words=[], negation=True):
        """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
            where chunk is a list of successive words: a known word optionally
            preceded by a modifier ("very good") or a negation ("not good").
--
            a[i] = (w, p * -0.5 if n < 0 else p, s, x)
        return a

    def annotate(self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None):
        """ Annotates the given word with polarity, subjectivity and intensity scores,
            and optionally a semantic label (e.g., MOOD for emoticons, IRONY for "(!)").
        """
        w = self.setdefault(word, {})
        w[pos] = w[None] = (polarity, subjectivity, intensity)
        if label:
            self.labeler[word] = label
--
# Unknown words are recognized as numbers if they contain only digits and -,.:/%$
CD = re.compile(r"^[0-9\-\,\.\:\/\%\$]+$")

def _suffix_rules(token, **kwargs):
    """ Default morphological tagging rules for English, based on word suffixes.
    """
    word, pos = token
    if word.endswith("ing"):
--
        pos = "VBP"
    return [word, pos]

def find_tags(tokens, lexicon={}, default=("NN", "NNP", "CD"), language="en", map=None, **kwargs):
    """ Returns a list of [token, tag]-items for the given list of tokens:
        ["The", "cat", "purs"] => [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
        Words are tagged using the given lexicon of (word, tag)-items.
        Unknown words are tagged NN by default.
        Unknown words that start with a capital letter are tagged NNP (unless language="de").
        Unknown words that consist only of digits and punctuation marks are tagged CD.
        Unknown words are then improved with morphological rules.
--
            and token[0].isupper() \
            and token[0].isalpha() \
            and language != "de":
                tagged[i] = [token, default[1]] # NNP
            elif CD.match(token) is not None:
                tagged[i] = [token, default[2]] # CD
            else:
                tagged[i] = [token, default[0]] # NN
                tagged[i] = f(tagged[i],
                    previous = i > 0 and tagged[i-1] or (None, None),
                        next = i < len(tagged)-1 and tagged[i+1] or (None, None))
--
        tagged = lexicon.context.apply(tagged)
        tagged = lexicon.entities.apply(tagged)
    if map is not None:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    return tagged

#--- PHRASE CHUNKER --------------------------------------------------------------------------------
--
CHUNKS[0].insert(1, CHUNKS[0].pop(3))
CHUNKS[1].insert(1, CHUNKS[1].pop(3))

def find_chunks(tagged, language="en"):
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
        The/DT nice/JJ fish/NN is/VBZ dead/JJ ./. =>
--
                chunked[i+1][2] = "B-NP"
    return chunked

def find_prepositions(chunked):
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        PP-chunks followed by NP-chunks make up a PNP-chunk.
--
# - the phrases are labeled: SBJ (subject), OBJ (object), LOC (location), ...
# - the phrase start is marked: B (begin), I (inside), O (outside),
# - the past tense "sat" is lemmatized => "sit".
# By default, the English parser uses the Penn Treebank II tagset:
# http://www.clips.ua.ac.be/pages/penn-treebank-tagset
PTB = PENN = "penn"

class Parser:

    def __init__(self, lexicon={}, default=("NN", "NNP", "CD"), language=None):
        """ A simple shallow parser using a Brill-based part-of-speech tagger.
            The given lexicon is a dictionary of known words and their part-of-speech tag.
            The given default tags are used for unknown words.
            Unknown words that start with a capital letter are tagged NNP (except for German).
            Unknown words that contain only digits and punctuation are tagged CD.
            The given language can be used to discern between
            Germanic and Romance languages for phrase chunking.
        """
        self.lexicon  = lexicon
        self.default  = default
        self.language = language

    def find_tokens(self, string, **kwargs):
        """ Returns a list of sentences from the given string.
            Punctuation marks are separated from each word by a space.
        """
--
                    replace = kwargs.get(      "replace", replacements),
                  linebreak = r"\n{2,}")

    def find_tags(self, tokens, **kwargs):
        """ Annotates the given list of tokens with part-of-speech tags.
            Returns a list of tokens, where each token is now a [word, tag]-list.
        """
--
        return find_tags(tokens,
                   language = kwargs.get("language", self.language),
                    lexicon = kwargs.get( "lexicon", self.lexicon),
                    default = kwargs.get( "default", self.default),
                        map = kwargs.get(     "map", None))

    def find_chunks(self, tokens, **kwargs):
        """ Annotates the given list of tokens with chunk tags.
            Several tags can be added, for example chunk + preposition tags.
        """
--
               find_chunks(tokens,
                   language = kwargs.get("language", self.language)))

    def find_prepositions(self, tokens, **kwargs):
        """ Annotates the given list of tokens with prepositional noun phrase tags.
        """
        return find_prepositions(tokens) # See also Parser.find_chunks().

    def find_labels(self, tokens, **kwargs):
        """ Annotates the given list of tokens with verb/predicate tags.
        """
        return find_relations(tokens)

    def find_lemmata(self, tokens, **kwargs):
        """ Annotates the given list of tokens with word lemmata.
        """
        return [token + [token[0].lower()] for token in tokens]

    def parse(self, s, tokenize=True, tags=True, chunks=True, relations=False, lemmata=False, encoding="utf-8", **kwargs):
        """ Takes a string (sentences) and returns a tagged Unicode string (TaggedString).
            Sentences in the output are separated by newlines.
            With tokenize=True, punctuation is split from words and sentences are separated by \n.
--

class TaggedString(unicode):

    def __new__(self, string, tags=["word"], language=None):
        """ Unicode string with tags and language attributes.
            For example: TaggedString("cat/NN/NP", tags=["word", "pos", "chunk"]).
        """
--
        s.language = language
        return s

    def split(self, sep=TOKENS):
        """ Returns a list of sentences, where each sentence is a list of tokens,
            where each token is a list of word + tags.
        """
--

class Spelling(lazydict):

    ALPHA = "abcdefghijklmnopqrstuvwxyz"

    def __init__(self, path=""):
        self._path = path

    def load(self):
        for x in _read(self._path):
            x = x.split()
            dict.__setitem__(self, x[0], int(x[1]))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language

    @classmethod
    def train(self, s, path="spelling.txt"):
        """ Counts the words in the given string and saves the probabilities at the given path.
            This can be used to generate a new model for the Spelling() constructor.
        """
--
        f.write(model)
        f.close()

    def _edit1(self, w):
        """ Returns a set of words with edit distance 1 from the given word.
        """
        # Of all spelling errors, 80% is covered by edit distance 1.
--
        )
        return set(delete + transpose + replace + insert)

    def _edit2(self, w):
        """ Returns a set of words with edit distance 2 from the given word
        """
        # Of all spelling errors, 99% is covered by edit distance 2.
        # Only keep candidates that are actually known words (20% speedup).
        return set(e2 for e1 in self._edit1(w) for e2 in self._edit1(e1) if e2 in self)

    def _known(self, words=[]):
        """ Returns the given list of words filtered by known words.
        """
        return set(w for w in words if w in self)

    def suggest(self, w):
        """ Return a list of (word, confidence) spelling corrections for the given word,
            based on the probability of known words with edit distance 1-2 from the given word.
        """
--

import os

def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def create_module(name):
  print 'creating module ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    f.write('# Filename : ' + name + '\n')
    f.write("'''Class comment to be completed.'''\n")
    f.write('\n')
    f.write('def __init__():\n')
    f.write('\n')
    f.write('def __str__():\n')
    f.write('\n')
  finally:
    if f != None:
--



def iterwords(fh):
    for number, line in enumerate(fh):
        for word in re.split(r'\s+', line.strip()):
            yield number, word
--
import re


def iterwords(fh):
    for number, line in enumerate(fh):
        for word in re.split(r'\s+', line.strip()):
            # Preprocess the words here, for example to strip out punctuation
--



def search(fh, query):
    query = re.split(r'\s+', query.strip().lower())
    matches = []
    words = iterwords(fh)
--
import os, fnmatch


def find_files(directory, pattern):
    for root, dirs, files in os.walk(directory):
        for basename in files:
            if fnmatch.fnmatch(basename, pattern):
--
def first2lines(): return str(('#' * 80) + '\n' + '#' + (' ' * 78) + '#') # first two lines
def leftSide(): return '# '                                               # left side of border
def rightSide(): return ' #'                                              # right side of border
def last2lines(): return str('#' + (' ' * 78) + '#' + '\n' + ('#' * 80))  # last two lines

# user entered comment goes here
uec = "This program will neatly format a programming comment block so that it's surrounded by pound signs (#). It does this by splitting the comment into a list and then concatenating strings each no longer than 76 characters long including the correct amount of right side space padding."
--

import sys

def compareFiles(filename1, filename2, ignorecase):
  """
  Given two filenames and an ignorecase booelean, compares filename1
  against filename2 and returns list of the differences and a count of
--

  return results, diffcount

def main():
  if (len(sys.argv) < 3) or (len(sys.argv) > 4):
    print 'usage: ./audit.py filename1 filename2 [--ignorecase]'
    sys.exit(1)
--

import os

def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def create_module(name):
  print 'creating module ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    f.write('# Filename : ' + name + '\n')
    f.write("'''Class comment to be completed.'''\n")
    f.write('\n')
    f.write('def __init__():\n')
    f.write('\n')
    f.write('def __str__():\n')
    f.write('\n')
  finally:
    if f != None:
--
PUNCTUATION_REGEX = re.compile('[{0}]'.format(re.escape(string.punctuation)))


def strip_punc(s, all=False):
    '''Removes punctuation from a string.

    :param s: The string.
--
        return s.strip().strip(string.punctuation)


def lowerstrip(s, all=False):
    '''Makes text all lowercase and strips punctuation and whitespace.

    :param s: The string.
--
    return strip_punc(s.lower().strip(), all=all)


def tree2str(tree, concat=' '):
    '''Convert a nltk.tree.Tree to a string.

    For example:
--
    return concat.join([word for (word, tag) in tree])


def filter_insignificant(chunk, tag_suffixes=('DT', 'CC', 'PRP$', 'PRP')):
    '''Filter out insignificant (word, tag) tuples from a chunk of text.'''
    good = []
    for word, tag in chunk:
--

import re, collections

def words(text): return re.findall('[a-z]+', text.lower())

def train(features):
    model = collections.defaultdict(lambda: 1)
    for f in features:
        model[f] += 1
    return model

NWORDS = train(words(file('big.txt').read()))

alphabet = 'abcdefghijklmnopqrstuvwxyz'

def edits1(word):
   s = [(word[:i], word[i:]) for i in range(len(word) + 1)]
   deletes    = [a + b[1:] for a, b in s if b]
   transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]
--
   inserts    = [a + c + b     for a, b in s for c in alphabet]
   return set(deletes + transposes + replaces + inserts)

def known_edits2(word):
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)

def known(words): return set(w for w in words if w in NWORDS)

def correct(word):
    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]
    return max(candidates, key=NWORDS.get)

################ Testing code from here on ################

def spelltest(tests, bias=None, verbose=False):
    import time
    n, bad, unknown, start = 0, 0, 0, time.clock()
    if bias:
--
'consider': 'concider', 'considerable': 'conciderable', 'contented':
'contenpted contende contended contentid', 'curtains':
'cartains certans courtens cuaritains curtans curtians curtions', 'decide': 'descide', 'decided':
'descided', 'definitely': 'definately difinately', 'definition': 'defenition',
'definitions': 'defenitions', 'description': 'discription', 'desiccate':
'desicate dessicate dessiccate', 'diagrammatically': 'diagrammaticaally',
'different': 'diffrent', 'driven': 'dirven', 'ecstasy': 'exstacy ecstacy',
'embarrass': 'embaras embarass', 'establishing': 'astablishing establising',
--
                     'what', 'which', 'who', 'whom','when', 'where', 'why', 'how',
                     # demonstrative pronouns
                     'this', 'that', 'these', 'those',
                     # definite and indefinite article
                     'a', 'an', 'the',
                     # coordinating conjunctions
                     'and', 'but','or', 'so', 'yet', 'nor',
--
input="/home/ahmed/Dropbox/DCAR/References/mediationdictionary.pdf"
output="/home/ahmed/tst/pdf.txt"

def convert_pdf_txt(input,output):
    """docstring for convert_pdf_txt"""
    os.system(("ps2ascii %s %s") %( input , output))
    print('Done!')
--

import re

def ztokeniseContents(contents):

  # Lower case
  contents = contents.lower()
--
stext = clipboard.get_selection()


def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#s = ','.join(wctext)

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(1.0)

--
def getVocabList():
  '''reads the fixed vocabulary list in vocab.txt and returns a dictionary mapping strings to integers'''

  # Read the fixed vocabulary list
--

eliza_chatbot = Chat(pairs, reflections)

def eliza_chat():
    print("Therapist\n---------")
    print("Talk to the program by typing in plain English, using normal upper-")
    print('and lower-case letters and punctuation.  Enter "quit" when done.')
--

    eliza_chatbot.converse()

def demo():
    eliza_chat()

if __name__ == "__main__":
--
                        "107 Chesapeake Blvd Elkton, MD 21921-6313, US",
                        ]

def coordinate(address_list):
    if len(address_list) > 25:
        print "25 records maximum per request"
        raise
--
import re, collections, sys
#This file uses code from http://norvig.com/spell-correct.html

def words(text): return re.findall('[a-z]+', text.lower())
  #It returns all the words from a text

def train(features):
  #It returns two dictionaries with the counts of words
  #and bigrams contained in the file features
    model = collections.defaultdict(lambda: 1)
    model2 = collections.defaultdict(lambda: 1)
    for f in features:
        model[f] += 1
    for i in range(len(features) - 1):
--

class correct2:

  def __init__(self, SET = '0'):
    # It creates lexicon (list of right words)
    # It creates a list of special words depending on the set
    # It creates a list of extra words
--



    self.alphabet = 'abcdefghijklmnopqrstuvwxyz'
    self.cache = {}       # Main cache
    self.caches = {}      # Cache of words corrected to special words
    self.cache2 = {}      # Cache of words corrected using bigrams
--
      self.lex[i+1] = [word for word in self.lexicon if len(word) == (i+1) ]
    self.lex[0] = [word for word in self.lexicon if len(word) >= 20 ]

  def in_lexicon( self, word ):
    #It checks if a word is a right word (a not mispelled word).
    i = len(word)
    if i in self.lex: return word in self.lex[i]
    else:             return word in self.lex[0]

  def edits1(self,word):
   # It returns a set with words that have an edit distance of 1 with the 'word'
   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]
   deletes    = [a + b[1:] for a, b in splits if b]
--
   a = set(deletes + transposes + replaces + inserts)
   return a

  def check_deletion(self,word, candidate):
      # It checks if a right word was eliminated to produce candidate.
      # If it was, the eliminate word is included to be returned.
      # For example, if word is tovinegar and candidate is vinegar,
--
      return candidate


  def known_edits2(self,word):
    # It returns two sets of right words. The words of one set have edit distance
    # of 2 with the 'word' and the words of the other set have edit distance of 1.
    # The original word is also included in both sets.
--

    return (a.union(c)).union( set([word]) ), c.union( set([word]) )

  def known(self,words): return set(w for w in words if w in self.small_lexicon or self.in_lexicon( w ))
    # It returns a set of right words that have edit distance of 1 with the 'word'


  def correct_2(self,word, wordp = "", wordn = "", type = 1):
    # It returns a word after mispelling errors are corrected.
    # word is the word to correct.
    # wordp is the word written before 'word'
--
                                                       # so possible word is the same as the original word.


  def correct(self,word, wordp = "", wordn = ""):
    if self.SET == 10 and word.isdigit():
      return word

--
  <content><![CDATA[
%s
]]></content>
  <!-- Optional: Set a tabTrigger to define how to trigger the snippet -->
  <tabTrigger>%s</tabTrigger>
  <description>%s</description>
  <!-- Optional: Set a scope to limit where the snippet will trigger -->
--
</snippet>"""

class MakeSnippetCommand(sublime_plugin.TextCommand):
    def run(self, edit):
        self.snippet_text = "\n".join([self.view.substr(i) for i in self.view.sel()])
        self.view.window().show_input_panel('Trigger', '', self.set_trigger, None, None)

    def set_trigger(self, trigger):
        self.trigger = trigger
        self.view.window().show_input_panel('Description', '', self.set_description, None, None)

    def set_description(self, description):
        self.description = description
        scopes = self.view.scope_name(self.view.sel()[0].begin()).strip().replace(' ', ', ')
        self.view.window().show_input_panel('Scope', scopes, self.set_scopes, None, None)

    def set_scopes(self, scopes):
        self.scopes = scopes
        self.ask_file_name()

    def ask_file_name(self):
        self.view.window().show_input_panel('File Name', self.trigger + '.sublime-snippet', self.make_snippet, None, None)

    def make_snippet(self, file_name):
        if re.match('^\w+\.sublime\-snippet$', file_name):
            file_path = os.path.join(sublime.packages_path(), 'User', file_name)

--
            self.ask_file_name()

class EditSnippetCommand(sublime_plugin.WindowCommand):
    def run(self):
        snippets = [
            [os.path.basename(filepath), filepath]
                for filepath
                    in iglob(os.path.join(sublime.packages_path(), 'User', '*.sublime-snippet'))]

        def on_done(index):
            if index >= 0:
                self.window.open_file(snippets[index][1])
            else:
--
                if self.window.get_view_index(view)[1] == -1:
                    view.close()

        def on_highlight(index):
            if index >= 0:
                self.window.open_file(snippets[index][1], sublime.TRANSIENT)

--
#! /usr/bin/python

def print_dict(dictionary, ident = '', braces=1):
    """ Recursively prints nested dictionaries."""

    for key, value in dictionary.iteritems():
--
import time
import re

def getwords(text, splitchars=' \t|!?.;:"#'):
    #()+=,/[]{}_
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
--
           if len(line.strip()) > 1] for essay in range(1, 21)]


def essay_sent(num, sent=0):
    return word_tokenize(essay_sentences(num)[sent])


def essay_sent_tags(num, sent=0):
    return [tag for word, tag in pos_tag(essay_sent(num, sent))]


def essay_text(num):
    return " ".join(essays[(num - 1)])


def essay_sentences(num):
    essay = essays[int(num)]
    text = "\n".join(essay)
    text.replace("\n", ".")
--
import shutil
import re

def usage():
    print """
Usage:
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))

def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
--
import os

def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    If 'subdir' is True, recursively access subdirectories under 'dir_name'.
    Additional arguments, if any, are file extensions to add to the list.
--
            fileList += dir_list(dirfile, subdir, *args)
    return fileList

def combine_files(fileList, fn):
    f = open(fn, 'w')
    for file in fileList:
        print 'Writing file %s' % file
--
import os

def recursive(path):
	"""Move through all files, directories, and subdirectories of a path"""
	yield path
	for name in os.listdir(path):
--
			yield fullpath


def nonrecursive(path):
	"""Move through all files, directories, and subdirectories of a path"""
	paths = [path]
	while paths:
--
import collections, re
from util import *

def feature6(**kwargs):
	# This function searches if an essay of a given set has the answers needed
	# for a high score using regular expressions.

--
structure of the essay. The functions are mostly called by the
function pre_process_struc in file se_procedure.py.
Functions names:
def label_section_sents(text, section_name, section_names, section_labels, section_index_first, section_index_last):
def count_this_heading(section_name, headings, text):
def find_last_section_para_index(section_name, heading_index_first, text):
def find_first_section_para_index(section_name, first_heading, text):
def find_first_intro_para_index(section_name, first_heading, text, headings):
def find_no_intro_heading_indices(text, headings):
def find_first_concl_para_index(text,result_last,headings,nf,nf2,dev):
def find_no_concl_heading_indices(section_name, headings, text,nf,nf2, dev):
def find_title_indices(index, text): # 'index' is the first paragraph of the introduction
def find_section_paras(text, section_name, headings, nf,nf2,dev):
def pre_process_sent(sent):
def get_headings(text)
def match_sentence_2_heading(sent, headings)
def get_more_headings_using_contents(text, headings)
def table_entries_recheck1(text)
def table_entries_recheck2(text)
def find_and_label_headings(text,ass_q_long_words):
def find_and_label_heading(text,sent, proc_sent, sent1, counter_s, para, para2, previouspara, nextpara, ass_q_long_words):
def find_and_label_section_sents(text50,headings,nf,nf2,dev):
"""


--
# Returns the full essay text having labelled each introduction (summary, conclusion...) sentence with its structural ID
# Does this by labelling every paragraph between and including the first section_name para and the last section_name para
# as already found.
def label_section_sents(text, section_name, section_names, section_labels, section_index_first, section_index_last):
    x = section_names.index(section_name) # Get the position of this section name in the section_names list
    label = section_labels[x] # so you can find its label in the section_labels list
    if section_index_first == [] and section_index_last == []:
--
# Go through every heading and see if contains the case-insensitive version of section_name.
# Return a list containing the found headings and their indices.
# Called by find_no_concl_heading_indices and other functions in this file.
def count_this_heading(section_name, headings, text):
    mylist = []
    mylabels = ['#-s:s#','#-s:d#','#-s:l#']
    p = re.compile(section_name, re.IGNORECASE)
--
# The para that precedes that next heading is assumed to be the last para of the section.
# Assumes that the sections (Intro, Concl, Summary, Preface) don't have headings inside them, which is not true in every case.
# Called by several functions in this file.
def find_last_section_para_index(section_name, heading_index_first, text):
    counter_p = heading_index_first + 1 # Get the paragraph following the para that is the heading
    position1 = heading_index_first + 1 # position1 is used to ensure we only get the first eligible paragraph returned as the result.
    section_index_last = []
--
# Function: find_first_section_para_index(section_name, first_heading, text)
# Called by find_section_paras when a section_name heading has been found.
# Returns the position of the first and last paras of section_name
def find_first_section_para_index(section_name, first_heading, text):
    first = []
    last = []
    counter1 = first_heading
--
# Function: find_first_intro_para_index(section_name, first_heading, text, headings)
# Called by find_section_paras when an Introduction heading has been found.
# Returns the position of the first and last paras of the introduction.
def find_first_intro_para_index(section_name, first_heading, text, headings, countTextChars):
    all_heading_indices = [item[1] for item in headings]
    counter1 = first_heading
    first = [] # edge case condition
--
# Assumes that the intro is the first para that has not been labelled a heading
# and that is either a multi-sentence para, or has a single long sentence in it.
# Calls find_last_section_para_index under certain conditions.
def find_no_intro_heading_indices(text, headings,countTextChars):
    counter1 = 0
    first = []
    last = []
--
# Called with arguments 'text' and the last paragraph of the essay that is not a heading ('result_last').
# Returns the index of the first paragraph of the conclusion.
# Called by find_no_concl_heading_indices.
def find_first_concl_para_index(text,result_last,headings,countTextChars,nf,nf2,dev):
    counter1 = result_last
    first_concl_para_index = []
    p = re.compile('this report', re.IGNORECASE)
--
# Works out which is the last paragraph, then uses that to find the first.
# Calls find_first_concl_para_index.
# Is called by find_section_paras only if no 'Conclusion' heading has been found.
def find_no_concl_heading_indices(section_name, headings, text,countTextChars,nf,nf2, dev):
    list_this_heading = count_this_heading('word count', headings, text)
    list_this_heading.reverse() # Reverse the order of headings with word count in them in case there is more than one, because you want the one at the end of the essay.
    heading_count = len(list_this_heading)
--
# earliest, then the one you are on is probably the title. This allows for a section heading
# for the intro, and a separate title.
# Requires pickling of title, loading at the beginning, etc.
def find_title_indices(section_name,index, text): # 'index' is the first paragraph of the section we are looking at (intro, preface, summary)
    if index == []: # edge case condition
        return ([],[])
    text = text[:index] # redefine text to be the part of the essay preceding the section you are looking at (intro, preface, summary)
    counter1 = len(text)-1 # set the counter to the index of the last paragraph of the new text
    first = []
    title = re.compile('^title', re.IGNORECASE)
--
    return (first,first)


def find_title_from_toc(text):
    captions = [] # Get a list of all the captions
    counter_p = 0
    while 1:
--
# First counts the number of headings for that section type, then uses the result to decide how to determine which are the section paragraphs.
# Calls different functions depending on section name and number of headings found.
# Uses information found about certain sections to find the index of the title.
def find_section_paras(text, section_name, headings, countTextChars, nf,nf2,dev):
    list_this_heading = count_this_heading(section_name, headings, text)
    if list_this_heading != [] and dev == 'DGF':
        print 'All headings like this:', list_this_heading
--
# Called by find_and_label_headings in this file.
# Processes one sentence partially (no POS, no lemm'ng) so that it can be
# directly compared with the assignment question long version.
def pre_process_sent(sent):
    sent = [w.lower() for w in sent[1:]] # Get rid of the structure label otherwise complete match may fail.
    sent = [w for w in sent if w not in essay_stop_words]
    # These lines are necessary to make sent processing exactly match ass q processing.
--
For the statistical analyses I really want to know how many real headings there are
but here I need a list that will be used to locate the beginning and end of the introduction and conclusion.
'''
def get_headings(text):
    counter_p = 0
    mylist2 = []
    while 1:
--
# Called by get_more_headings_using_contents in this file.
# Tests whether a sentence matches any of the headings.
# Uses regular expression 'search' rather than 'match' in case of page numbers being present in heading.
def match_sentence_2_heading(sent, headings):
    #while 1:
    result = False
    for item in headings:
--
# up as headings because they end in a number (the page number), but the twin
# entry in the essay body may be missed if it is particularly long, as it doesn't
# end in a number.
def get_more_headings_using_contents(text, headings):
    section_name = "Introduction"
    mylist1 = headings
    temp = count_this_heading(section_name, headings, text)
--
                break
        return mylist1

def table_entries_recheck1(text):
    mylist3 = []
    counter_p = 0
    while 2:
--
    return text


def table_entries_recheck2(text):
    letters = ['a','b','c','d','e','f','g','h','o','B','C','D','E','F','G','H'] # Note that 'o' is often used like a bullet point. I have deliberately left out 'A' for now. This needs improving.
    enums = ['i','ii','iii','iv','v','vi','vii','viii','ix','x','xi','xii','One','Two','Three','Four','Five','Six','Seven','Eight'] # xxxx testing this. It might create odd results.    #t = re.compile(untokend, re.IGNORECASE)
    mylist3 = []
--
    return text


def find_and_label_headings(text,ass_q_long_words,nf2,dev):
    temp = [x for y in ass_q_long_words for x in y] # Unnest paragraph level of nesting in ass_q
    ass_q_long_words = [item[1:] for item in temp] # Get rid of the structure label otherwise complete match may fail.
    #######################
--
    return tempD, allheadings, countTextChars

# xxxx This is a very crude check to see if a heading is a date. Needs improvement.
def identify_date(sentstring):
    mymonths = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']
    mylist = []
    for item in mymonths:
--


# xxxx Note that when you write nested 'if' clauses, you must provide 'else' cases for all cases where previous 'if's succeed.
def find_and_label_heading(text,sent, proc_sent, sent1, counter_s, counter_p, para, previouspara, nextpara, ass_q_long_words,countTextChars):
    countPassageChars = 0 # Count the number of characters in the text up to this paragraph
    w = counter_p + 1
    for para3 in text[:w]:
--
        return temp

# Called by pre_process_struc in se_procedure.py
def find_and_label_section_sents(text50,headings,countTextChars,nf,nf2,dev):

    # Set variables 'section_names' and 'section_labels'.
    # Orders must match.
--

import re

DEFINED_CHARACTERS_ORGINAL_ALF_UPPER_MDD 		= u'\u0622'
DEFINED_CHARACTERS_ORGINAL_ALF_UPPER_HAMAZA		= u'\u0623'
DEFINED_CHARACTERS_ORGINAL_ALF_LOWER_HAMAZA 	= u'\u0625'
DEFINED_CHARACTERS_ORGINAL_ALF 					= u'\u0627'
DEFINED_CHARACTERS_ORGINAL_LAM					= u'\u0644'

LAM_ALEF_GLYPHS = [
	[u'\u3BA6', u'\uFEF6', u'\uFEF5'],
--
	[u'\u06CC', u'\uFEEF', u'\uFEF3', u'\uFEF4', u'\uFEF0', 4]
]

def get_reshaped_glyph(target, location):
	if target in ARABIC_GLYPHS:
		return ARABIC_GLYPHS[target][location]
	else:
		return target
		
def get_glyph_type(target):
	if target in ARABIC_GLYPHS:
		return ARABIC_GLYPHS[target][5]
	else:
		return 2
		
def is_haraka(target):
	return target in HARAKAT

def replace_jalalah(unshaped_word):
	return re.sub(u'^\u0627\u0644\u0644\u0647$', u'\uFDF2', unshaped_word)

def replace_lam_alef(unshaped_word):
	list_word = list(unshaped_word)
	letter_before = u''
	for i in range(len(unshaped_word)):
		if not is_haraka(unshaped_word[i]) and unshaped_word[i] != DEFINED_CHARACTERS_ORGINAL_LAM:
			letter_before = unshaped_word[i]

		if unshaped_word[i] == DEFINED_CHARACTERS_ORGINAL_LAM:
			candidate_lam = unshaped_word[i]
			lam_position = i
			haraka_position = i + 1
--
			
	return u''.join(list_word).replace(u' ', u'')
		
def get_lam_alef(candidate_alef, candidate_lam, is_end_of_word):
	shift_rate = 1
	reshaped_lam_alef = u''
	if is_end_of_word:
		shift_rate += 1
	
	if DEFINED_CHARACTERS_ORGINAL_LAM == candidate_lam:
		if DEFINED_CHARACTERS_ORGINAL_ALF_UPPER_MDD == candidate_alef:
			reshaped_lam_alef = LAM_ALEF_GLYPHS[0][shift_rate]
		
		if DEFINED_CHARACTERS_ORGINAL_ALF_UPPER_HAMAZA == candidate_alef:
			reshaped_lam_alef = LAM_ALEF_GLYPHS[1][shift_rate]
		
		if DEFINED_CHARACTERS_ORGINAL_ALF == candidate_alef:
			reshaped_lam_alef = LAM_ALEF_GLYPHS[2][shift_rate]
		
		if DEFINED_CHARACTERS_ORGINAL_ALF_LOWER_HAMAZA == candidate_alef:
			reshaped_lam_alef = LAM_ALEF_GLYPHS[3][shift_rate]
	
	return reshaped_lam_alef

class DecomposedWord(object):
	def __init__(self, word):
		self.stripped_harakat = []
		self.harakat_positions = []
		self.stripped_regular_letters = []
--
				self.letters_position.append(i)
				self.stripped_regular_letters.append(c)

	def reconstruct_word(self, reshaped_word):
		l = list(u'\0' * (len(self.stripped_harakat) + len(reshaped_word)))
		for i in range(len(self.letters_position)):
			l[self.letters_position[i]] = reshaped_word[i]
--
			l[self.harakat_positions[i]] = self.stripped_harakat[i]
		return u''.join(l)

def get_reshaped_word(unshaped_word):
	unshaped_word = replace_jalalah(unshaped_word)
	unshaped_word = replace_lam_alef(unshaped_word)
	decomposed_word = DecomposedWord(unshaped_word)
--
		result = reshape_it(u''.join(decomposed_word.stripped_regular_letters))
	return decomposed_word.reconstruct_word(result)

def reshape_it(unshaped_word):
	if not unshaped_word:
		return u''
	if len(unshaped_word) == 1:
--
	return u''.join(reshaped_word)


def is_arabic_character(target):
	return target in ARABIC_GLYPHS or target in HARAKAT
	
def get_words(sentence):
	if sentence:
		return re.split('\\s', sentence)
	return []
	
def has_arabic_letters(word):
	for c in word:
		if is_arabic_character(c):
			return True
	return False

def is_arabic_word(word):
	for c in word:
		if not is_arabic_character(c):
			return False
	return True
	
def get_words_from_mixed_word(word):
	temp_word = u''
	words = []
	for c in word:
--
		words.append(temp_word)
	return words
	
def reshape(text):
	if text:
		lines = re.split('\\r?\\n', text)
		for i in range(len(lines)):
--
		return u'\n'.join(lines)
	return u''
	
def reshape_sentence(sentence):
	words = get_words(sentence)
	for i in range(len(words)):
		word = words[i]
--
import re

def policy_first_person():
  """
  Don't use the first person. Use third person.
  """
--
  re.compile(r"\s+our\s+", re.I), \
  re.compile(r"\s+us\s+", re.I)

def policy_second_person():
  """
  Don't use second person. Use third person.
  """
--
  re.compile(r"\s+you\s+", re.I), \
  re.compile(r"\s+your\s+", re.I)

def policy_negative():
  """
  Don't use negation. Use e.g. \"It didn't work\" vs. \"It did crash\".
  """
--
  re.compile(r"does not", re.I), \
  re.compile(r"can not", re.I)

def policy_passive_voice():
  """
  Don't use passive voice.
  """
--
  re.compile(r"\s+have\s+[^\.\;\:]*?\s+by", re.I), \
  re.compile(r"\s+had\s+[^\.\;\:]*?\s+by", re.I)

def policy_command_instruction():
  """
  Don't use commands or instruct your reader.
  """
--
  re.compile(r"must", re.I), \
  re.compile(r"should", re.I)

def policy_uncertainty():
  """
  Don't use uncertainties.
  """
--
  re.compile(r"maybe", re.I), \
  re.compile(r"probably", re.I)

def policy_announcement():
  """
  Don't announce your work.
  Bad: "In this paper, I will discuss how ABC..."
--
  re.compile(r"In the article", re.I), \
  re.compile(r"In the essay", re.I)

def policy_punctation_outside_quote():
  """
  Use punctation always outside a quote.
  """
--
  re.compile(r"'',", re.I), \
  re.compile(r"'';", re.I)

def policy_gerundial():
  """
  Don't use gerundial.
  """
--
  re.compile(r"were[^\.\;\:]*?ing", re.I), \
  re.compile(r"have[^\.\;\:]*?ing", re.I)

def policy_vague_pronous():
  """
  Don't use vague pronous.
  Bad: This, these, it, you
--
  re.compile(r"that", re.I), \
  re.compile(r"which", re.I)

def policy_too_few_conjunctive_adverb():
  """
  Don't use too few conjunctive adverbs.
  """
--
  re.compile(r"rather", re.I), \
  re.compile(r"for example", re.I)

def policy_phrase_and_word():
  """
  Don't think, believe or feel. State.
  """
--
  re.compile(r"being", re.I), \
  re.compile(r"one", re.I)

def policy_editorialization():
  """
  Don't editorialize your work.
  """
--
  re.compile(r"assuredly", re.I), \
  re.compile(r"literally", re.I)

def policy_contraction():
  """
  Don't use contractions. Write them out. Correctly.
  """
--
  re.compile(r"hadn't", re.I), \
  re.compile(r"hasn't", re.I)

def policy_sentence_begin():
  """
  Don't start a sentence with the wrong words.
  """
--
  re.compile(r"\.\s*Through *"), \
  re.compile(r"\.\s*With *")

def policy_redundant_pair():
  """
  Don't use redundant pairs.
  """
--
  re.compile(r"extreme in degree", re.I), \
  re.compile(r"of a strange type", re.I)

def policy_wordiness():
  """
  Don't bring gap fillers and be wordiness.
  """
--
  re.compile(r"evident", re.I), \
  re.compile(r"unique", re.I)

def policy_terms_and_phrases():
  """
  Don't use common sloppy language words.
  """
--
import time
import tarfile

def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    If 'subdir' is True, recursively access subdirectories under 'dir_name'.
    Additional arguments, if any, are file extensions to add to the list.
--
            fileList += dir_list(dirfile, subdir, *args)
    return fileList

def combine_files(fileList, fn):
    f = open(fn, 'w')
    for file in fileList:
        print 'Writing file %s' % file
--
from native_tags.decorators import comparison, function


def matches(pattern, text):
    'String comparison. True if string ``text`` matches regex ``pattern``'
    return re.compile(str(pattern)).match(text)
matches = comparison(matches)
matches.test = {'args':('\d','_'),'result':None}

def substitute(search, replace, text):
    'Regex substitution function. Replaces regex ``search`` with ``replace`` in ``text``'
    return re.sub(re.compile(str(search)), replace, text)
substitute = function(substitute)
substitute.test = {'args':('w','f','wtf'),'result':'ftf'}

def search(pattern, text):
    'Regex pattern search. Returns match if ``pattern`` is found in ``text``'
    return re.compile(str(pattern)).search(str(text))
search = function(search)
--
    dialog.choose_colour(title="Select Colour") Show a Colour Chooser dialog
    dialog.choose_directory(title="Select Directory", initialDir="~", rememberAs=None, **kwargs) Show a Directory Chooser dialog
    dialog.combo_menu(options, title="Choose an option", message="Choose an option", **kwargs) Show a combobox menu
    dialog.input_dialog(title="Enter a value", message="Enter a value", default="", **kwargs) Show an input dialog
    dialog.list_menu(options, title="Choose a value", message="Choose a value", default=None, **kwargs) Show a single-selection list menu
    dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs) Show a multiple-selection list menu
    dialog.open_file(title="Open File", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show an Open File dialog
    dialog.password_dialog(title="Enter password", message="Enter password", **kwargs) Show a password input dialog
    dialog.save_file(title="Save As", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show a Save As dialog
--
 #Control how much indentation to trim
#http://code.activestate.com/recipes/145672-multi-line-string-block-formatting/

def ltrimBlock(s, i=0, tz=4, sp=' ', n='\n'):
    p = sp * i  # p = line prefix

    # break the lines down
--

# Colorization
COLOR_CODES = ( { 'none': "",
                  'default': "\033[0m",
                  # primary colors
                  'black': "\033[0;30m",
                  'grey': "\033[0;37m",
--
                  'row0': "\x1b[0;35;40m",
                  'row1': "\x1b[0;36;40m" } )

# Default colors
DEFAULT_COLOR = COLOR_CODES['default']
CONTEXT_COLOR   = COLOR_CODES['dark_yellow']
PROJECT_COLOR   = COLOR_CODES['dark_purple']
STATUS_COLOR    = COLOR_CODES['dark_green']
--
class GTD(cmd.Cmd):
    """GTD(cmd.Cmd) class."""

    def __init__(self):
        cmd.Cmd.__init__(self)
        self.intro= """
  yaGTD is a free software available under the terms of the GNU GPL.
--
    # Private functions.
    #

    def _parse_args(self, args):
        """Parse command arguments= num + str."""

        num = string = None
--
        #print "parsed", num, string
        return (num, string)

    def _parse_batch(self, args):  # BB
        """Return search criterion and elemenst to add/change."""

        regexp = action = ''
--
            pass
        return regexp, action

    def _colorize(self, string):
        """Colorize the given 'string'."""

        s = string
--
        if self.colorize:
            # Scan to colorize attributes (accordingly to global settings) ...
            for attr in Task.attributes_list[3:]:
                color_fct = lambda m: eval(attr.upper() + '_COLOR') + m.group(0) + DEFAULT_COLOR
                s = eval(attr.upper() + '_REGEXP').sub(color_fct, s)

        return s

    def _parse_line(self, line):
        """Return a dictionary (task mapping) from 'line' parsing."""

        t = {}  # Task mapping
--

        return t

    def _dump_line(self, task):
        """Return a formatted line from the given 'task'."""

        s = task['title']  # init the line with the title
--

        return s

    def _disp(self, task):
        """Display the 'id' and a summary of the 'task'."""

        if FORMATTED_DISPLAY:
--

        id_str = "%3d:(%f)" % (task['id'], task.priority())
        if self.colorize:  # colorize #id
            id_str = COLOR_CODES['cyan'] + id_str + DEFAULT_COLOR

        return "%s %s" % (id_str, self._colorize(task_line))

    def _show(self, task):
        """Display details of the given 'task'."""

        s = ""
--
                    s = s + str(task[attrib])
        return s

    def _add(self, line):
        """Adds task from the given line and returns the new task_id."""

        if line:
--

            return task_id

    def _replace(self, idx, line):
        """Replace the task given by its idx by the new line."""

        # Frist, we need to find the task
--
            # And, set the start date if none
            self.do_append("%d S:%s" % (idx, datetime.datetime.now().strftime("%Y-%m-%d")))

    def _search(self, regexp, completed=False, quiet=False):
        """Retrieve tasks matching given regexp.
        If completed True, also look for completed tasks."""

--
        # BB: This tiny line will be damn useful for further use...
        return tasks

    def _searchplus(self, regexp):  # BB
        """It searches, and returns the IDs."""

        lines = self._search(regexp, quiet=True)
        return [ line['id'] for line in lines ]

    def _get_deadlines_ordered(self):
        """Get tasks ordered by deadline, oldest first."""

        tasks = {}  # due -> task
--

        return ordered

    def _get_deadline_string(self, task):
        """Return formatted string of a deadline task."""

        now = datetime.datetime.now()
--
    # Edition.
    #

    def do_add(self, line):
        """Add a new task (from the input line):
        GTD> add My new task. @context p:Project"""

--

    do_a = do_add

    def do_del(self, id):
        """Delete given task:
        GTD> del #id"""

--

    do_rm = do_del

    def do_edit(self, id):
        """Interactively edit task given by #id:
        GTD> edit #id
        GTD edit> My task to edit. @context p:Project"""
--
        if not task:
            return

        def pre_input_hook():
            readline.insert_text(self._dump_line(task))
            readline.redisplay()

--
        self._replace(idx, line)
        print "Task #%d updated" % idx

    def do_close(self, id):
        """Close the given task:
        GTD> close #id"""

--
    do_done = do_close
    do_do = do_close

    def do_replace(self, id_line):
        """Replace the entire task by a new one:
        GTD> replace #id My new task. @computer p:Project"""

--

    do_sub = do_replace

    def do_extend(self, id_desc):
        """Add more text (description) to task:
        GTD> extend #id Additional description."""

--

    do_notes = do_extend

    def do_append(self, id_line):
        """Add new elements to task but leave existing elements unchanged:
        GTD> append #id @newcontext p:NewProject"""

--

    do_addto = do_append

    def do_appendall(self, args):  # BB
        """Add new elements to several tasks but leaves other elements unchanged:
        GTD> appendall regexp @newcontext p:NewProject"""

--
        for taskid in tasks:
            self.do_append('%d %s' % (taskid, action))

    def do_modify(self, id_line):
        """Add/change elements of task but leave each other unchanged:
        GTD> modify #id @othercontext p:UpdtProject"""

--

    do_mod = do_modify

    def do_modifyall(self, args):  # BB
        """Add/change elements of several tasks but leave each other unchanged:
        GTD> modifyall regexp @othercontext p:UpdtProject"""

--
    # Specific GTD commands.
    #

    def do_someday(self, id):
        """Mark the given task as !someday:
        GTD> someday #id"""

--

    do_maybe = do_someday

    def do_waitingfor(self, id):
        """Mark the given task as !waitingfor:
        GTD> waitingfor #id"""

--
        if idx:
            self.do_modify("%d %s" % (idx, "!waitingfor"))

    def do_ref(self, id_ref):
        """Archive the given task as Reference:
        GTD> ref #id refname
        """
--
    # Additional options from Stephen Covey.
    #

    def do_urgency(self, id_level):
        """Set the Urgency (5=today, 4=this_week, 3=this_month, 2=this_quarter, 1=this_year):
        GTD> urgency #id lvl"""

--

    do_u = do_urgency

    def do_importance(self, id_level):
        """Set the Importance (5=crucial,4=high,3=normal,2=low,1=someday):
        GTD> importance #id lvl"""

--

    do_i = do_importance

    def do_complete(self, id_percent):
        """Set the percent complete:
        GTD> complete #id percentage"""

--

    do_c = do_complete

    def do_time(self, id_time):
        """Set the Time requiered to accomplish the task (in Weeks, Days, Hours or Minutes):
        GTD> time #id n[WDHM]"""

--
        if idx and time and re.match(TIMEDELTA_MATCH, time):
            self.do_modify("%d T:%s" % (idx, time))

    def do_recurrence(self, id_rec):
        """Set the Recurrence of the task (in Weeks, Days or Hours):
        GTD> recurrence #id n[WDH]"""

--
    do_recurs = do_recurrence
    do_rec = do_recurrence

    def do_start(self, id_date):
        """Set the Start / creation date:
        GTD> start #id YYYY-MM-DD"""

--
        if idx and date and re.match(DATE_MATCH, date):
            self.do_modify("%d S:%s" % (idx, date))

    def do_due(self, id_date):
        """Set the Due (target) date:
        GTD> due #id YYYY-MM-DD"""

--
        if idx and date and re.match(DATE_MATCH, date):
            self.do_modify("%d D:%s" % (idx, date))

    def do_duein(self, id_doff):
        """Set the Due (target) date in +/- days:
        GTD> duein #id +/-days"""

--

            self.do_modify("%d D:%s" % (idx, date))

    def do_end(self, id_date):
        """Set the End, closure date:
        GTD> end #id YYYY-MM-DD"""

--
    # Sorted/ordered display.
    #

    def do_listall(self, nb):
        """List #nb tasks (close or not and ordered by #id):
        GTD> listall [#nb]"""

--

    do_la = do_listall

    def do_list(self, nb):
        """List #nb open tasks (ordered by #id):
        GTD> list [#nb]"""

--

    do_ls = do_list

    def do_listinbox(self, nb):
        """List #nb quick added tasks (i.e. without context; ordered by #id):
        GTD> listinbox [#nb]"""

--

    do_li = do_listinbox

    def do_listref(self, nb):
        """List #nb ref tasks (ordered by #id):
        GTD> listref [#nb]"""

--

    do_lr = do_listref

    def do_show(self, id):
        """Show details of the given task:
        GTD> show #id"""

--
    do_view = do_show
    do_v = do_show

    def do_sort(self, nb):
        """Sort #nb tasks by priority (appear only after start date):
        GTD> sort [#nb]"""

--
        if len(tasks) > 10:
            print "%d tasks found" % len(tasks)

    def do_today(self, nb):
        """Sort #nb tasks by priority, and shows only today
        (the time now before and after start date):
        GTD> today [#nb]"""
--

    do_now = do_today

    def do_listpri(self, regexp):
        """Sort tasks matching given regexp (appear only after start date):
        GTD> listpri [regexp]"""

--

    do_lp = do_listpri

    def do_order(self, nb_attr):
        """Order #nb tasks by context/project/status/reference and priority
        (appear only after start date):
        GTD> order #nb [context|project|status|reference]"""
--
        nb, attr = self._parse_args(nb_attr)

        if not attr or attr not in ['context', 'project', 'status', 'reference']:
            attr = 'context'  # by default order by context

        for c, ts in self.todo.order(attr).items():
            if attr == 'reference':
--

    do_o = do_order

    def do_status(self, line):
        """Display projects statuses (percent complete):
        GTD> status"""

--

    do_summary = do_status

    def do_contexts(self, line):
        """Display contexts and next task for each context:
        GTD> contexts"""

--
            else:
                print

    def do_deadlines(self, line):
        """Display upcoming tasks ordered by deadline:
        GTD> deadlines #nb"""

--
        if len(ordered) > 10:
            print "%d due tasks found" % len(ordered)

    def do_overdue(self, line):
        """Display overdue tasks ordered by deadline (including due today):
        GTD> overdue #nb"""

--
    # Select/monitor.
    #

    def do_select(self, cmd):
        """ Select a task:
        GTD> select #id"""

--

        self.tasks_selected.append(idx)

    def do_deselect(self, cmd):
        """ Deselect a task:
        GTD> deselect #id"""

--

        self.tasks_selected.remove(idx)

    def do_selected(self, line=None):
        """Show selected tasks sorted by priority:
        GTD> selected"""

--
    # Re-search.
    #

    def do_search(self, regexp):
        """Retrieve tasks matching given regexp:
        GTD> search regexp"""

--

    do_s = do_search

    def do_searchall(self, regexp):
        """Retrieve tasks matching given regexp, including completed ones:
        GTD> searchall regexp"""

--
    # IO functions.
    #

    def do_load(self, todotxt):
        """Load from a todotxt file:
        GTD> load path/to/todo.txt"""

--

    do_l = do_load

    def do_save(self, todotxt):
        """Save to a todotxt file:
        GTD> save [path/to/todo.txt]"""

--
    do_write = do_save
    do_w = do_save

    def do_archive(self, donetxt):
        """Archive completed tasks:
        GTD> archive [path/to/done.txt]"""

--
        except IOError, err:
            print err  # and continue

    def do_print(self, rest):
        """Export into printable format (ReST):
        GTD> print [path/to/todo.rest]"""

--
    # Help/usage.
    #

    def do_usage(self, line=None):
        """Show language reference:
        GTD> usage"""

--
    # Quit.
    #

    def do_EOF(self, line=None):
        """Quit."""

        print "bye"
        self.do_save("")  # auto save (default location)
        sys.exit()

    do_quit = do_EOF
--
#
# Main entry point.
#
def main(options, todotxt=TODO_TXT, todoyaml=None):
    """A primitive Getting Things Done to-do list manager.
       Cmd version: line oriented command interpreters."""

--
    option_list = [
        make_option("-c", "--color",
                    action="store_true", dest="color",
                    default=False,
                    help="activate color highlightment"),
        make_option( "-q", "--quiet",
                    action="store_true", dest="quiet",
--
from HTMLParser import HTMLParser

class MLStripper(HTMLParser):
    def __init__(self):
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()
--
  version = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11'


def grabJoke(cleanliness, keyword):
  myopener = MyOpener()
  seed = random() * 100
  try:
--
UPPER_BOUND = .90


def is_unimportant(word):
    """Decides if a word is ok to toss out for the sentence comparisons"""
    return word in ['.', '!', ',', ] or '\'' in word or word in stop_words


def only_important(sent):
    """Just a little wrapper to filter on is_unimportant"""
    return filter(lambda w: not is_unimportant(w), sent)


def compare_sents(sent1, sent2):
    """Compare two word-tokenized sentences for shared words"""
    if not len(sent1) or not len(sent2):
        return 0
    return len(set(only_important(sent1)) & set(only_important(sent2))) / ((len(sent1) + len(sent2)) / 2.0)


def compare_sents_bounded(sent1, sent2):
    """If the result of compare_sents is not between LOWER_BOUND and
    UPPER_BOUND, it returns 0 instead, so outliers don't mess with the sum"""
    cmpd = compare_sents(sent1, sent2)
--
    return cmpd


def compute_score(sent, sents):
    """Computes the average score of sent vs the other sentences (the result of
    sent vs itself isn't counted because it's 1, and that's above
    UPPER_BOUND)"""
--
    return sum(compare_sents_bounded(sent, sent1) for sent1 in sents) / float(len(sents))


def summarize_block(block):
    """Return the sentence that best summarizes block"""
    sents = nltk.sent_tokenize(block)
    word_sents = map(nltk.word_tokenize, sents)
--
    return d[max(d.keys())]


def find_likely_body(b):
    """Find the tag with the most directly-descended <p> tags"""
    return max(b.find_all(), key=lambda t: len(t.find_all('p', recursive=False)))


class Summary(object):
    def __init__(self, url, article_html, title, summaries):
        self.url = url
        self.article_html = article_html
        self.title = title
        self.summaries = summaries

    def __repr__(self):
        return 'Summary({0}, {1}, {2}, {3})'.format(
            repr(self.url), repr(self.article_html), repr(self.title), repr(self.summaries)
        )

    def __str__(self):
        return u"{0} - {1}\n\n{2}".format(self.title, self.url, '\n'.join(self.summaries))


def summarize_page(url):
    import bs4
    import re
    import requests
--
#!/usr/bin/env python
# -*- coding: utf-8 -*-

def who():
    return


def where():
    return


def when():
    return


def what():
    return


def why():
    return


def how():
    return


def which():
    return
--

import urllib2

def getgoogleurl(search,siteurl=False):
    if siteurl==False:
        return 'http://www.google.com/search?q='+urllib2.quote(search)+'&oq='+urllib2.quote(search)
    else:
        return 'http://www.google.com/search?q=site:'+urllib2.quote(siteurl)+'%20'+urllib2.quote(search)+'&oq=site:'+urllib2.quote(siteurl)+'%20'+urllib2.quote(search)

def getgooglelinks(search,siteurl=False):
   #google returns 403 without user agent
   headers = {'User-agent':'Mozilla/11.0'}
   req = urllib2.Request(getgoogleurl(search,siteurl),None,headers)
--
                  links.append(link)
      return links

def googleAssignment(asnName):
	output = ''
	try:
		output += getgooglelinks(asnName,'http://en.wikipedia.org')[0] + "\n"
--
import robot
import PorterStemmer
import time
socket.setdefaulttimeout(10)
# in a true webcrawler you should not use re instead use a DOM parser
import re
# for sorting a nested list
from operator import itemgetter

def get_page(url):
        try:
            f = urllib.urlopen(url)
            page = f.read()
--
        except:
            return ""
        return ""
def get_next_target(page):
        start_link = page.find('<a href=')
        if start_link == -1:
            return None, 0
--
        end_quote = page.find('"', start_quote + 1)
        url = page[start_quote + 1:end_quote]
        return url, end_quote
def union(p,q):
        cnt = 0
        for e in q:
            if e not in p:
                p.append(e)
                cnt += 1
        return cnt
def get_all_links(page):
        links = []
        while True:
            url,endpos = get_next_target(page)
--
            else:
                break
        return links
def add_to_index(index,keyword,url):
        #if keyword in index:
           # if url not in index[keyword]:
              #  index[keyword].append(url)
        index[0] += 1
       # else:
           # index[keyword] = [url]
def split_string(source,splitlist):
    return ''.join([ w if w not in splitlist else ' ' for w in source]).split()
def add_page_to_index_re(index,url,content):
        i = 0
        # it is not a good idea to use regular expression to parse html
        # i did this just to give a quick and dirty result
--
                        add_to_index(index,word,url)

        return i
def format_url(root,page):
        if page[0] == '/':
            return root + page
        return page
def crawl_web(seed,max_pages=10,max_depth=1):
        root = seed
        tocrawl = [seed]
        depth = [0]
--
                        crawled.append(page)
                        print "%s of %s pages sucessfully crawled" % (len(crawled), max_pages)
        return index, graph
def stem(word,p):
        output = ''
        output += p.stem(word, 0,len(word)-1)
        return output

def lookup(index, keyword):
        if keyword in index:
            return index[keyword]
        return None
def sort_by_score(l):
        get_score = itemgetter(0)
        map(get_score,l)
        l = sorted(l,key=get_score)
        l.reverse()
        return l
def lookup_best(index, keyword, ranks):
        result = []
        if keyword in index:
            for url in index[keyword]:
--
        if len(result) > 0:
            result = sort_by_score(result)
        return result
def get_inlinks(page,graph):
        il = {}
        for p in graph:
            for ol in graph[p]:
                if ol == page:
                    il[p] = graph[p]
        return il
def compute_ranks(graph):
        d = 0.8 # damping factor
        numloops = 10
        ranks = {}
--
        import pickle
        GLOBAL_NUM_SEARCHES = 8
        GLOBAL_TRENDING_INTERVAL = 4
        def calculate_trending(trending,now,before,interval,threshhold=0.5):
            for s in now:
                if s in before:
                    slope = float(now[s] - before[s])/interval
--
                        if s in trending:
                            trending.pop(s)
            return trending
        def trending(searches,interval):
            curr_searches = {}
            prev_searches = {}
            is_trending = {}
--
                    prev_searches = curr_searches.copy()
                    curr_searches.clear()
            return is_trending
        def print_cmds():
                return "    Welcome to the CS101 Web Crawler\n" + \
                       "    What do you want to do?\n" + \
                       "    Enter 1 - To start crawling a web page\n" + \
--
                       "    Enter 6 - Delete Index\n" + \
                       "    Enter q - Quit\n" + \
                       "    crawler:"
        def execute_start_crawl(index):
                maxdepth = int(raw_input("    Enter Max Depth:"))
                maxpages = int(raw_input("    Enter Max Pages:"))
                url = raw_input("    Enter Web Url:")
                return crawl_web(url,maxpages,maxdepth)
        def delete_file(path):
            ret = ""
            if os.path.exists(path):
                try:
--
                except:
                    ret += "        Failed to delete {0}\n".format(path)
                print ret
        def clear_data(data,data_str,path):
            ret = ''
            delete_file(path)
            length = len(data)
--
            ret += "        Cleared {0} entries from {1}\n".format(length,data_str)
            print ret
            return data
        def open_file(data,data_str,path):
            ret = ""
            file = open(path,"wb")
            if file:
--
            else:
                ret += "        Failed to open {0} at {1}\n".format(data_str,path)
            print ret
        def load_file(data,data_str,path):
            ret = ""
            if os.path.exists(path):
                file1 = open(path,'rb')
--
                ret += "        {0} does not exist\n".format(path)
            print ret
            return data
        def execute_cmd(c,index,graph, ranks, searches):
                if c == '1':
                    start = time.time()
                    index, graph = execute_start_crawl(index)
--
                    raw_input("    Press Enter")
                    print ""
                return index, graph, ranks, searches
        def main():

                index = {}
                graph = {}
--
grades = [[float(n) for n in l.split()[1:]] for l in open(os.path.join("data/grades.txt")).readlines()[::-1][:-5]]


def correct_essay_grade(essay_index, grade_type):
    if grade_type not in cols:
        raise Exception('%s is not a valid grade category' % (grade_type),)
    return grades[essay_index][cols.index(grade_type)]


def grade_text(text, grade_type):
    if grade_type not in cols:
        raise Exception('%s is not a valid grade category' % (grade_type),)
    if grade_type == "3a":
--
        return grade_2b(text)


def grade_1a(text):
    sentences = sentence_tokenizer.parse(text)
    num_problems = 0
    num_sentences = 0
--
        return 1


def grade_1b(text):
    import agreement_utils
    rs = agreement_utils.parse(text)
    num_agreements, num_non_agreements, num_unsure = rs
--
        return floor(prob * 5)


def grade_1c(text):
    return 3.2


def grade_1d(text):
    import syntactic_formation
    import math
    sentence_problems = syntactic_formation.parse(text)
--
    return score


def grade_2a(text):
    import text_coherence

    # weights
--
    return max(1, min(5, (2 + (weight_2nd_person * len(second_person_pronouns)) + score + (len(pronoun_biz) * weight_sen))))


def grade_2b(text):
    import topic_coherence
    from math import ceil

--
    return ceil(min(5, score, nouns))


def grade_3a(text):
    sentences = sentence_tokenizer.parse(text)
    num_sentences = len(sentences)
    if num_sentences >= 6:
--
#https://github.com/wfeely/egymt/blob/master/scripts/split_data.py
import sys, random

#Splits the Arabic parallel corpus into a randomly selected training set, dev set, and test set using user-defined percentages
def main(args):
        #Check args
        if len(args) < 6:
                print "Usage: python split_data.py arabic_corpus english_corpus dialect train_percentage dev_percentage"
--
    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'
    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'

    def __init__(self, lang):
        self.lang = lang

    def __fetch(self, url):
        request = urllib2.Request(url)
        request.add_header('User-Agent', 'Mozilla/5.0')

--

        return result

    def article(self, article):
        url = self.url_article % (self.lang, urllib.quote_plus(article))
        content = self.__fetch(url).read()

--

        return content

    def image(self, image, thumb=None):
        url = self.url_image % (self.lang, image)
        result = self.__fetch(url)
        content = result.read()
--

        return content

    def search(self, query, page=1, limit=10):
        offset = (page - 1) * limit
        url = self.url_search % (self.lang, urllib.quote_plus(query), offset, 
limit)
--
                    'wordcount' : wordcount
                })

        # yaml.dump(results, default_style='', default_flow_style=False,
        #     allow_unicode=True)
        return results

--
from dateutil.parser import parse
import pandas as pd
import os
from collections import defaultdict


def load():
    pass

def get_paths():
    """
    Redefine data_path and submissions_path here to run the benchmarks on your machine
    """
    data_path = os.environ["DATAPATH"]
    submission_path = os.environ["DATAPATH"]
    return data_path, submission_path

def get_train_test_df(data_path = None):
    if data_path is None:
        data_path, submission_path = get_paths()

--

#get tags

def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#  post  tags

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(0.1)

--
import time
import re

def getwords(text, splitchars=' \t|!?.;:"#'):
    #()+=,/[]{}_
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
--
import os


def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    If 'subdir' is True, recursively access subdirectories under 'dir_name'.
    Additional arguments, if any, are file extensions to add to the list.
--
            fileList += dir_list(dirfile, subdir, *args)
    return fileList

def combine_files(fileList, fn):
    f = open(fn, 'w')
    for file in fileList:
        print 'Writing file %s' % file
--
import sys
import re

def ParseLog(filename, search_string):
  try:
    f = open(filename, 'rU')
  except IOError:
--

  return

def main():
  if len(sys.argv) < 2:
    print 'usage: ./checklog filename1 filename2 filename3 ...'
    sys.exit(0)
--
# Each rule consists of:
# suffix, inflection, category and classic flag.
plural_rules = [
    # 0) Indefinite articles and demonstratives.
    [["^a$|^an$", "some", None, False],
     ["^this$", "these", None, False],
     ["^that$", "those", None, False],
--
        "adjutant", "brigadier", "lieutenant", "major", "quartermaster"],
}

def pluralize(word, pos=NOUN, custom={}, classical=True):
    """ Returns the plural of a given word.
        For example: child -> children.
        Handles nouns and adjectives, using classical inflection by default
        (e.g. where "matrix" pluralizes to "matrices" instead of "matrixes").
        The custom dictionary is for user-defined replacements.
    """

    if word in custom:
--
            "our": "my",
}

def singularize(word, pos=NOUN, custom={}):

    if word in list(custom.keys()):
        return custom[word]
--
            There are many ways to introduce an academic essay or assignment. Most academic writers, however, appear to do one or more of the following in their introductions:
                establish the context, background and/or importance of the topic
                indicate a problem, controversy or a gap in the field of study
                define the topic or key terms
                state of the purpose of the essay/writing
                provide an overview of the coverage and/or structure of the writing
                Examples of phrases which are commonly employed to realise these functions are listed below. Note that there may be a certain amount of overlap between some of the categories under which the phrases are listed.
--
            In this paper I argue that ....
            In the pages that follow, it will be argued that ....
            This paper attempts to show that ....
            In this essay, I attempt to defend the view that ....
        Outline of structure:
            The main questions/issues addressed in this paper are: a), b and c).
            This paper has been divided into four parts. The first part deals with ....
--
            Chapter 3 describes the design, synthesis, characterization and evaluation of ....
            The last chapter assesses the ....
        Explaining Keywords
            While a variety of definitions of the term X have been suggested, this paper will use the definition first suggested by Smith (1968) who saw it as ....
            Throughout this paper the term X will refer to/will be used to refer to ....
            In this article the acronym/abbreviation XYZ will be used.
        Referring to literature
--
                Smith (2013) traces the development of Japanese history and philosophy during the 19th century.
                Jones(2013) provides in-depth analysis of the work of Aristotle showing its relevance to contemporary times.
                Smith (2013) draws our attention to distinctive categories of motivational beliefs often observed in ....
                Smith (2013) defines evidence based medicine as the conscious, explicit and judicious use of .....
                Rao (2013) highlights the need to break the link between economic growth and transport growth .....
                Smith (2013) discusses the challenges and strategies for facilitating and promoting ....
                Toh (2013) mentions the special situation of Singapore as an example of ....
--
            the author overlooks the fact that X contributes to Y.
            what Smith fails to do is to draw a distinction between ....
            another weakness is that we are given no explanation of how ....
            Smith fails to fully define what ....
            no attempt was made to quantify the association between X and Y.
        Offering constructive suggestions
            Smith's paper
--
            The issue of X is an intriguing one which could be usefully explored in further research.
            Future research should therefore concentrate on the investigation of ….
            More research is required on ....
            Large randomised controlled trials could provide more definitive evidence.

        Implications/recommendations for practice or policy
            These findings suggest several courses of action for ....
            An implication of these findings is that both X and Y should be taken into account when ....
            The findings of this study have a number of important implications for future practice.
            There is, therefore, a definite need for ....
            There are a number of important changes which need to be made.
            Another important practical implication is that ....
            Moreover, more X should be made available to ....
--



    Writing Definitions
            In academic work students are often expected to give definitions of key words and phrases in order to demonstrate to their tutors that they understand these terms clearly. Academic writers generally, however, define terms so that their readers understand exactly what is meant when certain key terms are used. When important words are not clearly understood misinterpretation may result. In fact, many disagreements (academic, legal, diplomatic, personal) arise as a result of different interpretations of the same term. In academic writing, teachers and their students often have to explore these differing interpretations before moving on to study a topic.


            Introductory phrases:
--
                This shows a need to be explicit about exactly what is meant by the word X.
                X is a term frequently used in the literature, but to date there is no consensus about ....

            Simple three-part definitions

            A university is

            an institution

            where knowledge is "produced" and passed on to others.
            Social Economics may be broadly defined as	the branch of economics

            [which is] concerned with the measurement, causes and consequences of social problems.
            Research may be defined as

            a systematic process

--
            The term X encompasses A), B), and C).


            In broad biological terms, X can be defined as any stimulus that is ....
            The broad use of the term X is sometimes equated with ....
            The term disease refers to a biological event characterised by .....
            In the literature, the term tends to be used to refer to ....
            X can be defined as .... It encompasses ....
            The term X is a relatively new name for a Y, commonly referred to....
            X can be loosely described as a correlation.

            Indicating difficulties in defining a term:

            In the field of language teaching, various definitions of X are found.
            X is a commonly used notion in language learning and yet it is a concept difficult to define precisely.
            A generally accepted definition of X is lacking.
            Smith (2001) identified four abilities that might be subsumed under the term X: a) ....
            The term X embodies a multitude of concepts which ....
            Although differences of opinion still exist, there appears to be some agreement that X refers to ....
            Unfortunately, X remains a poorly defined term.

            Specifying terms that are used in an essay/thesis:

            In this essay, the term overseas student will be used in its broadest sense to refer to all students who ....
            Throughout this thesis, the term education is used to refer to informal systems as well as formal systems.
            While a variety of definitions of the term X have been suggested, this paper will use the definition first suggested by Smith (1968) who saw it as ....
            In this paper, the term that will be used to describe this phenomenon is X
            In this dissertation the terms X and Y are used interchangeably to mean ....
            Referring to people's definitions (author prominent):

            Smith (1954) was apparently the first to use the term ....
            Chomsky writes that a grammar is a 'device of some sort for producing the .....' (1957, p.11).
            According to a definition provided by Smith (2001:23), fluency is 'the maximally ....
            The term "fluency" is used by Smith (2001) to refer to ....
            Smith (2001) uses the term "fluency" to refer to ....
            For Smith (2001), fluency means/refers to ....
            Macro-stabilisation policy is defined by Smith (2003: 119) as "...."
            Aristotle defines the imagination as "the movement which results upon an actual sensation."
            The term "matter" is used by Aristotle in four overlapping senses. First, it is the underlying ..... Secondly, it is the potential which ....
            Smith et al. (2002) have provided a new definition of health: "health is a state of being with physical, cultural, psychological ...."
            In 1987, sports psychologist John Smith popularized the term X to describe ....
            This definition is close to those of Smith (2012) and Jones (2013) who define X as ….

            Referring to people's definitions (author non-prominent):

            Validity is the degree to which an assessment process or device measures what it is intended to measure (Smith et al., 1986)
        Giving Examples
--
            Many diseases can result at least in part from stress, including: arthritis, asthma, migrane, headaches and ulcers.

        Classifying and Listing
            When we classify things, we group and name them on the basis of something that they have in common. By doing this we can understand certain qualities and features which they shares as a class. Classifying is also a way of understanding differences between things. In writing, classifying is often used as a way of introducing a reader to a new topic. Along with writing definitions, the function of classification may be used in the early part of an essay, or longer piece of writing. We list things when we want to treat and present a series of items or different pieces of information systematically. The order of a list may indicate rank importance.
            General Classifications

            X may be divided into	three main
--
            Scurvy is a disease	caused by
            resulting from
            stemming from	insufficient protein.
            vitamin deficiency.
            lack of vitamin C.
            Nouns expressing causality

            The most likely causes of X are poor diet and lack of exercise.

            A consequence of vitamin A deficiency is blindness.
            Physical activity is an important factor in maintaining fitness.
            Many other medications have an influence on cholesterol levels.
            Another reason why Xs are considered to be important is that .......
--

            Compared with people in oral cultures, people in literate cultures organise their lives around clocks and calendars.

            Oral societies tend to be more concerned with the present whereas literate societies have a very definite awareness of the past.

            Whereas Ghazali rejected non-Islamic philosophers, Aquinas incorporated ancient Greek thought into his own philosophical writings.

--
            So far this paper/chapter has focussed on X. The following section will discuss ....
            Before proceeding to examine X, it will be necessary to .....
            Before employing these theories to examine X, it is necessary to .....
            Having defined what is meant by X, I will now move on to discuss .....
            Having discussed how to construct X, the final section of this paper addresses ways of
            This section has analysed the causes of X and has argued that ..... The next part of this paper will ....
            This chapter has demonstrated that ..... It is now necessary to explain the course of .....
--
dialog.choose_colour(title="Select Colour") Show a Colour Chooser dialog
dialog.choose_directory(title="Select Directory", initialDir="~", rememberAs=None, **kwargs) Show a Directory Chooser dialog
dialog.combo_menu(options, title="Choose an option", message="Choose an option", **kwargs) Show a combobox menu
dialog.input_dialog(title="Enter a value", message="Enter a value", default="", **kwargs) Show an input dialog
dialog.list_menu(options, title="Choose a value", message="Choose a value", default=None, **kwargs) Show a single-selection list menu
dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[], **kwargs) Show a multiple-selection list menu
dialog.open_file(title="Open File", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show an Open File dialog
dialog.password_dialog(title="Enter password", message="Enter password", **kwargs) Show a password input dialog
dialog.save_file(title="Save As", initialDir="~", fileTypes="*|All Files", rememberAs=None, **kwargs) Show a Save As dialog
--
build_dir = os.path.join(docs_dir, '_build')

@task
def test():
    run("python run_tests.py", pty=True)

@task
def deps():
    print("Vendorizing nltk...")
    run("git clone https://github.com/nltk/nltk.git")
    run("rm -rf text/nltk")
--
    run("rm -rf nltk")

@task
def clean():
    run("rm -rf build")
    run("rm -rf dist")
    run("rm -rf textblob.egg-info")
--
    print("Cleaned up.")

@task
def clean_docs():
    run("rm -rf %s" % build_dir)

@task
def browse_docs():
    run("open %s" % os.path.join(build_dir, 'index.html'))

@task
def build_docs(clean=False, browse=False):
    if clean:
        clean_docs()
    run("sphinx-build %s %s" % (docs_dir, build_dir), pty=True)
--
        browse_docs()

@task
def build_readme():
    run("rst2html.py README.rst > README.html", pty=True)
    run("open README.html")

@task
def doctest():
    os.chdir(docs_dir)
    run("make doctest")
--
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

--
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True
--
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
--
# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
--

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
--
# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
--

class FlaskyStyle(Style):
    background_color = "#f8f8f8"
    default_style = ""

    styles = {
        # No corresponding class for the following:
--
# -*- coding: utf-8 -*-
'''Downloads the minimum necessary NLTK models and corpora required to support
TextBlob's basic features. Use this script if you only intend to use
TextBlob's default models. If you want to use other models, use
download_corpora.py. Modify for your own needs.
'''
from textblob.packages import nltk
--
    'wordnet' # Required for lemmatization
]

def main():
    for each in REQUIRED_CORPORA:
        print('Downloading "{0}"'.format(each))
        nltk.download(each)
--

class BadNLTKClassifier(NLTKClassifier):

    '''An NLTK classifier without ``nltk_class`` defined. Oops!'''
    pass

class TestNLTKClassifier(unittest.TestCase):

    def setUp(self):
        self.bad_classifier = BadNLTKClassifier(train_set)

    @attr("py27_only")
    def test_raises_value_error_without_nltk_class(self):
        with assert_raises(ValueError):
            self.bad_classifier.classifier

--

class TestNaiveBayesClassifier(unittest.TestCase):

    def setUp(self):
        self.classifier = NaiveBayesClassifier(train_set)

    def test_default_extractor(self):
        text = "I feel happy this morning."
        assert_equal(self.classifier.extract_features(text), basic_extractor(text, train_set))

    def test_classify(self):
        res = self.classifier.classify("I feel happy this morning")
        assert_equal(res, 'positive')
        assert_equal(len(self.classifier.train_set), len(train_set))

    def test_classify_a_list_of_words(self):
        res = self.classifier.classify(["I", "feel", "happy", "this", "morning"])
        assert_equal(res, "positive")

    def test_train_from_lists_of_words(self):
        # classifier can be trained on lists of words instead of strings
        train = [(doc.split(), label) for doc, label in train_set]
        classifier = NaiveBayesClassifier(train)
        assert_equal(classifier.accuracy(test_set),
                        self.classifier.accuracy(test_set))

    def test_prob_classify(self):
        res = self.classifier.prob_classify("I feel happy this morning")
        assert_equal(res.max(), "positive")
        assert_true(res.prob("positive") > res.prob("negative"))

    def test_accuracy(self):
        acc = self.classifier.accuracy(test_set)
        assert_true(isinstance(acc, float))

    def test_update(self):
        res1 = self.classifier.prob_classify("lorem ipsum")
        original_length = len(self.classifier.train_set)
        self.classifier.update([("lorem ipsum", "positive")])
--
        assert_true(res2.prob("positive") > res1.prob("positive"))
        assert_equal(original_length + 1, new_length)

    def test_labels(self):
        labels = self.classifier.labels()
        assert_true("positive" in labels)
        assert_true("negative" in labels)

    def test_show_informative_features(self):
        feats = self.classifier.show_informative_features()

    def test_informative_features(self):
        feats = self.classifier.informative_features(3)
        assert_true(isinstance(feats, list))
        assert_true(isinstance(feats[0], tuple))

    def test_custom_feature_extractor(self):
        cl = NaiveBayesClassifier(train_set, custom_extractor)
        cl.classify("Yay! I'm so happy it works.")
        assert_equal(cl.train_features[0][1], 'positive')

    def test_init_with_csv_file(self):
        cl = NaiveBayesClassifier(CSV_FILE, format="csv")
        assert_equal(cl.classify("I feel happy this morning"), 'pos')
        training_sentence = cl.train_set[0][0]
        assert_true(isinstance(training_sentence, unicode))

    def test_init_with_csv_file_without_format_specifier(self):
        cl = NaiveBayesClassifier(CSV_FILE)
        assert_equal(cl.classify("I feel happy this morning"), 'pos')
        training_sentence = cl.train_set[0][0]
        assert_true(isinstance(training_sentence, unicode))

    def test_init_with_json_file(self):
        cl = NaiveBayesClassifier(JSON_FILE, format="json")
        assert_equal(cl.classify("I feel happy this morning"), 'pos')
        training_sentence = cl.train_set[0][0]
        assert_true(isinstance(training_sentence, unicode))

    def test_init_with_json_file_without_format_specifier(self):
        cl = NaiveBayesClassifier(JSON_FILE)
        assert_equal(cl.classify("I feel happy this morning"), 'pos')
        training_sentence = cl.train_set[0][0]
        assert_true(isinstance(training_sentence, unicode))

    def test_accuracy_on_a_csv_file(self):
        a = self.classifier.accuracy(CSV_FILE)
        assert_true(isinstance(a, float))

    def test_accuracy_on_json_file(self):
        a = self.classifier.accuracy(JSON_FILE)
        assert_true(isinstance(a, float))

    def test_init_with_tsv_file(self):
        cl = NaiveBayesClassifier(TSV_FILE)
        assert_equal(cl.classify("I feel happy this morning"), 'pos')
        training_sentence = cl.train_set[0][0]
        assert_true(isinstance(training_sentence, unicode))

    @attr("py27_only")
    def test_init_with_bad_format_specifier(self):
        with assert_raises(ValueError):
            NaiveBayesClassifier(CSV_FILE, format='unknown')

    def test_repr(self):
        assert_equal(repr(self.classifier),
            "<NaiveBayesClassifier trained on {0} instances>".format(len(train_set)))


class TestDecisionTreeClassifier(unittest.TestCase):

    def setUp(self):
        self.classifier = DecisionTreeClassifier(train_set)

    def test_classify(self):
        res = self.classifier.classify("I feel happy this morning")
        assert_equal(res, 'positive')
        assert_equal(len(self.classifier.train_set), len(train_set))

    def test_accuracy(self):
        acc = self.classifier.accuracy(test_set)
        assert_true(isinstance(acc, float))

    def test_update(self):
        original_length = len(self.classifier.train_set)
        self.classifier.update([("lorem ipsum", "positive")])
        new_length = len(self.classifier.train_set)
        assert_equal(original_length + 1, new_length)

    def test_custom_feature_extractor(self):
        cl = DecisionTreeClassifier(train_set, custom_extractor)
        cl.classify("Yay! I'm so happy it works.")
        assert_equal(cl.train_features[0][1], 'positive')

    def test_pseudocode(self):
        code = self.classifier.pseudocode()
        assert_true("if" in code)

    def test_pp(self):
        pp = self.classifier.pprint(width=60)
        assert_true(isinstance(pp, unicode))

    def test_repr(self):
        assert_equal(repr(self.classifier),
            "<DecisionTreeClassifier trained on {0} instances>".format(len(train_set)))


class TestPositiveNaiveBayesClassifier(unittest.TestCase):

    def setUp(self):
        sports_sentences = ['The team dominated the game',
                          'They lost the ball',
                          'The game was intense',
--
        self.classifier = PositiveNaiveBayesClassifier(positive_set=sports_sentences,
                                                        unlabeled_set=various_sentences)

    def test_classifier(self):
        assert_true(isinstance(self.classifier.classifier,
                               nltk.classify.PositiveNaiveBayesClassifier))


    def test_classify(self):
        assert_true(self.classifier.classify("My team lost the game."))
        assert_false(self.classifier.classify("The cat is on the table."))

    def test_update(self):
        orig_pos_length = len(self.classifier.positive_set)
        orig_unlabeled_length = len(self.classifier.unlabeled_set)
        self.classifier.update(new_positive_data=['He threw the ball to the base.'],
--
        assert_equal(new_pos_length, orig_pos_length + 1)
        assert_equal(new_unlabeled_length, orig_unlabeled_length + 1)

    def test_accuracy(self):
        test_set = [
            ("My team lost the game", True),
            ("The ball was in the court.", True),
--
        accuracy = self.classifier.accuracy(test_set)
        assert_true(isinstance(accuracy, float))

    def test_repr(self):
        assert_equal(repr(self.classifier),
            "<PositiveNaiveBayesClassifier trained on {0} labeled and {1} unlabeled instances>"
                .format(len(self.classifier.positive_set),
--
                     )


def test_basic_extractor():
    text = "I feel happy this morning."
    feats = basic_extractor(text, train_set)
    assert_true(feats["contains(feel)"])
    assert_true(feats['contains(morning)'])
    assert_false(feats["contains(amazing)"])

def test_basic_extractor_with_list():
    text = "I feel happy this morning.".split()
    feats = basic_extractor(text, train_set)
    assert_true(feats["contains(feel)"])
    assert_true(feats['contains(morning)'])
    assert_false(feats["contains(amazing)"])

def test_contains_extractor_with_string():
    text = "Simple is better than complex"
    features = contains_extractor(text)
    assert_true(features["contains(Simple)"])
--
    assert_true(features['contains(complex)'])
    assert_false(features.get("contains(derp)", False))

def test_contains_extractor_with_list():
    text = ["Simple", "is", "better", "than", "complex"]
    features = contains_extractor(text)
    assert_true(features['contains(Simple)'])
--
    assert_true(features['contains(complex)'])
    assert_false(features.get("contains(derp)", False))

def custom_extractor(document):
    feats = {}
    tokens = document.split()
    for tok in tokens:
--
@attr('requires_internet')
class TestTranslator(unittest.TestCase):

    def setUp(self):
        self.translator = Translator()
        self.sentence = "This is a sentence."

    def test_translate(self):
        t = self.translator.translate(self.sentence, to_lang="es")
        assert_equal(t, "Esta es una frase.")

    def test_detect(self):
        lang = self.translator.detect(self.sentence)
        assert_equal(lang, "en")
        lang2 = self.translator.detect("Hola")
--
        lang4 = self.translator.detect("Programmiersprache")
        assert_equal(lang4, 'de')

    def test_detect_non_ascii(self):
        lang = self.translator.detect(unicode("关于中文维基百科"))
        assert_equal(lang, 'zh-CN')
        lang2 = self.translator.detect(unicode("известен още с псевдонимите"))
--
        assert_equal(lang3, "ru")


    def test_get_language_from_json5(self):
        json5 = '[[["This is a sentence.","This is a sentence.","",""]],,"en",,,,,,[["en"]],0]'
        lang = self.translator._get_language_from_json5(json5)
        assert_equal(lang, "en")
--

class TestPatternParser(unittest.TestCase):

    def setUp(self):
        self.parser = PatternParser()
        self.text = "And now for something completely different."

    def test_parse(self):
        assert_equal(self.parser.parse(self.text), pattern_parse(self.text))


--

    '''An example unit test case.'''

    def setUp(self):
        self.tokenizer = WordTokenizer()
        self.text = "Python is a high-level programming language."

    def tearDown(self):
        pass

    def test_tokenize(self):
        assert_equal(self.tokenizer.tokenize(self.text),
            ['Python', 'is', 'a', 'high-level', 'programming',
            'language', '.'])

    def test_exclude_punc(self):
        assert_equal(self.tokenizer.tokenize(self.text, include_punc=False),
            ['Python', 'is', 'a', 'high-level', 'programming',
            'language'])

    def test_itokenize(self):
        gen = self.tokenizer.itokenize(self.text)
        assert_equal(next(gen), "Python")
        assert_equal(next(gen), "is")
--

class TestSentenceTokenizer(unittest.TestCase):

    def setUp(self):
        self.tokenizer = SentenceTokenizer()
        self.text = "Beautiful is better than ugly. Simple is better than complex."

    def test_tokenize(self):
        assert_equal(self.tokenizer.tokenize(self.text),
            ["Beautiful is better than ugly.", "Simple is better than complex."])

    def test_tokenize_with_multiple_punctuation(self):
        text = "Hello world. How do you do?! My name's Steve..."
        assert_equal(self.tokenizer.tokenize(text),
            ["Hello world.", "How do you do?!", "My name's Steve..."])
--
        assert_equal(tokens,
            ["OMG!", "I am soooo LOL!!!"])

    def test_itokenize(self):
        gen = self.tokenizer.itokenize(self.text)
        assert_equal(next(gen), "Beautiful is better than ugly.")
        assert_equal(next(gen), "Simple is better than complex.")
--

class TestPatternTagger(unittest.TestCase):

    def setUp(self):
        self.text = ("Simple is better than complex. "
                    "Complex is better than complicated.")
        self.tagger = textblob.taggers.PatternTagger()

    def test_init(self):
        tagger = textblob.taggers.PatternTagger()
        assert_true(isinstance(tagger, textblob.taggers.BaseTagger))

    def test_tag(self):
        tags = self.tagger.tag(self.text)
        assert_equal(tags,
            [('Simple', 'NN'), ('is', 'VBZ'), ('better', 'JJR'),
--
@attr("requires_numpy")
class TestNLTKTagger(unittest.TestCase):

    def setUp(self):
        self.text = ("Simple is better than complex. "
                    "Complex is better than complicated.")
        self.tagger = textblob.taggers.NLTKTagger()

    def test_tag(self):
        tags = self.tagger.tag(self.text)
        print(tags)
        assert_equal(tags,
--
class TestPerceptronTagger(unittest.TestCase):

    @attr("py27_only")
    def test_init_raises_deprecation_error(self):
        with assert_raises(DeprecationError):
            textblob.taggers.PerceptronTagger(load=False)

--


@attr("py27_only")
def test_cannot_instantiate_incomplete_tagger():
    with assert_raises(TypeError):
        BadTagger()


def _read_tagged(text, sep='|'):
    sentences = []
    for sent in text.split('\n'):
        tokens = []
--

class TestConllExtractor(unittest.TestCase):

    def setUp(self):
        self.extractor = ConllExtractor()
        self.text = '''
Python is a widely used general-purpose,
--
        self.sentence = "Python is a widely used general-purpose, high-level programming language"

    @attr('slow')
    def test_extract(self):
        noun_phrases = self.extractor.extract(self.text)
        assert_true("Python" in noun_phrases)
        assert_true("design philosophy" in noun_phrases)
        assert_true("code readability" in noun_phrases)

    @attr('slow')
    def test_parse_sentence(self):
        parsed = self.extractor._parse_sentence(self.sentence)
        assert_true(isinstance(parsed, nltk.tree.Tree))

    @attr('slow')
    def test_filter_insignificant(self):
        chunk = self.extractor._parse_sentence(self.sentence)
        tags = [tag for word, tag in chunk.leaves()]
        assert_true('DT' in tags)
--
    pass

@attr("py27_only")
def test_cannot_instantiate_incomplete_extractor():
    with assert_raises(TypeError):
        BadExtractor()

--

class TestFormats(unittest.TestCase):

    def setUp(self):
        pass

    def test_detect_csv(self):
        format = formats.detect(CSV_FILE)
        assert_equal(format, formats.CSV)

    def test_detect_json(self):
        format = formats.detect(JSON_FILE)
        assert_equal(format, formats.JSON)

    def test_available(self):
        assert_true('csv' in formats.AVAILABLE.keys())
        assert_true('json' in formats.AVAILABLE.keys())
        assert_true('tsv' in formats.AVAILABLE.keys())

class TestDelimitedFormat(unittest.TestCase):

    def test_delimiter_defaults_to_comma(self):
        assert_equal(formats.DelimitedFormat.delimiter, ",")

    def test_detect(self):
        with open(CSV_FILE, 'r') as fp:
            stream = fp.read()
            assert_true(formats.DelimitedFormat.detect(stream))
--

class TestCSV(unittest.TestCase):

    def test_read_from_filename(self):
        data = formats.CSV(CSV_FILE)

    def test_detect(self):
        with open(CSV_FILE, 'r') as fp:
            stream = fp.read()
            assert_true(formats.CSV.detect(stream))
--

class TestTSV(unittest.TestCase):

    def test_read_from_filename(self):
        data = formats.TSV(TSV_FILE)

    def test_detect(self):
        with open(TSV_FILE, 'r') as fp:
            stream = fp.read()
            assert_true(formats.TSV.detect(stream))
--

class TestJSON(unittest.TestCase):

    def test_read_from_filename(self):
        formats.JSON(JSON_FILE)

    def test_detect(self):
        with open(JSON_FILE, 'r') as fp:
            stream = fp.read()
            assert_true(formats.JSON.detect(stream))
--
            stream = fp.read()
            assert_false(formats.JSON.detect(stream))

    def test_to_iterable(self):
        d = formats.JSON(JSON_FILE)
        logging.debug(d.dict)
        data = d.to_iterable()
--
from textblob.utils import lowerstrip, strip_punc

class UtilsTests(TestCase):
    def setUp(self):
        self.text = "this. Has. Punctuation?! "

    def test_strip_punc(self):
        assert_equal(strip_punc(self.text),
                    'this. Has. Punctuation')

    def test_strip_punc_all(self):
        assert_equal(strip_punc(self.text, all=True),
                    'this Has Punctuation')

    def test_lowerstrip(self):
        assert_equal(lowerstrip(self.text),
                    'this. has. punctuation')

--

class ImportTest(unittest.TestCase):

    def assert_deprecated_import(self, module):
        '''Assert that importing a module raises a DeprecationWarning.

        :param module: A string, the module name.
--
            __import__(module)
        assert_true(issubclass(w[-1].category, DeprecationWarning))

    def test_old_text_blob(self):
        self.assert_deprecated_import("text.blob")

if __name__ == '__main__':
--

class TestPatternSentiment(unittest.TestCase):

    def setUp(self):
        self.analyzer = PatternAnalyzer()

    def test_kind(self):
        assert_equal(self.analyzer.kind, CONTINUOUS)

    def test_analyze(self):
        p1 = "I feel great this morning."
        n1 = "This is a terrible car."
        assert_true(self.analyzer.analyze(p1)[0] > 0)
--

class TestNaiveBayesAnalyzer(unittest.TestCase):

    def setUp(self):
        self.analyzer = NaiveBayesAnalyzer()

    def test_kind(self):
        assert_equal(self.analyzer.kind, DISCRETE)

    @attr('slow')
    def test_analyze(self):
        p1 = 'I feel great this morning.'
        n1 = 'This is a terrible car.'
        p1_result = self.analyzer.analyze(p1)
--
        assert_true(isinstance(p1_result[2], float))
        assert_about_equal(p1_result[1] + p1_result[2], 1)

def assert_about_equal(first, second, places=4):
    return assert_equal(round(first, places), second)

if __name__ == '__main__':
--
class Tokenizer(object):

    @requires_nltk_corpus
    def tag(self, text):
        raise LookupError


@attr("py27_only")
def test_decorator_raises_missing_corpus_exception():
    t = Tokenizer()
    with assert_raises(MissingCorpusException):
        t.tag('hello world')
--

class WordListTest(TestCase):

    def setUp(self):
        self.words = 'Beautiful is better than ugly'.split()
        self.mixed = ['dog', 'dogs', 'blob', 'Blobs', 'text']

    def test_len(self):
        wl = tb.WordList(['Beautiful', 'is', 'better'])
        assert_equal(len(wl), 3)

    def test_slicing(self):
        wl = tb.WordList(self.words)
        first = wl[0]
        assert_true(isinstance(first, tb.Word))
--
        assert_true(isinstance(dogs, tb.WordList))
        assert_equal(dogs, tb.WordList(['Beautiful', 'is']))

    def test_repr(self):
        wl = tb.WordList(['Beautiful', 'is', 'better'])
        if PY2:
            assert_equal(repr(wl), "WordList([u'Beautiful', u'is', u'better'])")
        else:
            assert_equal(repr(wl), "WordList(['Beautiful', 'is', 'better'])")

    def test_str(self):
        wl = tb.WordList(self.words)
        assert_equal(str(wl), str(self.words))

    def test_singularize(self):
        wl = tb.WordList(['dogs', 'cats', 'buffaloes', 'men', 'mice'])
        assert_equal(wl.singularize(), tb.WordList(['dog', 'cat', 'buffalo', 'man', 'mouse'
                     ]))

    def test_pluralize(self):
        wl = tb.WordList(['dog', 'cat', 'buffalo'])
        assert_equal(wl.pluralize(), tb.WordList(['dogs', 'cats', 'buffaloes']))

    @attr('slow')
    def test_lemmatize(self):
        wl = tb.WordList(["cat", "dogs", "oxen"])
        assert_equal(wl.lemmatize(), tb.WordList(['cat', 'dog', 'ox']))

    def test_upper(self):
        wl = tb.WordList(self.words)
        assert_equal(wl.upper(), tb.WordList([w.upper() for w in self.words]))

    def test_lower(self):
        wl = tb.WordList(['Zen', 'oF', 'PYTHON'])
        assert_equal(wl.lower(), tb.WordList(['zen', 'of', 'python']))

    def test_count(self):
        wl = tb.WordList(['monty', 'python', 'Python', 'Monty'])
        assert_equal(wl.count('monty'), 2)
        assert_equal(wl.count('monty', case_sensitive=True), 1)
        assert_equal(wl.count('mon'), 0)

    def test_convert_to_list(self):
        wl = tb.WordList(self.words)
        assert_equal(list(wl), self.words)

    def test_append(self):
        wl = tb.WordList(['dog'])
        wl.append("cat")
        assert_true(isinstance(wl[1], tb.Word))
        wl.append(('a', 'tuple'))
        assert_true(isinstance(wl[2], tuple))

    def test_extend(self):
        wl = tb.WordList(["cats", "dogs"])
        wl.extend(["buffalo", 4])
        assert_true(isinstance(wl[2], tb.Word))
--

class SentenceTest(TestCase):

    def setUp(self):
        self.raw_sentence = \
            'Any place with frites and Belgian beer has my vote.'
        self.sentence = tb.Sentence(self.raw_sentence)

    def test_repr(self):
        # In Py2, repr returns bytestring
        if PY2:
            assert_equal(repr(self.sentence),
--
        else:
            assert_equal(repr(self.sentence), 'Sentence("{0}")'.format(self.raw_sentence))

    def test_stripped_sentence(self):
        assert_equal(self.sentence.stripped,
                     'any place with frites and belgian beer has my vote')

    def test_len(self):
        assert_equal(len(self.sentence), len(self.raw_sentence))

    @attr('slow')
    def test_dict(self):
        sentence_dict = self.sentence.dict
        assert_equal(sentence_dict, {
            'raw': self.raw_sentence,
--
            'noun_phrases': self.sentence.noun_phrases,
            })

    def test_pos_tags(self):
        then1 = datetime.now()
        tagged = self.sentence.pos_tags
        now1 = datetime.now()
--
        )

    @attr('slow')
    def test_noun_phrases(self):
        nps = self.sentence.noun_phrases
        assert_equal(nps, ['belgian beer'])

    def test_words_are_word_objects(self):
        words = self.sentence.words
        assert_true(isinstance(words[0], tb.Word))
        assert_equal(words[1].pluralize(), 'places')

    def test_string_equality(self):
        assert_equal(self.sentence, 'Any place with frites and Belgian beer has my vote.')

    @attr("requires_internet")
    def test_translate(self):
        blob = tb.Sentence("This is a sentence.")
        translated = blob.translate(to="es")
        assert_true(isinstance(translated, tb.Sentence))
        assert_equal(translated, "Esta es una frase.")

    def test_correct(self):
        blob = tb.Sentence("I havv bad speling.")
        assert_true(isinstance(blob.correct(), tb.Sentence))
        assert_equal(blob.correct(), tb.Sentence("I have bad spelling."))

    @attr('requires_internet')
    def test_translate_detects_language_by_default(self):
        blob = tb.TextBlob(unicode("ذات سيادة كاملة"))
        assert_equal(blob.translate(), "With full sovereignty")


class TextBlobTest(TestCase):

    def setUp(self):
        self.text = \
            """Beautiful is better than ugly.
Explicit is better than implicit.
--
        self.short = "Beautiful is better than ugly. "
        self.short_blob = tb.TextBlob(self.short)

    def test_init(self):
        blob = tb.TextBlob('Wow I love this place. It really rocks my socks!!!')
        assert_equal(len(blob.sentences), 2)
        assert_equal(blob.sentences[1].stripped, 'it really rocks my socks')
--
        # Must initialize with a string
        assert_raises(TypeError, tb.TextBlob.__init__, ['invalid'])

    def test_string_equality(self):
        blob = tb.TextBlob("Textblobs should be equal to strings.")
        assert_equal(blob, "Textblobs should be equal to strings.")

    def test_string_comparison(self):
        blob = tb.TextBlob("apple")
        assert_true(blob < "banana")
        assert_true(blob > 'aardvark')

    def test_hash(self):
        blob = tb.TextBlob('apple')
        assert_equal(hash(blob), hash('apple'))
        assert_not_equal(hash(blob), hash('banana'))

    def test_stripped(self):
        blob = tb.TextBlob("Um... well this ain't right.!..")
        assert_equal(blob.stripped, "um well this aint right")

    def test_ngrams(self):
        blob = tb.TextBlob("I am eating a pizza.")
        three_grams = blob.ngrams()
        assert_equal(three_grams, [
--
        ])

    @attr("py27_only")
    def test_clean_html(self):
        html = '<b>Python</b> is a widely used <a href="/wiki/General-purpose_programming_language" title="General-purpose programming language">general-purpose</a>, <a href="/wiki/High-level_programming_language" title="High-level programming language">high-level programming language</a>.'
        with assert_raises(NotImplementedError):
            tb.TextBlob(html, clean_html=True)

    def test_sentences(self):
        blob = self.blob
        assert_equal(len(blob.sentences), 19)
        assert_true(isinstance(blob.sentences[0], tb.Sentence))

    def test_sentiment_of_foreign_text(self):
        blob = tb.TextBlob(u'Nous avons cherch\xe9 un motel dans la r\xe9gion de '
            'Madison, mais les motels ne sont pas nombreux et nous avons '
            'finalement choisi un Motel 6, attir\xe9s par le bas '
            'prix de la chambre.')
        assert_true(isinstance(blob.sentiment[0], float))

    def test_iter(self):
        for i, letter in enumerate(self.short_blob):
            assert_equal(letter, self.short[i])

    def test_raw_sentences(self):
        blob = tb.TextBlob(self.text)
        assert_equal(len(blob.raw_sentences), 19)
        assert_equal(blob.raw_sentences[0], "Beautiful is better than ugly.")

    def test_blob_with_no_sentences(self):
        text = "this isn't really a sentence it's just a long string of words"
        blob = tb.TextBlob(text)
        # the blob just has one sentence
--
        assert_equal(blob.sentences[0].start_index, 0)
        assert_equal(blob.sentences[0].end_index, len(text))

    def test_len(self):
        blob = tb.TextBlob('lorem ipsum')
        assert_equal(len(blob), len('lorem ipsum'))

    def test_repr(self):
        blob1 = tb.TextBlob('lorem ipsum')
        if PY2:
            assert_equal(repr(blob1), b"TextBlob(\"{0}\")".format(binary_type('lorem ipsum')))
        else:
            assert_equal(repr(blob1), "TextBlob(\"{0}\")".format('lorem ipsum'))

    def test_cmp(self):
        blob1 = tb.TextBlob('lorem ipsum')
        blob2 = tb.TextBlob('lorem ipsum')
        blob3 = tb.TextBlob('dolor sit amet')
--
        assert_true(blob3 < blob2)  # test <
        assert_true(blob3 <= blob2)  # test <=

    def test_invalid_comparison(self):
        blob = tb.TextBlob("one")
        if PY2:
            # invalid comparison returns False
--
            with assert_raises(TypeError):
                blob < 2

    def test_words(self):
        blob = tb.TextBlob('Beautiful is better than ugly. '
                            'Explicit is better than implicit.')
        assert_true(isinstance(blob.words, tb.WordList))
--
            'Just', 'a', 'bundle', 'of', 'words'
            ]))

    def test_words_includes_apostrophes_in_contractions(self):
        blob = tb.TextBlob("Let's test this.")
        assert_equal(blob.words, tb.WordList(['Let', "'s", "test", "this"]))
        blob2 = tb.TextBlob("I can't believe it's not butter.")
        assert_equal(blob2.words, tb.WordList(['I', 'ca', "n't", "believe",
                                            'it', "'s", "not", "butter"]))

    def test_pos_tags(self):
        blob = tb.TextBlob('Simple is better than complex. '
                            'Complex is better than complicated.')
        assert_equal(blob.pos_tags, [
--
            ('complicated', 'VBN'),
            ])

    def test_tags(self):
        assert_equal(self.blob.tags, self.blob.pos_tags)

    def test_tagging_nonascii(self):
        b = tb.TextBlob('Learn how to make the five classic French mother sauces: '
                        'Béchamel, Tomato Sauce, Espagnole, Velouté and Hollandaise.')
        tags = b.tags
        assert_true(isinstance(tags[0][0], unicode))


    def test_pos_tags_includes_one_letter_articles(self):
        blob = tb.TextBlob("This is a sentence.")
        assert_equal(blob.pos_tags[2][0], 'a')

    @attr('slow')
    def test_np_extractor_defaults_to_fast_tagger(self):
        text = "Python is a high-level scripting language."
        blob1 = tb.TextBlob(text)
        assert_true(isinstance(blob1.np_extractor, FastNPExtractor))

    def test_np_extractor_is_shared_among_instances(self):
        blob1 = tb.TextBlob("This is one sentence")
        blob2 = tb.TextBlob("This is another sentence")
        assert_true(blob1.np_extractor is blob2.np_extractor)

    @attr('slow')
    def test_can_use_different_np_extractors(self):
        e = ConllExtractor()
        text = "Python is a high-level scripting language."
        blob = tb.TextBlob(text)
        blob.np_extractor = e
        assert_true(isinstance(blob.np_extractor, ConllExtractor))

    def test_can_use_different_sentanalyzer(self):
        blob = tb.TextBlob("I love this car", analyzer=NaiveBayesAnalyzer())
        assert_true(isinstance(blob.analyzer, NaiveBayesAnalyzer))

    @attr("slow")
    def test_discrete_sentiment(self):
        blob = tb.TextBlob("I feel great today.", analyzer=NaiveBayesAnalyzer())
        assert_equal(blob.sentiment[0], 'pos')

    def test_can_get_subjectivity_and_polarity_with_different_analyzer(self):
        blob = tb.TextBlob("I love this car.", analyzer=NaiveBayesAnalyzer())
        pattern = PatternAnalyzer()
        assert_equal(blob.polarity, pattern.analyze(str(blob))[0])
        assert_equal(blob.subjectivity, pattern.analyze(str(blob))[1])

    def test_pos_tagger_defaults_to_pattern(self):
        blob = tb.TextBlob("some text")
        assert_true(isinstance(blob.pos_tagger, PatternTagger))

    def test_pos_tagger_is_shared_among_instances(self):
        blob1 = tb.TextBlob("This is one sentence")
        blob2 = tb.TextBlob("This is another sentence.")
        assert_true(blob1.pos_tagger is blob2.pos_tagger)


    def test_can_use_different_pos_tagger(self):
        tagger = NLTKTagger()
        blob = tb.TextBlob("this is some text", pos_tagger=tagger)
        assert_true(isinstance(blob.pos_tagger, NLTKTagger))

    @attr('slow')
    def test_can_pass_np_extractor_to_constructor(self):
        e = ConllExtractor()
        blob = tb.TextBlob('Hello world!', np_extractor=e)
        assert_true(isinstance(blob.np_extractor, ConllExtractor))

    def test_getitem(self):
        blob = tb.TextBlob('lorem ipsum')
        assert_equal(blob[0], 'l')
        assert_equal(blob[0:5], tb.TextBlob('lorem'))

    def test_upper(self):
        blob = tb.TextBlob('lorem ipsum')
        assert_true(is_blob(blob.upper()))
        assert_equal(blob.upper(), tb.TextBlob('LOREM IPSUM'))

    def test_upper_and_words(self):
        blob = tb.TextBlob('beautiful is better')
        assert_equal(blob.upper().words, tb.WordList(['BEAUTIFUL', 'IS', 'BETTER'
                     ]))

    def test_lower(self):
        blob = tb.TextBlob('Lorem Ipsum')
        assert_true(is_blob(blob.lower()))
        assert_equal(blob.lower(), tb.TextBlob('lorem ipsum'))

    def test_find(self):
        text = 'Beautiful is better than ugly.'
        blob = tb.TextBlob(text)
        assert_equal(blob.find('better', 5, len(blob)), text.find('better', 5,
                     len(text)))

    def test_rfind(self):
        text = 'Beautiful is better than ugly. '
        blob = tb.TextBlob(text)
        assert_equal(blob.rfind('better'), text.rfind('better'))

    def test_startswith(self):
        blob = tb.TextBlob(self.text)
        assert_true(blob.startswith('Beautiful'))
        assert_true(blob.starts_with('Beautiful'))

    def test_endswith(self):
        blob = tb.TextBlob(self.text)
        assert_true(blob.endswith('of those!'))
        assert_true(blob.ends_with('of those!'))

    def test_split(self):
        blob = tb.TextBlob('Beautiful is better')
        assert_equal(blob.split(), tb.WordList(['Beautiful', 'is', 'better']))

    def test_title(self):
        blob = tb.TextBlob('Beautiful is better')
        assert_equal(blob.title(), tb.TextBlob('Beautiful Is Better'))

    def test_format(self):
        blob = tb.TextBlob('1 + 1 = {0}')
        assert_equal(blob.format(1 + 1), tb.TextBlob('1 + 1 = 2'))
        assert_equal('1 + 1 = {0}'.format(tb.TextBlob('2')), '1 + 1 = 2')

    def test_using_indices_for_slicing(self):
        blob = tb.TextBlob("Hello world. How do you do?")
        sent1, sent2 = blob.sentences
        assert_equal(blob[sent1.start:sent1.end], tb.TextBlob(str(sent1)))
        assert_equal(blob[sent2.start:sent2.end], tb.TextBlob(str(sent2)))


    def test_indices_with_only_one_sentences(self):
        blob = tb.TextBlob("Hello world.")
        sent1 = blob.sentences[0]
        assert_equal(blob[sent1.start:sent1.end], tb.TextBlob(str(sent1)))

    def test_indices_with_multiple_puncutations(self):
        blob = tb.TextBlob("Hello world. How do you do?! This has an ellipses...")
        sent1, sent2, sent3 = blob.sentences
        assert_equal(blob[sent2.start:sent2.end], tb.TextBlob("How do you do?!"))
        assert_equal(blob[sent3.start:sent3.end], tb.TextBlob("This has an ellipses..."))

    def test_indices_short_names(self):
        blob = tb.TextBlob(self.text)
        last_sentence = blob.sentences[len(blob.sentences) - 1]
        assert_equal(last_sentence.start, last_sentence.start_index)
        assert_equal(last_sentence.end, last_sentence.end_index)

    def test_replace(self):
        blob = tb.TextBlob('textblob is a blobby blob')
        assert_equal(blob.replace('blob', 'bro'),
                     tb.TextBlob('textbro is a broby bro'))
        assert_equal(blob.replace('blob', 'bro', 1),
                     tb.TextBlob('textbro is a blobby blob'))

    def test_join(self):
        l = ['explicit', 'is', 'better']
        wl = tb.WordList(l)
        assert_equal(tb.TextBlob(' ').join(l), tb.TextBlob('explicit is better'))
        assert_equal(tb.TextBlob(' ').join(wl), tb.TextBlob('explicit is better'))

    def test_multiple_punctuation_at_end_of_sentence(self):
        '''Test sentences that have multiple punctuation marks
        at the end of the sentence.'''
        blob = tb.TextBlob('Get ready! This has an ellipses...')
--
        assert_equal(blob2.sentences[1].raw, 'I am soooo LOL!!!')

    @attr('slow')
    def test_blob_noun_phrases(self):
        noun_phrases = self.np_test_blob.noun_phrases
        assert_true('python' in noun_phrases)
        assert_true('design philosophy' in noun_phrases)

    def test_word_counts(self):
        blob = tb.TextBlob('Buffalo buffalo ate my blue buffalo.')
        assert_equal(dict(blob.word_counts), {
                'buffalo': 3,
--
        assert_equal(blob2.words.count('special', case_sensitive=True), 1)

    @attr('slow')
    def test_np_counts(self):
        # Add some text so that we have a noun phrase that
        # has a frequency greater than 1
        noun_phrases = self.np_test_blob.noun_phrases
--
        assert_equal(noun_phrases.count('cpython'), 2)
        assert_equal(noun_phrases.count('not found'), 0)

    def test_add(self):
        blob1 = tb.TextBlob('Hello, world! ')
        blob2 = tb.TextBlob('Hola mundo!')
        # Can add two text blobs
--
        # operands must be strings
        assert_raises(TypeError, blob1.__add__, ['hello'])

    def test_unicode(self):
        blob = tb.TextBlob(self.text)
        assert_equal(str(blob), str(self.text))

    def test_strip(self):
        text = 'Beautiful is better than ugly. '
        blob = tb.TextBlob(text)
        assert_true(is_blob(blob))
        assert_equal(blob.strip(), tb.TextBlob(text.strip()))

    def test_strip_and_words(self):
        blob = tb.TextBlob('Beautiful is better! ')
        assert_equal(blob.strip().words, tb.WordList(['Beautiful', 'is', 'better'
                     ]))

    def test_index(self):
        blob = tb.TextBlob(self.text)
        assert_equal(blob.index('Namespaces'), self.text.index('Namespaces'))

    def test_sentences_after_concatenation(self):
        blob1 = tb.TextBlob('Beautiful is better than ugly. ')
        blob2 = tb.TextBlob('Explicit is better than implicit.')

        concatenated = blob1 + blob2
        assert_equal(len(concatenated.sentences), 2)

    def test_sentiment(self):
        positive = tb.TextBlob('This is the best, most amazing '
                            'text-processing library ever!')
        assert_true(positive.sentiment[0] > 0.0)
--
        zen = tb.TextBlob(self.text)
        assert_equal(round(zen.sentiment[0], 1), 0.2)

    def test_subjectivity(self):
        positive = tb.TextBlob("Oh my god this is so amazing! I'm so happy!")
        assert_true(isinstance(positive.subjectivity, float))
        assert_true(positive.subjectivity > 0)

    def test_polarity(self):
        positive = tb.TextBlob("Oh my god this is so amazing! I'm so happy!")
        assert_true(isinstance(positive.polarity, float))
        assert_true(positive.polarity > 0)

    def test_sentiment_of_emoticons(self):
        b1 = tb.TextBlob("Faces have values =)")
        b2 = tb.TextBlob("Faces have values")
        assert_true(b1.sentiment[0] > b2.sentiment[0])

    @attr('py27_only')
    def test_bad_init(self):
        with assert_raises(TypeError):
            tb.TextBlob(['bad'])
        with assert_raises(ValueError):
--
            tb.TextBlob("this is fine",
                        pos_tagger="this is not fine")

    def test_in(self):
        blob = tb.TextBlob('Beautiful is better than ugly. ')
        assert_true('better' in blob)
        assert_true('fugly' not in blob)

    @attr('slow')
    def test_json(self):
        blob = tb.TextBlob('Beautiful is better than ugly. ')
        assert_equal(blob.json, blob.to_json())
        blob_dict = json.loads(blob.json)[0]
--
        assert_almost_equal(blob_dict['subjectivity'],
                            blob.sentences[0].subjectivity, places=4)

    def test_words_are_word_objects(self):
        words = self.blob.words
        assert_true(isinstance(words[0], tb.Word))

    def test_words_have_pos_tags(self):
        blob = tb.TextBlob('Simple is better than complex. '
                            'Complex is better than complicated.')
        first_word, first_tag = blob.pos_tags[0]
        assert_true(isinstance(first_word, tb.Word))
        assert_equal(first_word.pos_tag, first_tag)

    def test_tokenizer_defaults_to_word_tokenizer(self):
        assert_true(isinstance(self.blob.tokenizer, WordTokenizer))

    def test_tokens_property(self):
        assert_true(self.blob.tokens,
            tb.WordList(WordTokenizer().tokenize(self.text)))

    def test_can_use_an_different_tokenizer(self):
        tokenizer = nltk.tokenize.TabTokenizer()
        blob = tb.TextBlob("This is\ttext.", tokenizer=tokenizer)
        assert_equal(blob.tokens, tb.WordList(["This is", "text."]))

    def test_tokenize_method(self):
        tokenizer = nltk.tokenize.TabTokenizer()
        blob = tb.TextBlob("This is\ttext.")
        # If called without arguments, should default to WordTokenizer
        assert_equal(blob.tokenize(), tb.WordList(["This", "is", "text", "."]))
        # Pass in the TabTokenizer
        assert_equal(blob.tokenize(tokenizer), tb.WordList(["This is", "text."]))

    @attr("requires_internet")
    def test_translate(self):
        blob = tb.TextBlob("This is a sentence.")
        translated = blob.translate(to="es")
        assert_true(isinstance(translated, tb.TextBlob))
--
        assert_equal(to_en, "This is a phrase .")

    @attr("requires_internet")
    def test_translate_non_ascii(self):
        blob = tb.TextBlob(unicode("ذات سيادة كاملة"))
        translated = blob.translate(from_lang="ar", to="en")
        assert_equal(translated, "With full sovereignty")
--
        assert_equal(translated, "Beautiful is better than ugly")

    @attr("requires_internet")
    def test_detect(self):
        es_blob = tb.TextBlob("Hola")
        assert_equal(es_blob.detect_language(), "es")
        en_blob = tb.TextBlob("Hello")
        assert_equal(en_blob.detect_language(), "en")

    @attr("requires_internet")
    def test_detect_non_ascii(self):
        blob = tb.TextBlob(unicode("ذات سيادة كاملة"))
        assert_equal(blob.detect_language(), "ar")

    def test_correct(self):
        blob = tb.TextBlob("I havv bad speling.")
        assert_true(isinstance(blob.correct(), tb.TextBlob))
        assert_equal(blob.correct(), tb.TextBlob("I have bad spelling."))
--
        blob4 = tb.TextBlob("?")
        assert_equal(blob4.correct(), "?")

    def test_parse(self):
        blob = tb.TextBlob("And now for something completely different.")
        assert_equal(blob.parse(), PatternParser().parse(blob.string))

    @attr("py27_only")
    def test_passing_bad_init_params(self):
        tagger = PatternTagger()
        with assert_raises(ValueError):
            tb.TextBlob("blah", parser=tagger)
--
        with assert_raises(ValueError):
            tb.TextBlob("blah", pos_tagger=analyzer)

    def test_classify(self):
        blob = tb.TextBlob("This is an amazing library. What an awesome classifier!",
            classifier=classifier)
        assert_equal(blob.classify(), 'pos')
--
            assert_equal(s.classify(), 'pos')

    @attr("py27_only")
    def test_classify_without_classifier(self):
        blob = tb.TextBlob("This isn't gonna be good")
        with assert_raises(NameError):
            blob.classify()

class WordTest(TestCase):

    def setUp(self):
        self.cat = tb.Word('cat')
        self.cats = tb.Word('cats')

    def test_init(self):
        tb.Word("cat")
        assert_true(isinstance(self.cat, tb.Word))
        word = tb.Word('cat', 'NN')
        assert_equal(word.pos_tag, 'NN')

    def test_singularize(self):
        singular = self.cats.singularize()
        assert_equal(singular, 'cat')
        assert_equal(self.cat.singularize(), 'cat')
        assert_true(isinstance(self.cat.singularize(), tb.Word))

    def test_pluralize(self):
        plural = self.cat.pluralize()
        assert_equal(self.cat.pluralize(), 'cats')
        assert_true(isinstance(plural, tb.Word))

    def test_repr(self):
        assert_equal(repr(self.cat), repr("cat"))

    def test_str(self):
        assert_equal(str(self.cat), 'cat')

    def test_has_str_methods(self):
        assert_equal(self.cat.upper(), "CAT")
        assert_equal(self.cat.lower(), "cat")
        assert_equal(self.cat[0:2], 'ca')

    @attr('requires_internet')
    def test_translate(self):
        assert_equal(tb.Word("cat").translate(to="es"), "gato")

    @attr('requires_internet')
    def test_translate_without_from_lang(self):
        assert_equal(tb.Word('hola').translate(), tb.Word('hello'))

    @attr('requires_internet')
    def test_detect_language(self):
        assert_equal(tb.Word("bonjour").detect_language(), 'fr')

    def test_spellcheck(self):
        blob = tb.Word("speling")
        suggestions = blob.spellcheck()
        assert_equal(suggestions[0][0], "spelling")

    def test_spellcheck_special_cases(self):
        # Punctuation
        assert_equal(tb.Word("!").spellcheck(), [("!", 1.0)])
        # Numbers
--
        assert_equal(tb.Word("A").spellcheck(), [("A", 1.0)])
        assert_equal(tb.Word("a").spellcheck(), [("a", 1.0)])

    def test_correct(self):
        w = tb.Word('speling')
        correct = w.correct()
        assert_equal(correct, tb.Word('spelling'))
        assert_true(isinstance(correct, tb.Word))

    @attr('slow')
    def test_lemma(self):
        w = tb.Word("cars")
        assert_equal(w.lemma, "car")
        w = tb.Word("wolves")
        assert_equal(w.lemma, "wolf")


    def test_synsets(self):
        w = tb.Word("car")
        assert_true(isinstance(w.synsets, (list, tuple)))
        assert_true(isinstance(w.synsets[0], Synset))

    def test_synsets_with_pos_argument(self):
        w = tb.Word("work")
        noun_syns = w.get_synsets(pos=wn.NOUN)
        for synset in noun_syns:
            assert_equal(synset.pos, wn.NOUN)

    def test_definitions(self):
        w = tb.Word("octopus")
        for definition in w.definitions:
            print(type(definition))
            assert_true(isinstance(definition, basestring))

    def test_define(self):
        w = tb.Word("hack")
        synsets = w.get_synsets(wn.NOUN)
        definitions = w.define(wn.NOUN)
        assert_equal(len(synsets), len(definitions))


class TestWordnetInterface(TestCase):

    def setUp(self):
        pass

    def test_synset(self):
        syn = wn.Synset("dog.n.01")
        word = tb.Word("dog")
        assert_equal(word.synsets[0], syn)

    def test_lemma(self):
        lemma = wn.Lemma('eat.v.01.eat')
        word = tb.Word("eat")
        assert_equal(word.synsets[0].lemmas[0], lemma)
--

class BlobberTest(TestCase):

    def setUp(self):
        self.blobber = tb.Blobber()  # The default blobber

    def test_creates_blobs(self):
        blob1 = self.blobber("this is one blob")
        assert_true(isinstance(blob1, tb.TextBlob))
        blob2 = self.blobber("another blob")
        assert_equal(blob1.pos_tagger, blob2.pos_tagger)

    def test_default_tagger(self):
        blob = self.blobber("Some text")
        assert_true(isinstance(blob.pos_tagger, PatternTagger))

    def test_default_np_extractor(self):
        blob = self.blobber("Some text")
        assert_true(isinstance(blob.np_extractor, FastNPExtractor))

    def test_default_tokenizer(self):
        blob = self.blobber("Some text")
        assert_true(isinstance(blob.tokenizer, WordTokenizer))

    def test_str_and_repr(self):
        expected = "Blobber(tokenizer=WordTokenizer(), pos_tagger=PatternTagger(), np_extractor=FastNPExtractor(), analyzer=PatternAnalyzer(), parser=PatternParser(), classifier=None)"
        assert_equal(repr(self.blobber), expected)
        assert_equal(str(self.blobber), repr(self.blobber))

    def test_overrides(self):
        b = tb.Blobber(tokenizer=SentenceTokenizer(),
                        np_extractor=ConllExtractor())
        blob = b("How now? Brown cow?")
--
        # but aren't the same object
        assert_not_equal(blob, blob2)

    def test_override_analyzer(self):
        b = tb.Blobber(analyzer=NaiveBayesAnalyzer())
        blob = b("How now?")
        blob2 = b("Brown cow")
        assert_true(isinstance(blob.analyzer, NaiveBayesAnalyzer))
        assert_true(blob.analyzer is blob2.analyzer)

    def test_overrider_classifier(self):
        b = tb.Blobber(classifier=classifier)
        blob = b("I am so amazing")
        assert_equal(blob.classify(), 'pos')

def is_blob(obj):
    return isinstance(obj, tb.TextBlob)

if __name__ == '__main__':
--
    from distutils.core import setup
    from distutils.util import convert_path

    def _find_packages(where='.', exclude=()):
        """Return a list all Python packages found within directory 'where'

        'where' should be supplied as a "cross-platform" (i.e. URL-style) path; it
--

    find_packages = _find_packages

def find_version(fname):
    '''Attempts to find the version number in the file names fname.
    Raises RuntimeError if not found.
    '''
--
    status = subprocess.call(TEST_CMD, shell=True)
    sys.exit(status)

def read(fname):
    with open(fname) as fp:
        content = fp.read()
    return content
--
PYPY = "PyPy" in sys.version


def main():
    args = get_argv()
    success = nose.run(argv=args)
    sys.exit(0) if success else sys.exit(1)


def get_argv():
    args = [sys.argv[0], "tests", '--exclude', 'nltk']
    attr_conditions = []  # Use nose's attribselect plugin to filter tests
    if "force-all" in sys.argv:
--
    headers = {'User-Agent': ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) '
            'AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.168 Safari/535.19')}

    def translate(self, source, from_lang='en', to_lang='en', host=None, type_=None):
        '''Translate the source text from one language to another.'''
        if PY2:
            source = source.encode('utf-8')
--
        json5 = self._get_json5(url, host=host, type_=type_)
        return self._unescape(self._get_translation_from_json5(json5))

    def detect(self, source, host=None, type_=None):
        '''Detect the source text's language.'''
        if PY2:
            source = source.encode('utf-8')
--
        lang = self._get_language_from_json5(json5)
        return lang

    def _get_language_from_json5(self, content):
        match = self.detection_pattern.match(content)
        if not match:
            return None
        return match.group(1)

    def _get_translation_from_json5(self, content):
        result = ""
        pos = 2
        while True:
--
            pos = m.end()
        return result

    def _get_json5(self, url, host=None, type_=None):
        req = request.Request(url=url, headers=self.headers)
        if host or type_:
            req.set_proxy(host=host, type=type_)
--
        content = r.read()
        return content.decode('utf-8')

    def _unescape(self, text):
        return re.sub(r"\\.?", lambda x:eval('"%s"'%x.group(0)), text)
--
# -*- coding: utf-8 -*-
'''Default parsers to English for backwards compatibility so you can still do

>>> from textblob.parsers import PatternParser

--
# -*- coding: utf-8 -*-
'''Abstract base classes for models (taggers, noun phrase extractors, etc.)
which define the interface for descendant classes.

.. versionchanged:: 0.7.0
    All base classes are defined in the same module, ``textblob.base``.
'''
from __future__ import absolute_import
from abc import ABCMeta, abstractmethod
--
    ``tag()`` method.
    '''
    @abstractmethod
    def tag(self, text, tokenize=True):
        '''Return a list of tuples of the form (word, tag)
        for a given set of text.
        '''
--
    '''

    @abstractmethod
    def extract(self, text):
        '''Return a list of noun phrases (strings) for a body of text.'''
        return

--
    that returns a list of noun phrases as strings.
    '''
    @abstractmethod
    def tokenize(self, text):
        '''Return a list of tokens (strings) for a body of text.

        :rtype: list
        '''
        return

    def itokenize(self, text, *args, **kwargs):
        '''Return a generator that generates tokens "on-demand".

        .. versionadded:: 0.6.0
--
    '''
    kind = DISCRETE

    def __init__(self):
        self._trained = False

    def train(self):
        # Train me
        self._trained = True

    @abstractmethod
    def analyze(self, text):
        '''Return the result of of analysis. Typically returns either a
        tuple, float, or dictionary.'''
        # Lazily train the classifier
--
    descendants must implement a ``parse()`` method.
    '''
    @abstractmethod
    def parse(self, text):
        '''Parses the text.'''
        return
--

    '''Implements rich operators for an object.'''

    def _compare(self, other, method):
        try:
            return method(self._cmpkey(), other._cmpkey())
        except (AttributeError, TypeError):
--
            # so I can't compare with "other". Try the reverse comparison
            return NotImplemented

    def __lt__(self, other):
        return self._compare(other, lambda s, o: s < o)

    def __le__(self, other):
        return self._compare(other, lambda s, o: s <= o)

    def __eq__(self, other):
        return self._compare(other, lambda s, o: s == o)

    def __ge__(self, other):
        return self._compare(other, lambda s, o: s >= o)

    def __gt__(self, other):
        return self._compare(other, lambda s, o: s > o)

    def __ne__(self, other):
        return self._compare(other, lambda s, o: s != o)


--

    '''Allow blob objects to be comparable with both strings and blobs.'''

    def _compare(self, other, method):
        if isinstance(other, basestring):
            # Just compare with the other string
            return method(self._cmpkey(), other)
--
    of __str__ ensures consistent behavior between Python 2 and 3.
    '''

    def __repr__(self):
        '''Returns a string representation for debugging.'''
        class_name = self.__class__.__name__
        text = self.__unicode__().encode("utf-8") if PY2 else str(self)
--
                                        text=text)
        return binary_type(ret) if PY2 else ret

    def __str__(self):
        '''Returns a string representation used in print statements
        or str(my_blob).'''
        return self._strkey()

    def __len__(self):
        '''Returns the length of the raw text.'''
        return len(self._strkey())

    def __iter__(self):
        '''Makes the object iterable as if it were a string,
        iterating through the raw string's characters.
        '''
        return iter(self._strkey())

    def __contains__(self, sub):
        '''Implements the `in` keyword like a Python string.'''
        return sub in self._strkey()

    def __getitem__(self, index):
        '''Returns a  substring. If index is an integer, returns a Python
        string of a single character. If a range is given, e.g. `blob[3:5]`,
        a new instance of the class is returned.
--
            # Return a new blob object
            return self.__class__(self._strkey()[index])

    def find(self, sub, start=0, end=sys.maxsize):
        '''Behaves like the built-in str.find() method. Returns an integer,
        the index of the first occurrence of the substring argument sub in the
        sub-string given by [start:end].
        '''
        return self._strkey().find(sub, start, end)

    def rfind(self, sub, start=0, end=sys.maxsize):
        '''Behaves like the built-in str.rfind() method. Returns an integer,
        the index of he last (right-most) occurence of the substring argument
        sub in the sub-sequence given by [start:end].
        '''
        return self._strkey().rfind(sub, start, end)

    def index(self, sub, start=0, end=sys.maxsize):
        '''Like blob.find() but raise ValueError when the substring
        is not found.
        '''
        return self._strkey().index(sub, start, end)

    def rindex(self, sub, start=0, end=sys.maxsize):
        '''Like blob.rfind() but raise ValueError when substring is not
        found.
        '''
        return self._strkey().rindex(sub, start, end)

    def startswith(self, prefix, start=0, end=sys.maxsize):
        """Returns True if the blob starts with the given prefix."""
        return self._strkey().startswith(prefix, start, end)

    def endswith(self, suffix, start=0, end=sys.maxsize):
        """Returns True if the blob ends with the given suffix."""
        return self._strkey().endswith(suffix, start, end)

--
    starts_with = startswith
    ends_with = endswith

    def title(self):
        """Returns a blob object with the text in title-case."""
        return self.__class__(self._strkey().title())

    def format(self, *args, **kwargs):
        """Perform a string formatting operation, like the built-in
        `str.format(*args, **kwargs)`. Returns a blob object.
        """
        return self.__class__(self._strkey().format(*args, **kwargs))

    def split(self, sep=None, maxsplit=sys.maxsize):
        """Behaves like the built-in str.split().
        """
        return self._strkey().split(sep, maxsplit)

    def strip(self, chars=None):
        """Behaves like the built-in str.strip([chars]) method. Returns
        an object with leading and trailing whitespace removed.
        """
        return self.__class__(self._strkey().strip(chars))

    def upper(self):
        """Like str.upper(), returns new object with all upper-cased characters.
        """
        return self.__class__(self._strkey().upper())

    def lower(self):
        """Like str.lower(), returns new object with all lower-cased characters.
        """
        return self.__class__(self._strkey().lower())

    def join(self, iterable):
        """Behaves like the built-in `str.join(iterable)` method, except
        returns a blob object.

--
        """
        return self.__class__(self._strkey().join(iterable))

    def replace(self, old, new, count=sys.maxsize):
        """Return a new blob object with all the occurence of `old` replaced
        by `new`.
        """
--
    * separate periods that appear at the end of line
    '''

    def tokenize(self, text, include_punc=True):
        '''Return a list of word tokens.

        :param text: string of text.
        :param include_punc: (optional) whether to include punctuation as separate tokens. Default to True.
        '''
        tokens = nltk.tokenize.word_tokenize(text)
        if include_punc:
--
    '''

    @requires_nltk_corpus
    def tokenize(self, text):
        '''Return a list of sentences.'''
        ret = []
        sentences = nltk.tokenize.sent_tokenize(text)  # Initial tokenization
--
    :ivar _symbol: The node value corresponding to this
        ``Nonterminal``.  This value must be immutable and hashable.
    """
    def __init__(self, symbol):
        """
        Construct a new non-terminal from the given symbol.

--
        self._symbol = symbol
        self._hash = hash(symbol)

    def symbol(self):
        """
        Return the node value corresponding to this ``Nonterminal``.

--
        """
        return self._symbol

    def __eq__(self, other):
        """
        Return True if this non-terminal is equal to ``other``.  In
        particular, return True if ``other`` is a ``Nonterminal``
--
        """
        return type(self) == type(other) and self._symbol == other._symbol

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, Nonterminal):
            raise_unorderable_types("<", self, other)
        return self._symbol < other._symbol

    def __hash__(self):
        return self._hash

    def __repr__(self):
        """
        Return a string representation for this ``Nonterminal``.

--
        else:
            return '%s' % unicode_repr(self._symbol)

    def __str__(self):
        """
        Return a string representation for this ``Nonterminal``.

--
        else:
            return '%s' % unicode_repr(self._symbol)

    def __div__(self, rhs):
        """
        Return a new nonterminal whose symbol is ``A/B``, where ``A`` is
        the symbol for this nonterminal, and ``B`` is the symbol for rhs.
--
        """
        return Nonterminal('%s/%s' % (self._symbol, rhs._symbol))

def nonterminals(symbols):
    """
    Given a string containing a list of symbol names, return a list of
    ``Nonterminals`` constructed from those symbols.
--
class FeatStructNonterminal(FeatDict, Nonterminal):
    """A feature structure that's also a nonterminal.  It acts as its
    own symbol, and automatically freezes itself when hashed."""
    def __hash__(self):
        self.freeze()
        return FeatStruct.__hash__(self)
    def symbol(self):
        return self

def is_nonterminal(item):
    """
    :return: True if the item is a ``Nonterminal``.
    :rtype: bool
--
# Terminals
#################################################################

def is_terminal(item):
    """
    Return True if the item is a terminal, which currently is
    if it is hashable and not a ``Nonterminal``.
--
    :ivar _rhs: The right-hand side of the production.
    """

    def __init__(self, lhs, rhs):
        """
        Construct a new ``Production``.

--
        self._rhs = tuple(rhs)
        self._hash = hash((self._lhs, self._rhs))

    def lhs(self):
        """
        Return the left-hand side of this ``Production``.

--
        """
        return self._lhs

    def rhs(self):
        """
        Return the right-hand side of this ``Production``.

--
        """
        return self._rhs

    def __len__(self):
        """
        Return the length of the right-hand side.

--
        """
        return len(self._rhs)

    def is_nonlexical(self):
        """
        Return True if the right-hand side only contains ``Nonterminals``

--
        """
        return all(is_nonterminal(n) for n in self._rhs)

    def is_lexical(self):
        """
        Return True if the right-hand contain at least one terminal token.

--
        """
        return not self.is_nonlexical()

    def __str__(self):
        """
        Return a verbose string representation of the ``Production``.

--
        result += " ".join(unicode_repr(el) for el in self._rhs)
        return result

    def __repr__(self):
        """
        Return a concise string representation of the ``Production``.

--
        """
        return '%s' % self

    def __eq__(self, other):
        """
        Return True if this ``Production`` is equal to ``other``.

--
                self._lhs == other._lhs and
                self._rhs == other._rhs)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, Production):
            raise_unorderable_types("<", self, other)
        return (self._lhs, self._rhs) < (other._lhs, other._rhs)

    def __hash__(self):
        """
        Return a hash value for the ``Production``.

--
    A dependency grammar production.  Each production maps a single
    head word to an unordered list of one or more modifier words.
    """
    def __str__(self):
        """
        Return a verbose string representation of the ``DependencyProduction``.

--

    :see: ``Production``
    """
    def __init__(self, lhs, rhs, **prob):
        """
        Construct a new ``WeightedProduction``.

--
        ImmutableProbabilisticMixIn.__init__(self, **prob)
        Production.__init__(self, lhs, rhs)

    def __str__(self):
        return Production.__unicode__(self) + ' [%.6g]' % self.prob()

    def __eq__(self, other):
        return (type(self) == type(other) and
                self._lhs == other._lhs and
                self._rhs == other._rhs and
                self.prob() == other.prob())

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((self._lhs, self._rhs, self.prob()))

#################################################################
--
    If you need efficient key-based access to productions, you
    can use a subclass to implement it.
    """
    def __init__(self, start, productions, calculate_leftcorners=True):
        """
        Create a new context-free grammar, from the given start state
        and set of ``Production``s.

        :param start: The start symbol
        :type start: Nonterminal
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        :param calculate_leftcorners: False if we don't want to calculate the
            leftcorner relation. In that case, some optimized chart parsers won't work.
--
        if calculate_leftcorners:
            self._calculate_leftcorners()

    def _calculate_indexes(self):
        self._lhs_index = {}
        self._rhs_index = {}
        self._empty_index = {}
--
            # Lexical tokens in the right hand side.
            for token in prod._rhs:
                if is_terminal(token):
                    self._lexical_index.setdefault(token, set()).add(prod)

    def _calculate_leftcorners(self):
        # Calculate leftcorner relations, for use in optimized parsing.
        self._immediate_leftcorner_categories = dict((cat, set([cat])) for cat in self._categories)
        self._immediate_leftcorner_words = dict((cat, set()) for cat in self._categories)
--
                lc.update(self._immediate_leftcorner_words.get(left, set()))


    def start(self):
        """
        Return the start symbol of the grammar

--

    # tricky to balance readability and efficiency here!
    # can't use set operations as they don't preserve ordering
    def productions(self, lhs=None, rhs=None, empty=False):
        """
        Return the grammar productions, filtered by the left-hand side
        or the first item in the right-hand side.
--
            return [prod for prod in self._lhs_index.get(lhs, [])
                    if prod in self._rhs_index.get(rhs, [])]

    def leftcorners(self, cat):
        """
        Return the set of all nonterminals that the given nonterminal
        can start with, including itself.
--
        """
        return self._leftcorners.get(cat, set([cat]))

    def is_leftcorner(self, cat, left):
        """
        True if left is a leftcorner of cat, where left can be a
        terminal or a nonterminal.
--
            return any(left in self._immediate_leftcorner_words.get(parent, set())
                       for parent in self.leftcorners(cat))

    def leftcorner_parents(self, cat):
        """
        Return the set of all nonterminals for which the given category
        is a left corner. This is the inverse of the leftcorner relation.
--
        """
        return self._leftcorner_parents.get(cat, set([cat]))

    def check_coverage(self, tokens):
        """
        Check whether the grammar rules cover the given list of tokens.
        If not, then raise an exception.
--
            raise ValueError("Grammar does not cover some of the "
                             "input words: %r." % missing)

    def _calculate_grammar_forms(self):
        """
        Pre-calculate of which form(s) the grammar is.
        """
--
        self._all_unary_are_lexical = all(p.is_lexical() for p in prods
                                          if len(p) == 1)

    def is_lexical(self):
        """
        Return True if all productions are lexicalised.
        """
        return self._is_lexical

    def is_nonlexical(self):
        """
        Return True if all lexical rules are "preterminals", that is,
        unary rules which can be separated in a preprocessing step.
--
        """
        return self._is_nonlexical

    def min_len(self):
        """
        Return the right-hand side length of the shortest grammar production.
        """
        return self._min_len

    def max_len(self):
        """
        Return the right-hand side length of the longest grammar production.
        """
        return self._max_len

    def is_nonempty(self):
        """
        Return True if there are no empty productions.
        """
        return self._min_len > 0

    def is_binarised(self):
        """
        Return True if all productions are at most binary.
        Note that there can still be empty and unary productions.
        """
        return self._max_len <= 2

    def is_flexible_chomsky_normal_form(self):
        """
        Return True if all productions are of the forms
        A -> B C, A -> B, or A -> "s".
        """
        return self.is_nonempty() and self.is_nonlexical() and self.is_binarised()

    def is_chomsky_normal_form(self):
        """
        Return True if the grammar is of Chomsky Normal Form, i.e. all productions
        are of the form A -> B C, or A -> "s".
--
        return (self.is_flexible_chomsky_normal_form() and
                self._all_unary_are_lexical)

    def __repr__(self):
        return '<Grammar with %d productions>' % len(self._productions)

    def __str__(self):
        result = 'Grammar with %d productions' % len(self._productions)
        result += ' (start state = %r)' % self._start
        for production in self._productions:
--
    productions.  The set of terminals and nonterminals
    is implicitly specified by the productions.
    """
    def __init__(self, start, productions):
        """
        Create a new feature-based grammar, from the given start
        state and set of ``Productions``.

        :param start: The start symbol
        :type start: FeatStructNonterminal
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        """
        ContextFreeGrammar.__init__(self, start, productions)
--
    # indexed on the TYPE feature of the nonterminals.
    # This is calculated by the method _get_type_if_possible().

    def _calculate_indexes(self):
        self._lhs_index = {}
        self._rhs_index = {}
        self._empty_index = {}
--
            # Lexical tokens in the right hand side.
            for token in prod._rhs:
                if is_terminal(token):
                    self._lexical_index.setdefault(token, set()).add(prod)

    def productions(self, lhs=None, rhs=None, empty=False):
        """
        Return the grammar productions, filtered by the left-hand side
        or the first item in the right-hand side.
--
            return [prod for prod in self._lhs_index.get(self._get_type_if_possible(lhs), [])
                    if prod in self._rhs_index.get(self._get_type_if_possible(rhs), [])]

    def leftcorners(self, cat):
        """
        Return the set of all words that the given category can start with.
        Also called the "first set" in compiler construction.
        """
        raise NotImplementedError("Not implemented yet")

    def leftcorner_parents(self, cat):
        """
        Return the set of all categories for which the given category
        is a left corner.
        """
        raise NotImplementedError("Not implemented yet")

    def _get_type_if_possible(self, item):
        """
        Helper function which returns the ``TYPE`` feature of the ``item``,
        if it exists, otherwise it returns the ``item`` itself
--
    from ordinary strings.  This is to stop the ``FeatStruct``
    ``FOO[]`` from being compare equal to the terminal "FOO".
    """
    def __init__(self, value):
        self._value = value
        self._hash = hash(value)

    def __repr__(self):
        return '<%s>' % self._value

    def __eq__(self, other):
        return type(self) == type(other) and self._value == other._value

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, FeatureValueType):
            raise_unorderable_types("<", self, other)
        return self._value < other._value

    def __hash__(self):
        return self._hash


--
    productions.  Each production specifies a head/modifier relationship
    between a pair of words.
    """
    def __init__(self, productions):
        """
        Create a new dependency grammar, from the set of ``Productions``.

        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        """
        self._productions = productions

    def contains(self, head, mod):
        """
        :param head: A head word.
        :type head: str
--
                    return True
        return False

    def __contains__(self, head, mod):
        """
        Return True if this ``DependencyGrammar`` contains a
        ``DependencyProduction`` mapping 'head' to 'mod'.
--
        return False

    #   # should be rewritten, the set comp won't work in all comparisons
    # def contains_exactly(self, head, modlist):
    #   for production in self._productions:
    #       if(len(production._rhs) == len(modlist)):
    #           if(production._lhs == head):
--
    #   return False


    def __str__(self):
        """
        Return a verbose string representation of the ``DependencyGrammar``

--
            str += '\n  %s' % production
        return str

    def __repr__(self):
        """
        Return a concise string representation of the ``DependencyGrammar``
        """
--

    """

    def __init__(self, productions, events, tags):
        self._productions = productions
        self._events = events
        self._tags = tags

    def contains(self, head, mod):
        """
        Return True if this ``DependencyGrammar`` contains a
        ``DependencyProduction`` mapping 'head' to 'mod'.
--
                    return True
        return False

    def __str__(self):
        """
        Return a verbose string representation of the ``StatisticalDependencyGrammar``

--
            str += '\n %s:\t(%s)' % (tag_word, self._tags[tag_word])
        return str

    def __repr__(self):
        """
        Return a concise string representation of the ``StatisticalDependencyGrammar``
        """
--
    """
    EPSILON = 0.01

    def __init__(self, start, productions, calculate_leftcorners=True):
        """
        Create a new context-free grammar, from the given start state
        and set of ``WeightedProductions``.

        :param start: The start symbol
        :type start: Nonterminal
        :param productions: The list of productions that defines the grammar
        :type productions: list(Production)
        :raise ValueError: if the set of productions with any left-hand-side
            do not have probabilities that sum to a value within
--

# Contributed by Nathan Bodenstab <bodenstab@cslu.ogi.edu>

def induce_pcfg(start, productions):
    """
    Induce a PCFG grammar from a list of productions.

--

    :param start: The start symbol
    :type start: Nonterminal
    :param productions: The list of productions that defines the grammar
    :type productions: list(Production)
    """

--

# Parsing CFGs

def parse_cfg_production(input):
    """
    Return a list of context-free ``Productions``.
    """
    return parse_production(input, standard_nonterm_parser)

def parse_cfg(input, encoding=None):
    """
    Return the ``ContextFreeGrammar`` corresponding to the input string(s).

--

# Parsing Probabilistic CFGs

def parse_pcfg_production(input):
    """
    Return a list of PCFG ``WeightedProductions``.
    """
    return parse_production(input, standard_nonterm_parser, probabilistic=True)

def parse_pcfg(input, encoding=None):
    """
    Return a probabilistic ``WeightedGrammar`` corresponding to the
    input string(s).
--

# Parsing Feature-based CFGs

def parse_fcfg_production(input, fstruct_parser):
    """
    Return a list of feature-based ``Productions``.
    """
    return parse_production(input, fstruct_parser)

def parse_fcfg(input, features=None, logic_parser=None, fstruct_parser=None,
               encoding=None):
    """
    Return a feature structure based ``FeatureGrammar``.

    :param input: a grammar, either in the form of a string or else
        as a list of strings.
    :param features: a tuple of features (default: SLASH, TYPE)
    :param logic_parser: a parser for lambda-expressions,
        by default, ``LogicParser()``
    :param fstruct_parser: a feature structure parser
        (only if features and logic_parser is None)
    """
--
_TERMINAL_RE = re.compile(r'( "[^"]+" | \'[^\']+\' ) \s*', re.VERBOSE)
_DISJUNCTION_RE = re.compile(r'\| \s*', re.VERBOSE)

def parse_production(line, nonterm_parser, probabilistic=False):
    """
    Parse a grammar rule, given as a string, and return
    a list of productions.
--
        return [Production(lhs, rhs) for rhs in rhsides]


def parse_grammar(input, nonterm_parser, probabilistic=False, encoding=None):
    """
    Return a pair consisting of a starting category and a list of
    ``Productions``.
--

_STANDARD_NONTERM_RE = re.compile('( [\w/][\w/^<>-]* ) \s*', re.VERBOSE)

def standard_nonterm_parser(string, pos):
    m = _STANDARD_NONTERM_RE.match(string, pos)
    if not m: raise ValueError('Expected a nonterminal, found: '
                               + string[pos:])
--
                             re.VERBOSE)
_SPLIT_DG_RE = re.compile(r'''('[^']'|[-=]+>|"[^"]+"|'[^']+'|\|)''')

def parse_dependency_grammar(s):
    productions = []
    for linenum, line in enumerate(s.split('\n')):
        line = line.strip()
--
        raise ValueError('No productions found!')
    return DependencyGrammar(productions)

def parse_dependency_production(s):
    if not _PARSE_DG_RE.match(s):
        raise ValueError('Bad production string')
    pieces = _SPLIT_DG_RE.split(s)
--
# Demonstration
#################################################################

def cfg_demo():
    """
    A demonstration showing how ``ContextFreeGrammars`` can be created and used.
    """
--
    Det  -> 'my'          [.28]
    """)

def pcfg_demo():
    """
    A demonstration showing how a ``WeightedGrammar`` can be created and used.
    """
--
    for parse in parser.nbest_parse(sent):
        print(parse)

def fcfg_demo():
    import nltk.data
    g = nltk.data.load('grammars/book_grammars/feat0.fcfg')
    print(g)
    print()

def dg_demo():
    """
    A demonstration showing the creation and inspection of a
    ``DependencyGrammar``.
--
    """)
    print(grammar)

def sdg_demo():
    """
    A demonstration of how to read a string representation of
    a CoNLL format dependency tree.
--
    tree = dg.tree()
    print(tree.pprint())

def demo():
    cfg_demo()
    pcfg_demo()
    fcfg_demo()
--
        uma cas de port e janel , em cim dum coxilh .
    """

    def __init__ (self):
        self._model = []

        self._model.append( self.read_rule("step0.pt") )
--
        self._model.append( self.read_rule("step5.pt") )
        self._model.append( self.read_rule("step6.pt") )

    def read_rule (self, filename):
        rules = load('nltk:stemmers/rslp/' + filename, format='raw').decode("utf8")
        lines = rules.split("\n")

--

        return rules

    def stem(self, word):
        word = word.lower()

        # the word ends in 's'? apply rule for plural reduction
--

        return word

    def apply_rule(self, word, rule_index):
        rules = self._model[rule_index]
        for rule in rules:
            suffix_length = len(rule[0])
--
morphological rules, and part-of-speech and sense ambiguities
(eg. ``ceil-`` is not the stem of ``ceiling``).

StemmerI defines a standard interface for stemmers.
"""

from nltk.stem.api import StemmerI
--
    # Note that only lower case sequences are stemmed. Forcing to lower case
    # should be done before stem(...) is called.

    def __init__(self):

        ## --NEW--
        ## This is a table of irregular forms. It is quite short, but still
--
                
        self.vowels = frozenset(['a', 'e', 'i', 'o', 'u'])

    def _cons(self, word, i):
        """cons(i) is TRUE <=> b[i] is a consonant."""
        if word[i] in self.vowels:
            return False
--
                return (not self._cons(word, i - 1))
        return True

    def _m(self, word, j):
        """m() measures the number of consonant sequences between k0 and j.
        if c is a consonant sequence and v a vowel sequence, and <..>
        indicates arbitrary presence,
--
                i = i + 1
            i = i + 1

    def _vowelinstem(self, stem):
        """vowelinstem(stem) is TRUE <=> stem contains a vowel"""
        for i in range(len(stem)):
            if not self._cons(stem, i):
                return True
        return False

    def _doublec(self, word):
        """doublec(word) is TRUE <=> word ends with a double consonant"""
        if len(word) < 2:
            return False
--
            return False
        return self._cons(word, len(word)-1)

    def _cvc(self, word, i):
        """cvc(i) is TRUE <=>

        a) ( --NEW--) i == 1, and word[0] word[1] is vowel consonant, or
--

        return 1

    def _step1ab(self, word):
        """step1ab() gets rid of plurals and -ed or -ing. e.g.

           caresses  ->  caress
--
                
        return word

    def _step1c(self, word):
        """step1c() turns terminal y to i when there is another vowel in the stem.
        --NEW--: This has been modified from the original Porter algorithm so that y->i
        is only done when y is preceded by a consonant, but not if the stem
--
        else:
            return word

    def _step2(self, word):
        """step2() maps double suffices to single ones.
        so -ization ( = -ize plus -ation) maps to -ize etc. note that the
        string before the suffix must give m() > 0.
--
        else:
            return word

    def _step3(self, word):
        """step3() deals with -ic-, -full, -ness etc. similar strategy to step2."""
        
        ch = word[-1]
--
        else:
            return word

    def _step4(self, word):
        """step4() takes off -ant, -ence etc., in context <c>vcvc<v>."""
        
        if len(word) <= 1: # Only possible at this stage given unusual inputs to stem_word like 'oed'       
--
        else:
            return word

    def _step5(self, word):
        """step5() removes a final -e if m() > 1, and changes -ll to -l if
        m() > 1.
        """
--
            
        return word

    def stem_word(self, p, i=0, j=None):
        """
        Returns the stem of p, or, if i and j are given, the stem of p[i:j+1].
        """
--
        word = self._step5(word)
        return word
        
    def _adjust_case(self, word, stem):
        lower = word.lower()

        ret = ""
--
    ## --NLTK--
    ## Don't use this procedure; we want to work with individual
    ## tokens, instead.  (commented out the following procedure)
    #def stem(self, text):
    #    parts = re.split("(\W+)", text)
    #    numWords = (len(parts) + 1)/2
    #
--
    #    return ret

    ## --NLTK--
    ## Define a stem() method that implements the StemmerI interface.
    def stem(self, word):
        stem = self.stem_word(word.lower(), 0, len(word) - 1)
        return self._adjust_case(word, stem)

    ## --NLTK--
    ## Add a string representation function
    def __repr__(self):
        return '<PorterStemmer>'

## --NLTK--
--
##--NLTK--
## Added a demo() function

def demo():
    """
    A demonstration of the porter stemmer on a sample from
    the Penn Treebank corpus.
--
    )


    def __init__(self):
        """Create an instance of the Lancaster stemmer.
        """
        # Setup an empty rule dictionary - this will be filled in later
        self.rule_dictionary = {}

    def parseRules(self, rule_tuple):
        """Validate the set of rules used in this stemmer.
        """
        valid_rule = re.compile("^[a-z]+\*?\d[a-z]*[>\.]?$")
--
            else:
                self.rule_dictionary[first_letter] = [rule]

    def stem(self, word):
        """Stem a word using the Lancaster stemmer.
        """
        # Lower-case the word, since all the rules are lower-cased
--
        # Save a copy of the original word
        intact_word = word

        # If the user hasn't supplied any rules, setup the default rules
        if len(self.rule_dictionary) == 0:
            self.parseRules(LancasterStemmer.rule_tuple)

        return self.__doStemming(word, intact_word)

    def __doStemming(self, word, intact_word):
        """Perform the actual word stemming
        """

--
                    proceed = False
        return word

    def __getLastLetter(self, word):
        """Get the zero-based index of the last alphabetic character in this string
        """
        last_letter = -1
--
                break
        return last_letter

    def __isAcceptable(self, word, remove_total):
        """Determine if the word is acceptable for stemming.
        """
        word_is_acceptable = False
--
        return word_is_acceptable


    def __applyRule(self, word, remove_total, append_string):
        """Apply the stemming rule to the word
        """
        # Remove letters from the end of the word
--
            word += append_string
        return word

    def __repr__(self):
        return '<LancasterStemmer>'


--
    :type language: str or unicode
    :param ignore_stopwords: If set to True, stopwords are
                             not stemmed and returned unchanged.
                             Set to False by default.
    :type ignore_stopwords: bool
    :raise ValueError: If there is no stemmer for the specified
                           language, a ValueError is raised.
--
                 "hungarian", "italian", "norwegian", "porter", "portuguese",
                 "romanian", "russian", "spanish", "swedish")

    def __init__(self, language, ignore_stopwords=False):
        if language not in self.languages:
            raise ValueError("The language '%s' is not supported." % language)
        stemmerclass = globals()[language.capitalize() + "Stemmer"]
--

    :param ignore_stopwords: If set to True, stopwords are
                             not stemmed and returned unchanged.
                             Set to False by default.
    :type ignore_stopwords: bool
    """

    def __init__(self, ignore_stopwords=False):
        # The language is the name of the class, minus the final "Stemmer".
        language = type(self).__name__.lower()
        if language.endswith("stemmer"):
--
                raise ValueError("%r has no list of stopwords. Please set"
                                 " 'ignore_stopwords' to 'False'." % self)

    def __repr__(self):
        """
        Print out the string representation of the respective class.

--
    nltk.stem.porter for more information.

    """
    def __init__(self, ignore_stopwords=False):
        _LanguageSpecificStemmer.__init__(self, ignore_stopwords)
        porter.PorterStemmer.__init__(self)

--
class _ScandinavianStemmer(_LanguageSpecificStemmer):

    """
    This subclass encapsulates a method for defining the string region R1.
    It is used by the Danish, Norwegian, and Swedish stemmer.

    """

    def _r1_scandinavian(self, word, vowels):
        """
        Return the region R1 that is used by the Scandinavian stemmers.

--
class _StandardStemmer(_LanguageSpecificStemmer):

    """
    This subclass encapsulates two methods for defining the standard versions
    of the string regions R1, R2, and RV.

    """

    def _r1r2_standard(self, word, vowels):
        """
        Return the standard interpretations of the string regions R1 and R2.

--
               FrenchStemmer, GermanStemmer, ItalianStemmer,
               PortugueseStemmer, RomanianStemmer, and SpanishStemmer.
               It is not to be invoked directly!
        :note: A detailed description of how to define R1 and R2
               can be found at http://snowball.tartarus.org/texts/r1r2.html

        """
--



    def _rv_standard(self, word, vowels):
        """
        Return the standard interpretation of the string region RV.

--

    """

    # The language's vowels and other important characters are defined.
    __vowels = "aeiouy\xE6\xE5\xF8"
    __consonants = "bcdfghjklmnpqrstvwxz"
    __double_consonants = ("bb", "cc", "dd", "ff", "gg", "hh", "jj",
--
    __step2_suffixes = ("gd", "dt", "gt", "kt")
    __step3_suffixes = ("elig", "l\xF8st", "lig", "els", "ig")

    def stem(self, word):
        """
        Stem a Danish word and return the stemmed form.

--
    __step1_suffixes = ("heden", "ene", "en", "se", "s")
    __step3b_suffixes = ("baar", "lijk", "bar", "end", "ing", "ig")

    def stem(self, word):
        """
        Stem a Dutch word and return the stemmed form.

--
                       "succeeded" : "succeed",
                       "succeeding" : "succeed"}

    def stem(self, word):

        """
        Stem an English word and return the stemmed form.
--
                        'imm\xE4', 'mpi', 'mpa', 'mp\xE4', 'mmi',
                        'mma', 'mm\xE4', 'eja', 'ej\xE4')

    def stem(self, word):
        """
        Stem a Finnish word and return the stemmed form.

--
    __step4_suffixes = ('i\xE8re', 'I\xE8re', 'ion', 'ier', 'Ier',
                        'e', '\xEB')

    def stem(self, word):
        """
        Stem a French word and return the stemmed form.

--



    def __rv_french(self, word, vowels):
        """
        Return the region RV that is used by the French stemmer.

--
        vowel not at the beginning of the word, or the end of the word
        if these positions cannot be found. (Exceptionally, u'par',
        u'col' or u'tap' at the beginning of a word is also taken to
        define RV as the region to their right.)

        :param word: The French word whose region RV is determined.
        :type word: str or unicode
--
    __step3_suffixes = ("isch", "lich", "heit", "keit",
                          "end", "ung", "ig", "ik")

    def stem(self, word):
        """
        Stem a German word and return the stemmed form.

--
    __step9_suffixes = ("\xE1k", "\xE9k", "\xF6k", "ok",
                        "ek", "ak", "k")

    def stem(self, word):
        """
        Stem an Hungarian word and return the stemmed form.

--



    def __r1_hungarian(self, word, vowels, digraphs):
        """
        Return the region R1 that is used by the Hungarian stemmer.

        If the word begins with a vowel, R1 is defined as the region
        after the first consonant or digraph (= two letters stand for
        one phoneme) in the word. If the word begins with a consonant,
        it is defined as the region after the first vowel in the word.
        If the word does not contain both a vowel and consonant, R1
        is the null region at the end of the word.

--
                        'ivo', 'ono', 'uta', 'ute', 'uti', 'uto',
                        'ar', 'ir')

    def stem(self, word):
        """
        Stem an Italian word and return the stemmed form.

--
    __step3_suffixes = ("hetslov", "eleg", "elig", "elov", "slov",
                          "leg", "eig", "lig", "els", "lov", "ig")

    def stem(self, word):
        """
        Stem a Norwegian word and return the stemmed form.

--
    __step4_suffixes = ("os", "a", "i", "o", "\xE1",
                        "\xED", "\xF3")

    def stem(self, word):
        """
        Stem a Portuguese word and return the stemmed form.

--
                        '\xE2i', '\u0103m', 'em', 'im', '\xE2m',
                        'se')

    def stem(self, word):
        """
        Stem a Romanian word and return the stemmed form.

--
    __superlative_suffixes = ("ei`she", "ei`sh")
    __derivational_suffixes = ("ost'", "ost")

    def stem(self, word):
        """
        Stem a Russian word and return the stemmed form.

--



    def __regions_russian(self, word):
        """
        Return the regions RV and R2 which are used by the Russian stemmer.

--



    def __cyrillic_to_roman(self, word):
        """
        Transliterate a Russian word into the Roman alphabet.

--



    def __roman_to_cyrillic(self, word):
        """
        Transliterate a Russian word back into the Cyrillic alphabet.

--
    __step3_suffixes = ("os", "a", "e", "o", "\xE1",
                        "\xE9", "\xED", "\xF3")

    def stem(self, word):
        """
        Stem a Spanish word and return the stemmed form.

--
    __step2_suffixes = ("dd", "gd", "nn", "dt", "gt", "kt", "tt")
    __step3_suffixes = ("fullt", "l\xF6st", "els", "lig", "ig")

    def stem(self, word):
        """
        Stem a Swedish word and return the stemmed form.

--



def demo():
    """
    This function provides a demonstration of the Snowball stemmers.

--
    using Arabic '1256' coding.
    '''

    def __init__(self):
        self.stm = 'defult none'

        self.p3 = ['\u0643\u0627\u0644', '\u0628\u0627\u0644',
                   '\u0648\u0644\u0644', '\u0648\u0627\u0644']    # length three prefixes
--
                           '\u0627\u0644\u0630\u0649']


    def stem(self, token):
        """
        Stemming a word token using the ISRI stemmer.
        """
--
                return self.stm
        return self.stm              # if word length >7 , then no stemming

    def norm(self, num):
        """
        normalization:
        num=1  normalize diacritics
--
            self.stm = self.re_intial_hamza.sub(r'\u0627',self.stm)
            return self.stm

    def pre32(self):
        """remove length three and length two prefixes in this order"""
        if len(self.stm)>=6:
            for pre3 in self.p3:
--
                            self.stm = self.stm[2:]
                            return self.stm

    def suf32(self):
        """remove length three and length two suffixes in this order"""
        if len(self.stm)>=6:
            for suf3 in self.s3:
--
                            return self.stm


    def waw(self):
        """remove connective ‘و’ if it precedes a word beginning with ‘و’ """
        if (len(self.stm)>=4)&(self.stm[:2] == '\u0648\u0648'):
            self.stm = self.stm[1:]
            return self.stm

    def pro_w4(self):
        """process length four patterns and extract length three roots"""
        if self.stm[0] in self.pr4[0]:      #  مفعل
            self.stm = self.stm[1:]
--
                self.pre1()    # do - normalize short prefix
            return self.stm

    def pro_w53(self):
        """process length five patterns and extract length three roots"""
        if ((self.stm[2] in self.pr53[0]) & (self.stm[0] == '\u0627')):    #  افتعل   -  افاعل
            self.stm = self.stm[1]+self.stm[3:]
--
                self.pre1()   # do - normalize short prefix
            return self.stm

    def pro_w54(self):
        """process length five patterns and extract length four roots"""
        if (self.stm[0] in self.pr53[2]):       #تفعلل - افعلل - مفعلل
            self.stm = self.stm[1:]
--
            self.stm = self.stm[:2]+self.stm[3:]
            return self.stm

    def end_w5(self):
        """ending step (word of length five)"""
        if len(self.stm)==3:
            return self.stm
--
            self.pro_w54()
            return self.stm

    def pro_w6(self):
        """process length six patterns and extract length three roots"""
        if ((self.stm.startswith('\u0627\u0633\u062a')) or (self.stm.startswith('\u0645\u0633\u062a'))):   #   مستفعل   -    استفعل
            self.stm= self.stm[3:]
--
                self.pre1()    # do - normalize short prefix
            return self.stm

    def pro_w64(self):
        """process length six patterns and extract length four roots"""
        if (self.stm[0] and self.stm[4])=='\u0627':      #  افعلال
            self.stm=self.stm[1:4]+self.stm[5]
--
            self.stm = self.stm[2:]
            return self.stm

    def end_w6(self):
        """ending step (word of length six)"""
        if len(self.stm)==3:
            return self.stm
--
            self.pro_w64()
            return self.stm

    def suf1(self):
        """normalize short sufix"""
        for sf1 in self.s1:
            if self.stm.endswith(sf1):
                self.stm = self.stm[:-1]
                return self.stm

    def pre1(self):
        """normalize short prefix"""
        for sp1 in self.p1:
            if self.stm.startswith(sp1):
--
        hardrock
    """

    def __init__(self):
        pass

    def lemmatize(self, word, pos=NOUN):
        lemmas = wordnet._morphy(word, pos)
        return min(lemmas, key=len) if lemmas else word

    def __repr__(self):
        return '<WordNetLemmatizer>'


# unload wordnet
def teardown_module(module=None):
    from nltk.corpus import wordnet
    wordnet._unload()

--
    :type min: int
    :param min: The minimum length of string to stem
    """
    def __init__(self, regexp, min=0):

        if not hasattr(regexp, 'pattern'):
            regexp = re.compile(regexp)
        self._regexp = regexp
        self._min = min

    def stem(self, word):
        if len(word) < self._min:
            return word
        else:
            return self._regexp.sub('', word)

    def __repr__(self):
        return '<RegexpStemmer: %r>' % self._regexp.pattern


--
    words.  This process is known as stemming.

    """
    def stem(self, token):
        """
        Strip affixes from the token and return the stem.

--
    efficient speed-up in the clustering process.
    """

    def __init__(self, num_clusters=1, normalise=True, svd_dimensions=None):
        VectorSpaceClusterer.__init__(self, normalise, svd_dimensions)
        self._num_clusters = num_clusters
        self._dendrogram = None
        self._groups_values = None

    def cluster(self, vectors, assign_clusters=False, trace=False):
        # stores the merge order
        self._dendrogram = Dendrogram(
            [numpy.array(vector, numpy.float64) for vector in vectors])
        return VectorSpaceClusterer.cluster(self, vectors, assign_clusters, trace)

    def cluster_vectorspace(self, vectors, trace=False):
        # variables describing the initial situation
        N = len(vectors)
        cluster_len = [1]*N
--

        self.update_clusters(self._num_clusters)

    def _merge_similarities(self, dist, cluster_len, i, j):
        # the new cluster i merged from i and j adopts the average of
        # i and j's similarity to each other cluster, weighted by the
        # number of points in the clusters i and j
--
        dist[i, j+1:] = dist[i, j+1:]*i_weight + dist[j, j+1:]*j_weight
        dist[i, i+1:] /= weight_sum

    def update_clusters(self, num_clusters):
        clusters = self._dendrogram.groups(num_clusters)
        self._centroids = []
        for cluster in clusters:
--
            self._centroids.append(centroid)
        self._num_clusters = len(self._centroids)

    def classify_vectorspace(self, vector):
        best = None
        for i in range(self._num_clusters):
            centroid = self._centroids[i]
--
                best = (dist, i)
        return best[1]

    def dendrogram(self):
        """
        :return: The dendrogram representing the current clustering
        :rtype:  Dendrogram
        """
        return self._dendrogram

    def num_clusters(self):
        return self._num_clusters

    def __repr__(self):
        return '<GroupAverageAgglomerative Clusterer n=%d>' % self._num_clusters

def demo():
    """
    Non-interactive demonstration of the clusterers with simple 2-D data.
    """
--
probabilities. This process continues until the likelihood of the data does
not significantly increase.

They all extend the ClusterI interface which defines common operations
available with each clusterer. These operations include.
   - cluster: clusters a sequence of vectors
   - classify: assign a vector to a cluster
--
    the likelihood of the data does not significantly increase.
    """

    def __init__(self, initial_means, priors=None, covariance_matrices=None,
                       conv_threshold=1e-6, bias=0.1, normalise=False,
                       svd_dimensions=None):
        """
--
        self._priors = priors
        self._bias = bias

    def num_clusters(self):
        return self._num_clusters

    def cluster_vectorspace(self, vectors, trace=False):
        assert len(vectors) > 0

        # set the parameters to initial values
--
                converged = True
            lastl = l

    def classify_vectorspace(self, vector):
        best = None
        for j in range(self._num_clusters):
            p = self._priors[j] * self._gaussian(self._means[j],
--
                best = (p, j)
        return best[1]

    def likelihood_vectorspace(self, vector, cluster):
        cid = self.cluster_names().index(cluster)
        return self._priors[cluster] * self._gaussian(self._means[cluster],
                                self._covariance_matrices[cluster], vector)

    def _gaussian(self, mean, cvm, x):
        m = len(mean)
        assert cvm.shape == (m, m), \
            'bad sized covariance matrix, %s' % str(cvm.shape)
--
            # i.e. the inverse of cvm is huge (cvm is almost zero)
            return 0

    def _loglikelihood(self, vectors, priors, means, covariances):
        llh = 0.0
        for vector in vectors:
            p = 0
--
            llh += numpy.log(p)
        return llh

    def __repr__(self):
        return '<EMClusterer means=%s>' % list(self._means)

def demo():
    """
    Non-interactive demonstration of the clusterers with simple 2-D data.
    """
--
    commonly occurring output means are chosen.
    """

    def __init__(self, num_means, distance, repeats=1,
                       conv_test=1e-6, initial_means=None,
                       normalise=False, svd_dimensions=None,
                       rng=None, avoid_empty_clusters=False):
--
        :param  rng:        random number generator (or None)
        :type   rng:        Random
        :param avoid_empty_clusters: include current centroid in computation
                                     of next one; avoids undefined behavior
                                     when clusters become empty
        :type avoid_empty_clusters: boolean
        """
--
        self._rng = (rng if rng else random.Random())
        self._avoid_empty_clusters = avoid_empty_clusters

    def cluster_vectorspace(self, vectors, trace=False):
        if self._means and self._repeats > 1:
            print('Warning: means will be discarded for subsequent trials')

--
            # use the best means
            self._means = min_means

    def _cluster_vectorspace(self, vectors, trace=False):
        if self._num_means < len(vectors):
            # perform k-means clustering
            converged = False
--
                # remember the new means
                self._means = new_means

    def classify_vectorspace(self, vector):
        # finds the closest cluster centroid
        # returns that cluster's index
        best_distance = best_index = None
--
                best_index, best_distance = index, dist
        return best_index

    def num_clusters(self):
        if self._means:
            return len(self._means)
        else:
            return self._num_means

    def means(self):
        """
        The means used for clustering.
        """
        return self._means

    def _sum_distances(self, vectors1, vectors2):
        difference = 0.0
        for u, v in zip(vectors1, vectors2):
            difference += self._distance(u, v)
        return difference

    def _centroid(self, cluster, mean):
        if self._avoid_empty_clusters:
            centroid = copy.copy(mean)
            for vector in cluster:
--
            return centroid / (1+float(len(cluster)))
        else:
            if not len(cluster):
                sys.stderr.write('Error: no centroid defined for empty cluster.\n')
                sys.stderr.write('Try setting argument \'avoid_empty_clusters\' to True\n')
                assert(False)
            centroid = copy.copy(cluster[0])
--
                centroid += vector
            return centroid / float(len(cluster))

    def __repr__(self):
        return '<KMeansClusterer means=%s repeats=%d>' % \
                    (self._means, self._repeats)

#################################################################################

def demo():
    # example from figure 14.9, page 517, Manning and Schutze

    from nltk.cluster import KMeansClusterer, euclidean_distance
--
    Optionally performs singular value decomposition to reduce the
    dimensionality.
    """
    def __init__(self, normalise=False, svd_dimensions=None):
        """
        :param normalise:       should vectors be normalised to length 1
        :type normalise:        boolean
--
        self._should_normalise = normalise
        self._svd_dimensions = svd_dimensions

    def cluster(self, vectors, assign_clusters=False, trace=False):
        assert len(vectors) > 0

        # normalise the vectors
--
            print(self._Tt, vectors)
            return [self.classify(vector) for vector in vectors]

    def cluster_vectorspace(self, vectors, trace):
        """
        Finds the clusters using the given set of vectors.
        """
        raise NotImplementedError()

    def classify(self, vector):
        if self._should_normalise:
            vector = self._normalise(vector)
        if self._Tt is not None:
--
        cluster = self.classify_vectorspace(vector)
        return self.cluster_name(cluster)

    def classify_vectorspace(self, vector):
        """
        Returns the index of the appropriate cluster for the vector.
        """
        raise NotImplementedError()

    def likelihood(self, vector, label):
        if self._should_normalise:
            vector = self._normalise(vector)
        if self._Tt is not None:
            vector = numpy.dot(self._Tt, vector)
        return self.likelihood_vectorspace(vector, label)

    def likelihood_vectorspace(self, vector, cluster):
        """
        Returns the likelihood of the vector belonging to the cluster.
        """
        predicted = self.classify_vectorspace(vector)
        return (1.0 if cluster == predicted else 0.0)

    def vector(self, vector):
        """
        Returns the vector after normalisation and dimensionality reduction
        """
--
            vector = numpy.dot(self._Tt, vector)
        return vector

    def _normalise(self, vector):
        """
        Normalises the vector to unit length.
        """
        return vector / sqrt(numpy.dot(vector, vector))

def euclidean_distance(u, v):
    """
    Returns the euclidean distance between vectors u and v. This is equivalent
    to the length of the vector (u - v).
--
    diff = u - v
    return sqrt(numpy.dot(diff, diff))

def cosine_distance(u, v):
    """
    Returns 1 minus the cosine of the angle between vectors v and u. This is equal to
    1 - (u.v / |u||v|).
--
class _DendrogramNode(object):
    """ Tree node of a dendrogram. """

    def __init__(self, value, *children):
        self._value = value
        self._children = children

    def leaves(self, values=True):
        if self._children:
            leaves = []
            for child in self._children:
--
        else:
            return [self]

    def groups(self, n):
        queue = [(self._value, self)]

        while len(queue) < n:
--
    to the merge function.
    """

    def __init__(self, items=[]):
        """
        :param  items: the items at the leaves of the dendrogram
        :type   items: sequence of (any)
--
        self._original_items = copy.copy(self._items)
        self._merge = 1

    def merge(self, *indices):
        """
        Merges nodes at given indices in the dendrogram. The nodes will be
        combined which then replaces the first node specified. All other nodes
--
        for i in indices[1:]:
            del self._items[i]

    def groups(self, n):
        """
        Finds the n-groups of items (leaves) reachable from a cut at depth n.
        :param  n: number of groups
--
            root = self._items[0]
        return root.groups(n)

    def show(self, leaf_labels=[]):
        """
        Print the dendrogram in ASCII art to standard out.
        :param leaf_labels: an optional list of strings to use for labeling the leaves
--
        rhalf = width - lhalf - 1

        # display functions
        def format(centre, left=' ', right=' '):
            return '%s%s%s' % (lhalf*left, centre, right*rhalf)
        def display(str):
            stdout.write(str)

        # for each merge, top down
--
        display(''.join(item.center(width) for item in last_row))
        display('\n')

    def __repr__(self):
        if len(self._items) > 1:
            root = _DendrogramNode(self._merge, *self._items)
        else:
--
    Interface covering basic clustering functionality.
    """

    def cluster(self, vectors, assign_clusters=False):
        """
        Assigns the vectors to clusters, learning the clustering parameters
        from the data. Returns a cluster identifier for each vector.
        """
        raise NotImplementedError()

    def classify(self, token):
        """
        Classifies the token into a cluster, setting the token's CLUSTER
        parameter to that cluster identifier.
        """
        raise NotImplementedError()

    def likelihood(self, vector, label):
        """
        Returns the likelihood (a float) of the token having the
        corresponding cluster.
--
        else:
            return 0.0

    def classification_probdist(self, vector):
        """
        Classifies the token into a cluster, returning
        a probability distribution over the cluster identifiers.
--
            likelihoods[cluster] /= sum
        return DictionaryProbDist(likelihoods)

    def num_clusters(self):
        """
        Returns the number of clusters.
        """
        raise NotImplementedError()

    def cluster_names(self):
        """
        Returns the names of the clusters.
        """
        return list(range(self.num_clusters()))

    def cluster_name(self, index):
        """
        Returns the names of the cluster at index.
        """
--

from nltk.data import load

def brown_tagset(tagpattern=None):
    _format_tagset("brown_tagset", tagpattern)

def claws5_tagset(tagpattern=None):
    _format_tagset("claws5_tagset", tagpattern)

def upenn_tagset(tagpattern=None):
    _format_tagset("upenn_tagset", tagpattern)

#####################################################################
# UTILITIES
#####################################################################

def _print_entries(tags, tagdict):
    for tag in tags:
        entry = tagdict[tag]
        defn = [tag + ": " + entry[0]]
        examples = wrap(entry[1], width=75, initial_indent='    ', subsequent_indent='    ')
        print("\n".join(defn + examples))

def _format_tagset(tagset, tagpattern=None):
    tagdict = load("help/tagsets/" + tagset + ".pickle")
    if not tagpattern:
        _print_entries(sorted(tagdict), tagdict)
--
if __name__ == '__main__':
    brown_tagset(r'NN.*')
    upenn_tagset(r'.*\$')
    claws5_tagset('UNDEFINED')
    brown_tagset(r'NN')
--
from . import tag, text, tokenize, tree, treetransforms, util

# override any accidentally imported demo
def demo():
    print("To run the demo code for a module, type nltk.module.demo()")
--
        Nose plugin manager that replaces standard doctest plugin
        with a patched version.
        """
        def loadPlugins(self):
            for plug in builtin.plugins:
                if plug != Doctest:
                    self.addPlugin(plug())
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import

def setup_module(module):
    from nose import SkipTest
    from nltk.inference.mace import Mace
    try:
--
from __future__ import absolute_import
from nltk.compat import PY3

def setup_module(module):
    from nose import SkipTest
    if PY3:
        raise SkipTest("compat.doctest is for Python 2.x")
--
from glob import glob
import os.path

def additional_tests():
    #print "here-000000000000000"
    #print "-----", glob(os.path.join(os.path.dirname(__file__), '*.doctest'))
    dir = os.path.dirname(__file__)
--
# probability.doctest uses HMM which requires numpy;
# skip probability.doctest if numpy is not available

def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import

def setup_module(module):
    from nose import SkipTest
    from nltk.parse.malt import MaltParser

--


# skip segmentation.doctest if numpy is not available
def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import

def setup_module(module):
    from nose import SkipTest
    from nltk.inference.mace import Mace
    try:
--
from __future__ import absolute_import

# reset the variables counter before running tests
def setup_module(module):
    from nltk.sem import logic
    logic._counter._value = 0
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import

def setup_module(module):
    from nose import SkipTest
    import nltk.data
    try:
--

from nltk.corpus import teardown_module

def setup_module(module):
    from nose import SkipTest
    if not PY3:
        raise SkipTest("portuguese_en.doctest was skipped because "
--
from nltk.text import Text
from nltk.compat import PY3, python_2_unicode_compatible

def setup_module(module):
    from nose import SkipTest
    if PY3:
        raise SkipTest("test_2x_compat is for testing nltk.compat under Python 2.x")
--
class TestTextTransliteration(unittest.TestCase):
    txt = Text(["São", "Tomé", "and", "Príncipe"])

    def test_repr(self):
        self.assertEqual(repr(self.txt), br"<Text: S\xe3o Tom\xe9 and Pr\xedncipe...>")

    def test_str(self):
        self.assertEqual(str(self.txt), b"<Text: Sao Tome and Principe...>")

--
from __future__ import absolute_import, unicode_literals
from nltk import hmm

def _wikipedia_example_hmm():
    # Example from wikipedia
    # (http://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm)

--
    return model, states, symbols, seq


def test_forward_probability():
    from numpy.testing import assert_array_almost_equal

    # example from p. 385, Huang et al
--
    assert_array_almost_equal(fp, expected)


def test_forward_probability2():
    from numpy.testing import assert_array_almost_equal

    model, states, symbols, seq = _wikipedia_example_hmm()
--
    assert_array_almost_equal(wikipedia_results, fp, 4)


def test_backward_probability():
    from numpy.testing import assert_array_almost_equal

    model, states, symbols, seq = _wikipedia_example_hmm()
--
    assert_array_almost_equal(wikipedia_results, bp, 4)


def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import, unicode_literals

def test_basic():
    from nltk.tag import pos_tag
    from nltk.tokenize import word_tokenize

--
                      ('.', '.')]


def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--

class SnowballTest(unittest.TestCase):

    def test_russian(self):
        # Russian words both consisting of Cyrillic
        # and Roman letters can be stemmed.
        stemmer_russian = SnowballStemmer("russian")
        assert stemmer_russian.stem("авантненькая") == "авантненьк"
        assert stemmer_russian.stem("avenantnen'kai^a") == "avenantnen'k"

    def test_german(self):
        stemmer_german = SnowballStemmer("german")
        stemmer_german2 = SnowballStemmer("german", ignore_stopwords=True)

--
        assert stemmer_german.stem("keinen") == 'kein'
        assert stemmer_german2.stem("keinen") == 'keinen'

    def test_short_strings_bug(self):
        stemmer = SnowballStemmer('english')
        assert stemmer.stem("y's") == 'y'
--
    (0.76,  0.24),
]

def assert_classifier_correct(algorithm):
    try:
        classifier = classify.MaxentClassifier.train(
            TRAIN, algorithm, trace=0, max_iter=1000
--
        assert abs(pdist.prob('y') - py) < 1e-2, (pdist.prob('y'), py)


def test_megam():
    assert_classifier_correct('MEGAM')

def test_tadm():
    assert_classifier_correct('TADM')
--
        'corpora/inaugural/1909-Taft.txt', # A longer file (32k chars)
    ]

    def data(self):
        for name in self.names:
            f = nltk.data.find(name)
            with f.open() as fp:
                file_data = fp.read().decode('utf8')
            yield f, file_data

    def test_correct_values(self):
        # Check that corpus views produce the correct sequence of values.

        for f, file_data in self.data():
--
            v = StreamBackedCorpusView(f, read_line_block)
            self.assertEqual(list(v), self.linetok.tokenize(file_data))

    def test_correct_length(self):
        # Check that the corpus views report the correct lengths:

        for f, file_data in self.data():
--
_EPSILON = 1e-8


def close_enough(x, y):
    """Verify that two sequences of n-gram association values are within
       _EPSILON of each other.
    """
--
    return True


def test_bigram2():

    sent = 'this this is is a a test test'.split()

--
        [(('a', 'a'), 1.0), (('a', 'test'), 1.0), (('is', 'a'), 1.0), (('is', 'is'), 1.0), (('test', 'test'), 1.0), (('this', 'is'), 1.0), (('this', 'this'), 1.0)])


def test_bigram3():

    sent = 'this this is is a a test test'.split()

--
        [(('a', 'test'), 1.584962500721156), (('is', 'a'), 1.584962500721156), (('this', 'is'), 1.584962500721156), (('a', 'a'), 0.0), (('is', 'is'), 0.0), (('test', 'test'), 0.0), (('this', 'this'), 0.0)])


def test_bigram5():

    sent = 'this this is is a a test test'.split()

--
from nose.plugins.skip import SkipTest
from nltk.util import py26

def skip(reason):
    """
    Unconditionally skip a test.
    """
    def decorator(test_item):
        is_test_class = isinstance(test_item, type) and issubclass(test_item, TestCase)

        if is_test_class and py26():
--

        if not is_test_class:
            @wraps(test_item)
            def skip_wrapper(*args, **kwargs):
                raise SkipTest(reason)
            skip_wrapper.__name__ = test_item.__name__
            test_item = skip_wrapper
--
    return decorator


def skipIf(condition, reason):
    """
    Skip a test if the condition is true.
    """
--

class NaiveBayesClassifierTest(unittest.TestCase):

    def test_simple(self):
        training_features = [
            ({'nice': True, 'good': True}, 'positive'),
            ({'bad': True, 'mean': True}, 'negative')
--

class TestUdhr(unittest.TestCase):

    def test_words(self):
        for name in udhr.fileids():
            try:
                words = list(udhr.words(name))
--
                raise
            self.assertTrue(words)

    def test_raw_unicode(self):
        for name in udhr.fileids():
            txt = udhr.raw(name)
            assert not isinstance(txt, bytes), name
--

class TestIndian(unittest.TestCase):

    def test_words(self):
        words = indian.words()[:3]
        self.assertEqual(words, ['মহিষের', 'সন্তান', ':'])

    def test_tagged_words(self):
        tagged_words = indian.tagged_words()[:3]
        self.assertEqual(tagged_words, [('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM')])


class TestCess(unittest.TestCase):
    def test_catalan(self):
        words = cess_cat.words()[:15]
        txt = "El Tribunal_Suprem -Fpa- TS -Fpt- ha confirmat la condemna a quatre anys d' inhabilitació especial"
        self.assertEqual(words, txt.split())

    def test_esp(self):
        words = cess_esp.words()[:15]
        txt = "El grupo estatal Electricité_de_France -Fpa- EDF -Fpt- anunció hoy , jueves , la compra del"
        self.assertEqual(words, txt.split())


class TestFloresta(unittest.TestCase):
    def test_words(self):
        words = floresta.words()[:10]
        txt = "Um revivalismo refrescante O 7_e_Meio é um ex-libris de a"
        self.assertEqual(words, txt.split())

class TestSinicaTreebank(unittest.TestCase):

    def test_sents(self):
        first_3_sents = sinica_treebank.sents()[:3]
        self.assertEqual(
            first_3_sents,
            [['一'], ['友情'], ['嘉珍', '和', '我', '住在', '同一條', '巷子']]
        )

    def test_parsed_sents(self):
        parsed_sents = sinica_treebank.parsed_sents()[25]
        self.assertEqual(parsed_sents,
            Tree('S', [
--
class TestCoNLL2007(unittest.TestCase):
    # Reading the CoNLL 2007 Dependency Treebanks

    def test_sents(self):
        sents = conll2007.sents('esp.train')[0]
        self.assertEqual(
            sents[:6],
            ['El', 'aumento', 'del', 'índice', 'de', 'desempleo']
        )

    def test_parsed_sents(self):

        parsed_sents = conll2007.parsed_sents('esp.train')[0]

--

@skipIf(not ptb.fileids(), "A full installation of the Penn Treebank is not available")
class TestPTB(unittest.TestCase):
    def test_fileids(self):
        self.assertEqual(
            ptb.fileids()[:4],
            ['BROWN/CF/CF01.MRG', 'BROWN/CF/CF02.MRG', 'BROWN/CF/CF03.MRG', 'BROWN/CF/CF04.MRG']
        )

    def test_words(self):
        self.assertEqual(
            ptb.words('WSJ/00/WSJ_0003.MRG')[:7],
            ['A', 'form', 'of', 'asbestos', 'once', 'used', '*']
        )

    def test_tagged_words(self):
        self.assertEqual(
            ptb.tagged_words('WSJ/00/WSJ_0003.MRG')[:3],
            [('A', 'DT'), ('form', 'NN'), ('of', 'IN')]
        )

    def test_categories(self):
        self.assertEqual(
            ptb.categories(),
            ['adventure', 'belles_lettres', 'fiction', 'humor', 'lore', 'mystery', 'news', 'romance', 'science_fiction']
        )

    def test_news_fileids(self):
        self.assertEqual(
            ptb.fileids('news')[:3],
            ['WSJ/00/WSJ_0001.MRG', 'WSJ/00/WSJ_0002.MRG', 'WSJ/00/WSJ_0003.MRG']
        )

    def test_category_words(self):
        self.assertEqual(
            ptb.words(categories=['humor','fiction'])[:6],
            ['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back']
--
from io import BytesIO
from nltk.corpus.reader import SeekableUnicodeStreamReader

def check_reader(unicode_string, encoding, n=1000):
    bytestr = unicode_string.encode(encoding)
    strlen = len(unicode_string)
    stream = BytesIO(bytestr)
--

]

def test_reader():
    for string in STRINGS:
        for encoding in ENCODINGS:
            try:
--
How fun!  Let's repeat it twenty times.
"""*10

def test_reader_on_large_string():
    for encoding in ENCODINGS:
        try:
            # skip strings that can't be encoded with the current encoding
            LARGE_STRING.encode(encoding)
            def _check(encoding, n=1000):
                check_reader(LARGE_STRING, encoding, n)

            yield _check, encoding
--
        except UnicodeEncodeError:
            pass

def teardown_module(module=None):
    import gc
    gc.collect()
--
class _UnicodeOutputChecker(doctest.OutputChecker):
    _literal_re = re.compile(r"(\W|^)[uU]([rR]?[\'\"])", re.UNICODE)

    def _remove_u_prefixes(self, txt):
        return re.sub(self._literal_re, r'\1\2', txt)

    def check_output(self, want, got, optionflags):
        res = doctest.OutputChecker.check_output(self, want, got, optionflags)
        if res:
            return True
--
    """
    OPTION_BY_NAME = ('doctestencoding',)

    def loadTestsFromFileUnicode(self, filename):
        if self.extension and anyp(filename.endswith, self.extension):
            name = os.path.basename(filename)
            dh = codecs.open(filename, 'r', self.options.get('doctestencoding'))
--
            else:
                yield False # no tests to load

    def loadTestsFromFile(self, filename):

        cases = self.loadTestsFromFileUnicode(filename)

--
            else:
                yield self._patchTestCase(case)

    def loadTestsFromModule(self, module):
        """Load doctests from the module.
        """
        for suite in super(DoctestPluginHelper, self).loadTestsFromModule(module):
            cases = [self._patchTestCase(case) for case in suite._get_tests()]
            yield self.suiteClass(cases, context=module, can_split=False)

    def _patchTestCase(self, case):
        if case:
            case._dt_test.globs['print_function'] = print_function
            case._dt_checker = _checker
        return case

    def configure(self, options, config):
        # it is overriden in order to fix doctest options discovery

        Plugin.configure(self, options, config)
--
# -*- coding: utf-8 -*-
from __future__ import absolute_import

def teardown_module(module=None):
    from nltk.corpus import wordnet
    wordnet._unload()
--
from __future__ import absolute_import

# most of classify.doctest requires numpy
def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--

# FIXME: the entire discourse.doctest is skipped if Prover9/Mace4 is
# not installed, but there are pure-python parts that don't need Prover9.
def setup_module(module):
    from nose import SkipTest
    from nltk.inference.mace import Mace
    try:
--
    then provides some operations on the semantics dealing with holes, labels
    and finding legal ways to plug holes with labels.
    """
    def __init__(self, usr):
        """
        Constructor.  `usr' is a ``sem.Expression`` representing an
        Underspecified Representation Structure (USR).  A USR has the following
--
        self.top_most_labels = self._find_top_most_labels()
        self.top_hole = self._find_top_hole()

    def is_node(self, x):
        """
        Return true if x is a node (label or hole) in this semantic
        representation.
        """
        return x in (self.labels | self.holes)

    def _break_down(self, usr):
        """
        Extract holes, labels, formula fragments and constraints from the hole
        semantics underspecified representation (USR).
--
        else:
            raise ValueError(usr.node)

    def _find_top_nodes(self, node_list):
        top_nodes = node_list.copy()
        for f in compat.itervalues(self.fragments):
            #the label is the first argument of the predicate
--
                    top_nodes.discard(arg)
        return top_nodes

    def _find_top_most_labels(self):
        """
        Return the set of labels which are not referenced directly as part of
        another formula fragment.  These will be the top-most labels for the
--
        """
        return self._find_top_nodes(self.labels)

    def _find_top_hole(self):
        """
        Return the hole that will be the top of the formula tree.
        """
--
        assert len(top_holes) == 1   # it must be unique
        return top_holes.pop()

    def pluggings(self):
        """
        Calculate and return all the legal pluggings (mappings of labels to
        holes) of this semantics given the constraints.
--
        self._plug_nodes([(self.top_hole, [])], self.top_most_labels, {}, record)
        return record

    def _plug_nodes(self, queue, potential_labels, plug_acc, record):
        """
        Plug the nodes in `queue' with the labels in `potential_labels'.

--
        else:
            raise Exception('queue empty')

    def _plug_hole(self, hole, ancestors0, queue, potential_labels0,
                   plug_acc0, record):
        """
        Try all possible ways of plugging a single hole.
--
                # be finite but the bookkeeping would be harder.
                self._plug_nodes(queue + [(l, ancestors)], potential_labels, plug_acc, record)

    def _violates_constraints(self, label, ancestors):
        """
        Return True if the `label' cannot be placed underneath the holes given
        by the set `ancestors' because it would violate the constraints imposed
--
                    return True
        return False

    def _sanity_check_plugging(self, plugging, node, ancestors):
        """
        Make sure that a given plugging is legal.  We recursively go through
        each node and make sure that no constraints are violated.
--
            if self.is_node(arg):
                self._sanity_check_plugging(plugging, arg, [label] + ancestors)

    def formula_tree(self, plugging):
        """
        Return the first-order logic formula tree for this underspecified
        representation using the plugging given.
        """
        return self._formula_tree(plugging, self.top_hole)

    def _formula_tree(self, plugging, node):
        if node in plugging:
            return self._formula_tree(plugging, plugging[node])
        elif node in self.fragments:
--
    This class represents a constraint of the form (L =< N),
    where L is a label and N is a node (a label or a hole).
    """
    def __init__(self, lhs, rhs):
        self.lhs = lhs
        self.rhs = rhs
    def __eq__(self, other):
        if self.__class__ == other.__class__:
            return self.lhs == other.lhs and self.rhs == other.rhs
        else:
            return False
    def __ne__(self, other):
        return not (self == other)
    def __hash__(self):
        return hash(repr(self))
    def __repr__(self):
        return '(%s < %s)' % (self.lhs, self.rhs)


def hole_readings(sentence, grammar_filename=None, verbose=False):
    if not grammar_filename:
        grammar_filename = 'grammars/sample_grammars/hole.fcfg'

--

from nltk.sem.util import (batch_parse, batch_interpret, batch_evaluate,
                           root_semrep, parse_valuation)
from nltk.sem.evaluate import (Valuation, Assignment, Model, Undefined,
                               is_rel, set2rel, arity)
from nltk.sem.logic import (LogicParser, boolean_ops, binding_ops,
                            equality_preds, parse_logic)
--
    semantic parser that produces Discourse Representation Structures (DRSs).
    """

    def __init__(self, boxer_drs_interpreter=None, elimeq=False, bin_dir=None, verbose=False):
        """
        :param boxer_drs_interpreter: A class that converts from the
        ``AbstractBoxerDrs`` object hierarchy to a different object.  The
        default is ``NltkDrtBoxerDrsInterpreter``, which converts to the NLTK
        DRT hierarchy.
        :param elimeq: When set to true, Boxer removes all equalities from the
        DRSs and discourse referents standing in the equality relation are
--

        self.set_bin_dir(bin_dir, verbose)

    def set_bin_dir(self, bin_dir, verbose=False):
        self._candc_bin = self._find_binary('candc', bin_dir, verbose)
        self._candc_models_path = os.path.normpath(os.path.join(self._candc_bin[:-5], '../models'))
        self._boxer_bin = self._find_binary('boxer', bin_dir, verbose)

    def interpret(self, input, discourse_id=None, question=False, verbose=False):
        """
        Use Boxer to give a first order representation.

--
            raise Exception('Unable to interpret: "%s"' % input)
        return d

    def interpret_multisentence(self, input, discourse_id=None, question=False, verbose=False):
        """
        Use Boxer to give a first order representation.

--
            raise Exception('Unable to interpret: "%s"' % input)
        return d

    def batch_interpret(self, inputs, discourse_ids=None, question=False, verbose=False):
        """
        Use Boxer to give a first order representation.

--
        """
        return self.batch_interpret_multisentence([[input] for input in inputs], discourse_ids, question, verbose)

    def batch_interpret_multisentence(self, inputs, discourse_ids=None, question=False, verbose=False):
        """
        Use Boxer to give a first order representation.

--
        drs_dict = self._parse_to_drs_dict(boxer_out, use_disc_id)
        return [drs_dict.get(id, None) for id in discourse_ids]

    def _call_candc(self, inputs, discourse_ids, question, verbose=False):
        """
        Call the ``candc`` binary with the given input.

--
                '--candc-printer', 'boxer']
        return self._call('\n'.join(sum((["<META>'%s'" % id] + d for d,id in zip(inputs,discourse_ids)), [])), self._candc_bin, args, verbose)

    def _call_boxer(self, candc_out, verbose=False):
        """
        Call the ``boxer`` binary with the given input.

--
        os.remove(temp_filename)
        return stdout

    def _find_binary(self, name, bin_dir, verbose=False):
        return find_binary(name,
            path_to_bin=bin_dir,
            env_vars=['CANDCHOME'],
--
            binary_names=[name, name + '.exe'],
            verbose=verbose)

    def _call(self, input_str, binary, args=[], verbose=False):
        """
        Call the binary with the given input.

--

        return stdout

    def _parse_to_drs_dict(self, boxer_out, use_disc_id):
        lines = boxer_out.split('\n')
        drs_dict = {}
        i = 0
--
            i += 1
        return drs_dict

    def _parse_drs(self, drs_string, discourse_id, use_disc_id):
        return BoxerOutputDrsParser([None,discourse_id][use_disc_id]).parse(drs_string)


class BoxerOutputDrsParser(DrtParser):
    def __init__(self, discourse_id=None):
        """
        This class is used to parse the Prolog DRS output from Boxer into a
        hierarchy of python objects.
--
        self.quote_chars = [("'", "'", "\\", False)]
        self._label_counter = None

    def parse(self, data, signature=None):
        self._label_counter = Counter(-1)
        return DrtParser.parse(self, data, signature)

    def get_all_symbols(self):
        return ['(', ')', ',', '[', ']',':']

    def handle(self, tok, context):
        return self.handle_drs(tok)

    def attempt_adjuncts(self, expression, context):
        return expression

    def parse_condition(self, indices):
        """
        Parse a DRS condition

--
            raise UnexpectedTokenException(tok)
        return accum

    def handle_drs(self, tok):
        if tok == 'drs':
            return self.parse_drs()
        elif tok in ['merge', 'smerge']:
--
        elif tok in ['alfa']:
            return self._handle_alfa(self._make_merge_expression)(None, [])

    def handle_condition(self, tok, indices):
        """
        Handle a DRS condition

--

        return sum([[cond(sent_index, word_indices) for cond in conds] for sent_index, word_indices in self._sent_and_word_indices(indices)], [])

    def _handle_not(self):
        self.assertToken(self.token(), '(')
        drs = self.parse_Expression(None)
        self.assertToken(self.token(), ')')
        return BoxerNot(drs)

    def _handle_pred(self):
        #pred(_G3943, dog, n, 0)
        self.assertToken(self.token(), '(')
        variable = self.parse_variable()
--
        sense = int(self.token())
        self.assertToken(self.token(), ')')

        def _handle_pred_f(sent_index, word_indices):
            return BoxerPred(self.discourse_id, sent_index, word_indices, variable, name, pos, sense)
        return _handle_pred_f

    def _handle_named(self):
        #named(x0, john, per, 0)
        self.assertToken(self.token(), '(')
        variable = self.parse_variable()
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: BoxerNamed(self.discourse_id, sent_index, word_indices, variable, name, type, sense)

    def _handle_rel(self):
        #rel(_G3993, _G3943, agent, 0)
        self.assertToken(self.token(), '(')
        var1 = self.parse_variable()
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: BoxerRel(self.discourse_id, sent_index, word_indices, var1, var2, rel, sense)

    def _handle_timex(self):
        #timex(_G18322, date([]: (+), []:'XXXX', [1004]:'04', []:'XX'))
        self.assertToken(self.token(), '(')
        arg = self.parse_variable()
--
        self.assertToken(self.token(), ')')
        return new_conds

    def _handle_time_expression(self, arg):
        #date([]: (+), []:'XXXX', [1004]:'04', []:'XX')
        tok = self.token()
        self.assertToken(self.token(), '(')
--
        return [lambda sent_index, word_indices: BoxerPred(self.discourse_id, sent_index, word_indices, arg, tok, 'n', 0)] + \
               [lambda sent_index, word_indices: cond for cond in conds]

    def _handle_date(self, arg):
        #[]: (+), []:'XXXX', [1004]:'04', []:'XX'
        conds = []
        (sent_index, word_indices), = self._sent_and_word_indices(self._parse_index_list())
--

        return conds

    def _handle_time(self, arg):
        #time([1018]:'18', []:'XX', []:'XX')
        conds = []
        self._parse_index_list()
--

        return conds

    def _handle_card(self):
        #card(_G18535, 28, ge)
        self.assertToken(self.token(), '(')
        variable = self.parse_variable()
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: BoxerCard(self.discourse_id, sent_index, word_indices, variable, value, type)

    def _handle_prop(self):
        #prop(_G15949, drs(...))
        self.assertToken(self.token(), '(')
        variable = self.parse_variable()
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: BoxerProp(self.discourse_id, sent_index, word_indices, variable, drs)

    def _parse_index_list(self):
        #[1001,1002]:
        indices = []
        self.assertToken(self.token(), '[')
--
        self.assertToken(self.token(), ':')
        return indices

    def parse_drs(self):
        #drs([[1001]:_G3943],
        #    [[1002]:pred(_G3943, dog, n, 0)]
        #   )
--
        self.assertToken(self.token(), ')')
        return BoxerDrs(label, list(refs), conds)

    def _handle_binary_expression(self, make_callback):
        self.assertToken(self.token(), '(')
        drs1 = self.parse_Expression(None)
        self.assertToken(self.token(), ',')
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: make_callback(sent_index, word_indices, drs1, drs2)

    def _handle_alfa(self, make_callback):
        self.assertToken(self.token(), '(')
        type = self.token()
        self.assertToken(self.token(), ',')
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: make_callback(sent_index, word_indices, drs1, drs2)

    def _handle_eq(self):
        self.assertToken(self.token(), '(')
        var1 = self.parse_variable()
        self.assertToken(self.token(), ',')
--
        return lambda sent_index, word_indices: BoxerEq(self.discourse_id, sent_index, word_indices, var1, var2)


    def _handle_whq(self):
        self.assertToken(self.token(), '(')
        self.assertToken(self.token(), '[')
        ans_types = []
--
        self.assertToken(self.token(), ')')
        return lambda sent_index, word_indices: BoxerWhq(self.discourse_id, sent_index, word_indices, ans_types, d1, ref, d2)

    def _make_merge_expression(self, sent_index, word_indices, drs1, drs2):
        return BoxerDrs(drs1.label, drs1.refs + drs2.refs, drs1.conds + drs2.conds)

    def _make_or_expression(self, sent_index, word_indices, drs1, drs2):
        return BoxerOr(self.discourse_id, sent_index, word_indices, drs1, drs2)

    def _make_imp_expression(self, sent_index, word_indices, drs1, drs2):
        return BoxerDrs(drs1.label, drs1.refs, drs1.conds, drs2)

    def parse_variable(self):
        var = self.token()
        assert re.match('^[ex]\d+$', var), var
        return int(var[1:])

    def parse_index(self):
        return int(self.token())

    def _sent_and_word_indices(self, indices):
        """
        :return: list of (sent_index, word_indices) tuples
        """
--
    """
    Reparse the str form of subclasses of ``AbstractBoxerDrs``
    """
    def __init__(self, discourse_id=None):
        DrtParser.__init__(self)
        self.discourse_id = discourse_id

    def get_all_symbols(self):
        return [DrtTokens.OPEN, DrtTokens.CLOSE, DrtTokens.COMMA, DrtTokens.OPEN_BRACKET, DrtTokens.CLOSE_BRACKET]

    def attempt_adjuncts(self, expression, context):
        return expression

    def handle(self, tok, context):
        try:
            if tok == 'drs':
                self.assertNextToken(DrtTokens.OPEN)
--
            raise ParseException(self._currentIndex, str(e))
        assert False, repr(tok)

    def nullableIntToken(self):
        t = self.token()
        return [None,int(t)][t != 'None']

    def get_next_token_variable(self, description):
        try:
            return self.token()
        except ExpectedMoreTokensException as e:
--


class AbstractBoxerDrs(object):
    def variables(self):
        """
        :return: (set<variables>, set<events>, set<propositions>)
        """
        variables, events, propositions = self._variables()
        return (variables - (events | propositions), events, propositions - events)

    def variable_types(self):
        vartypes = {}
        for t,vars in zip(('z','e','p'), self.variables()):
            for v in vars:
                vartypes[v] = t
        return vartypes

    def _variables(self):
        """
        :return: (set<variables>, set<events>, set<propositions>)
        """
        return (set(), set(), set())

    def atoms(self):
        return set()

    def clean(self):
        return self

    def _clean_name(self, name):
        return name.replace('-','_').replace("'", "_")

    def renumber_sentences(self, f):
        return self

    def __hash__(self):
        return hash("%s" % self)


@python_2_unicode_compatible
class BoxerDrs(AbstractBoxerDrs):
    def __init__(self, label, refs, conds, consequent=None):
        AbstractBoxerDrs.__init__(self)
        self.label = label
        self.refs = refs
        self.conds = conds
        self.consequent = consequent

    def _variables(self):
        variables = (set(), set(), set())
        for cond in self.conds:
            for s,v in zip(variables, cond._variables()):
--
                s.update(v)
        return variables

    def atoms(self):
        atoms = reduce(operator.or_, (cond.atoms() for cond in self.conds), set())
        if self.consequent is not None:
            atoms.update(self.consequent.atoms())
        return atoms

    def clean(self):
        consequent = (self.consequent.clean() if self.consequent else None)
        return BoxerDrs(self.label, self.refs, [c.clean() for c in self.conds], consequent)

    def renumber_sentences(self, f):
        consequent = (self.consequent.renumber_sentences(f) if self.consequent else None)
        return BoxerDrs(self.label, self.refs, [c.renumber_sentences(f) for c in self.conds], consequent)

    def __repr__(self):
        s = 'drs(%s, [%s], [%s])' % (self.label,
                                    ', '.join("%s" % r for r in self.refs),
                                    ', '.join("%s" % c for c in self.conds))
--
            s = 'imp(%s, %s)' % (s, self.consequent)
        return s

    def __eq__(self, other):
        return self.__class__ == other.__class__ and \
               self.label == other.label and \
               self.refs == other.refs and \
--
               reduce(operator.and_, (c1==c2 for c1,c2 in zip(self.conds, other.conds))) and \
               self.consequent == other.consequent

    def __ne__(self, other):
        return not self == other

    __hash__ = AbstractBoxerDrs.__hash__
--

@python_2_unicode_compatible
class BoxerNot(AbstractBoxerDrs):
    def __init__(self, drs):
        AbstractBoxerDrs.__init__(self)
        self.drs = drs

    def _variables(self):
        return self.drs._variables()

    def atoms(self):
        return self.drs.atoms()

    def clean(self):
        return BoxerNot(self.drs.clean())

    def renumber_sentences(self, f):
        return BoxerNot(self.drs.renumber_sentences(f))

    def __repr__(self):
        return 'not(%s)' % (self.drs)

    def __eq__(self, other):
        return self.__class__ == other.__class__ and self.drs == other.drs

    def __ne__(self, other):
        return not self == other

    __hash__ = AbstractBoxerDrs.__hash__

@python_2_unicode_compatible
class BoxerIndexed(AbstractBoxerDrs):
    def __init__(self, discourse_id, sent_index, word_indices):
        AbstractBoxerDrs.__init__(self)
        self.discourse_id = discourse_id
        self.sent_index = sent_index
        self.word_indices = word_indices

    def atoms(self):
        return set([self])

    def __eq__(self, other):
        return self.__class__ == other.__class__ and \
               self.discourse_id == other.discourse_id and \
               self.sent_index == other.sent_index and \
               self.word_indices == other.word_indices and \
               reduce(operator.and_, (s==o for s,o in zip(self, other)))

    def __ne__(self, other):
        return not self == other

    __hash__ = AbstractBoxerDrs.__hash__

    def __repr__(self):
        s = '%s(%s, %s, [%s]' % (self._pred(), self.discourse_id,
                                 self.sent_index, ', '.join("%s" % wi for wi in self.word_indices))
        for v in self:
--
        return s + ')'

class BoxerPred(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var, name, pos, sense):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var = var
        self.name = name
        self.pos = pos
        self.sense = sense

    def _variables(self):
        return (set([self.var]), set(), set())

    def change_var(self, var):
        return BoxerPred(self.discourse_id, self.sent_index, self.word_indices, var, self.name, self.pos, self.sense)

    def clean(self):
        return BoxerPred(self.discourse_id, self.sent_index, self.word_indices, self.var, self._clean_name(self.name), self.pos, self.sense)

    def renumber_sentences(self, f):
        new_sent_index = f(self.sent_index)
        return BoxerPred(self.discourse_id, new_sent_index, self.word_indices, self.var, self.name, self.pos, self.sense)

    def __iter__(self):
        return iter((self.var, self.name, self.pos, self.sense))

    def _pred(self):
        return 'pred'

class BoxerNamed(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var, name, type, sense):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var = var
        self.name = name
        self.type = type
        self.sense = sense

    def _variables(self):
        return (set([self.var]), set(), set())

    def change_var(self, var):
        return BoxerNamed(self.discourse_id, self.sent_index, self.word_indices, var, self.name, self.type, self.sense)

    def clean(self):
        return BoxerNamed(self.discourse_id, self.sent_index, self.word_indices, self.var, self._clean_name(self.name), self.type, self.sense)

    def renumber_sentences(self, f):
        return BoxerNamed(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.name, self.type, self.sense)

    def __iter__(self):
        return iter((self.var, self.name, self.type, self.sense))

    def _pred(self):
        return 'named'

class BoxerRel(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var1, var2, rel, sense):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var1 = var1
        self.var2 = var2
        self.rel = rel
        self.sense = sense

    def _variables(self):
        return (set([self.var1, self.var2]), set(), set())

    def clean(self):
        return BoxerRel(self.discourse_id, self.sent_index, self.word_indices, self.var1, self.var2, self._clean_name(self.rel), self.sense)

    def renumber_sentences(self, f):
        return BoxerRel(self.discourse_id, f(self.sent_index), self.word_indices, self.var1, self.var2, self.rel, self.sense)

    def __iter__(self):
        return iter((self.var1, self.var2, self.rel, self.sense))

    def _pred(self):
        return 'rel'

class BoxerProp(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var, drs):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var = var
        self.drs = drs

    def _variables(self):
        return tuple(map(operator.or_, (set(), set(), set([self.var])), self.drs._variables()))

    def referenced_labels(self):
        return set([self.drs])

    def atoms(self):
        return self.drs.atoms()

    def clean(self):
        return BoxerProp(self.discourse_id, self.sent_index, self.word_indices, self.var, self.drs.clean())

    def renumber_sentences(self, f):
        return BoxerProp(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.drs.renumber_sentences(f))

    def __iter__(self):
        return iter((self.var, self.drs))

    def _pred(self):
        return 'prop'

class BoxerEq(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var1, var2):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var1 = var1
        self.var2 = var2

    def _variables(self):
        return (set([self.var1, self.var2]), set(), set())

    def atoms(self):
        return set()

    def renumber_sentences(self, f):
        return BoxerEq(self.discourse_id, f(self.sent_index), self.word_indices, self.var1, self.var2)

    def __iter__(self):
        return iter((self.var1, self.var2))

    def _pred(self):
        return 'eq'

class BoxerCard(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, var, value, type):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.var = var
        self.value = value
        self.type = type

    def _variables(self):
        return (set([self.var]), set(), set())

    def renumber_sentences(self, f):
        return BoxerCard(self.discourse_id, f(self.sent_index), self.word_indices, self.var, self.value, self.type)

    def __iter__(self):
        return iter((self.var, self.value, self.type))

    def _pred(self):
        return 'card'

class BoxerOr(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, drs1, drs2):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.drs1 = drs1
        self.drs2 = drs2

    def _variables(self):
        return tuple(map(operator.or_, self.drs1._variables(), self.drs2._variables()))

    def atoms(self):
        return self.drs1.atoms() | self.drs2.atoms()

    def clean(self):
        return BoxerOr(self.discourse_id, self.sent_index, self.word_indices, self.drs1.clean(), self.drs2.clean())

    def renumber_sentences(self, f):
        return BoxerOr(self.discourse_id, f(self.sent_index), self.word_indices, self.drs1, self.drs2)

    def __iter__(self):
        return iter((self.drs1, self.drs2))

    def _pred(self):
        return 'or'

class BoxerWhq(BoxerIndexed):
    def __init__(self, discourse_id, sent_index, word_indices, ans_types, drs1, variable, drs2):
        BoxerIndexed.__init__(self, discourse_id, sent_index, word_indices)
        self.ans_types = ans_types
        self.drs1 = drs1
        self.variable = variable
        self.drs2 = drs2

    def _variables(self):
        return tuple(map(operator.or_, (set([self.variable]), set(), set()), self.drs1._variables(), self.drs2._variables()))

    def atoms(self):
        return self.drs1.atoms() | self.drs2.atoms()

    def clean(self):
        return BoxerWhq(self.discourse_id, self.sent_index, self.word_indices, self.ans_types, self.drs1.clean(), self.variable, self.drs2.clean())

    def renumber_sentences(self, f):
        return BoxerWhq(self.discourse_id, f(self.sent_index), self.word_indices, self.ans_types, self.drs1, self.variable, self.drs2)

    def __iter__(self):
        return iter(('['+','.join(self.ans_types)+']', self.drs1, self.variable, self.drs2))

    def _pred(self):
        return 'whq'



class PassthroughBoxerDrsInterpreter(object):
    def interpret(self, ex):
        return ex


class NltkDrtBoxerDrsInterpreter(object):
    def __init__(self, occur_index=False):
        self._occur_index = occur_index

    def interpret(self, ex):
        """
        :param ex: ``AbstractBoxerDrs``
        :return: ``AbstractDrs``
--
            return DRS(drs1.refs + drs2.refs, drs1.conds + drs2.conds)
        assert False, '%s: %s' % (ex.__class__.__name__, ex)

    def _make_atom(self, pred, *args):
        accum = DrtVariableExpression(Variable(pred))
        for arg in args:
            accum = DrtApplicationExpression(accum, DrtVariableExpression(Variable(arg)))
        return accum

    def _add_occur_indexing(self, base, ex):
        if self._occur_index and ex.sent_index is not None:
            if ex.discourse_id:
                base += '_%s'  % ex.discourse_id
--

if __name__ == '__main__':
    opts = OptionParser("usage: %prog TEXT [options]")
    opts.add_option("--verbose", "-v", help="display verbose logs", action="store_true", default=False, dest="verbose")
    opts.add_option("--fol", "-f", help="output FOL", action="store_true", default=False, dest="fol")
    opts.add_option("--question", "-q", help="input is a question", action="store_true", default=False, dest="question")
    opts.add_option("--occur", "-o", help="occurrence index", action="store_true", default=False, dest="occur_index")
    (options, args) = opts.parse_args()

    if len(args) != 1:
--
    A Concept class, loosely based on SKOS
    (http://www.w3.org/TR/swbp-skos-core-guide/).
    """
    def __init__(self, prefLabel, arity, altLabels=[], closures=[], extension=set()):
        """
        :param prefLabel: the preferred label for the concept
        :type prefLabel: str
--
        #public access is via a list (for slicing)
        self.extension = sorted(list(extension))

    def __str__(self):
        #_extension = ''
        #for element in sorted(self.extension):
            #if isinstance(element, tuple):
--
        return "Label = '%s'\nArity = %s\nExtension = %s" % \
               (self.prefLabel, self.arity, self.extension)

    def __repr__(self):
        return "Concept('%s')" % self.prefLabel

    def augment(self, data):
        """
        Add more data to the ``Concept``'s extension set.

--
        return self._extension


    def _make_graph(self, s):
        """
        Convert a set of pairs into an adjacency linked list encoding of a graph.
        """
--
                g[x] = [y]
        return g

    def _transclose(self, g):
        """
        Compute the transitive closure of a graph represented as a linked list.
        """
--
                            g[x].append(y)
        return g

    def _make_pairs(self, g):
        """
        Convert an adjacency linked list back into a set of pairs.
        """
--
        return set(pairs)


    def close(self):
        """
        Close a binary relation in the ``Concept``'s extension set.

--
        self.extension = sorted(list(self._extension))


def clause2concepts(filename, rel_name, schema, closures=[]):
    """
    Convert a file of Prolog clauses into a list of ``Concept`` objects.

--

    return concepts

def cities2table(filename, rel_name, dbname, verbose=False, setup=False):
    """
    Convert a file of Prolog clauses into a database table.

--
        print("Committing update to %s" % dbname)
    cur.close()

def sql_query(dbname, query):
    """
    Execute an SQL query over a database.
    :param dbname: filename of persistent store
--
        warnings.warn("Make sure the database file %s is installed and uncompressed." % dbname)
        raise

def _str2records(filename, rel):
    """
    Read a file into memory and convert each relation clause into a list.
    """
--
            recs.append(record)
    return recs

def unary_concept(label, subj, records):
    """
    Make a unary concept out of the primary key in a record.

--
        c.augment(record[subj])
    return c

def binary_concept(label, closures, subj, obj, records):
    """
    Make a binary concept out of the primary key and another field in a record.

--
    return c


def process_bundle(rels):
    """
    Given a list of relation metadata bundles, make a corresponding
    dictionary of concepts, indexed by the relation name.
--
    return concepts


def make_valuation(concepts, read=False, lexicon=False):
    """
    Convert a list of ``Concept`` objects into a list of (label, extension) pairs;
    optionally create a ``Valuation`` object.
--
    else: return vals


def val_dump(rels, db):
    """
    Make a ``Valuation`` from a list of relation metadata bundles and dump to
    persistent database.
--
    db_out.close()


def val_load(db):
    """
    Load a ``Valuation`` from a persistent database.

--
        return val


#def alpha(str):
    #"""
    #Utility to filter out non-alphabetic constants.

--
            #return True


def label_indivs(valuation, lexicon=False):
    """
    Assign individual constants to the individuals in the domain of a ``Valuation``.

--
    valuation.update(pairs)
    return valuation

def make_lex(symbols):
    """
    Create lexical CFG rules for each individual symbol.

--
# Interface function to emulate other corpus readers
###########################################################################

def concepts(items = items):
    """
    Build a list of concepts corresponding to the relation names in ``items``.

--
###########################################################################


def main():
    import sys
    from optparse import OptionParser
    description = \
--
    """

    opts = OptionParser(description=description)
    opts.set_defaults(verbose=True, lex=False, vocab=False)
    opts.add_option("-s", "--store", dest="outdb",
                    help="store a valuation in DB", metavar="DB")
    opts.add_option("-l", "--load", dest="indb",
--
                    print(valuation)


def sql_demo():
    """
    Print out every row from the 'city.db' database.
    """
--

@python_2_unicode_compatible
class FStructure(dict):
    def safeappend(self, key, item):
        """
        Append 'item' to the list at 'key'.  If no list exists for 'key', then
        construct one.
--
            self[key] = []
        self[key].append(item)

    def __setitem__(self, key, value):
        dict.__setitem__(self, key.lower(), value)

    def __getitem__(self, key):
        return dict.__getitem__(self, key.lower())

    def __contains__(self, key):
        return dict.__contains__(self, key.lower())

    def to_glueformula_list(self, glue_dict):
        depgraph = self.to_depgraph()
        return glue_dict.to_glueformula_list(depgraph)

    def to_depgraph(self, rel=None):
        from nltk.parse.dependencygraph import DependencyGraph
        depgraph = DependencyGraph()
        nodelist = depgraph.nodelist
--

        return depgraph

    def _to_depgraph(self, nodelist, head, rel):
        index = len(nodelist)

        nodelist.append({'address': index,
--
                    raise Exception('feature %s is not an FStruct, a list, or a tuple' % feature)

    @staticmethod
    def read_depgraph(depgraph):
        return FStructure._read_depgraph(depgraph.root, depgraph)

    @staticmethod
    def _read_depgraph(node, depgraph, label_counter=None, parent=None):
        if not label_counter:
            label_counter = Counter()

--
            return fstruct

    @staticmethod
    def _make_label(value):
        """
        Pick an alphabetic character as identifier for an entity in the model.

--
        else:
            return letter

    def __repr__(self):
        return self.__unicode__().replace('\n', '')

    def __str__(self):
        return self.pprint()

    def pprint(self, indent=3):
        try:
            accum = '%s:[' % self.label
        except NameError:
--



def demo_read_depgraph():
    from nltk.parse.dependencygraph import DependencyGraph
    dg1 = DependencyGraph("""\
Esso       NNP     2       SUB
--
                            ImpExpression, NegatedExpression, OrExpression,
                            VariableExpression, skolem_function, unique_variable)

def skolemize(expression, univ_scope=None, used_variables=None):
    """
    Skolemize the expression and convert to conjunctive normal form (CNF)
    """
--
    else:
        raise Exception('\'%s\' cannot be skolemized' % expression)

def to_cnf(first, second):
    """
    Convert this split disjunction to conjunctive normal form (CNF)
    """
--
    Expression extends.
    """

    def applyto(self, other):
        return DrtApplicationExpression(self, other)

    def __neg__(self):
        return DrtNegatedExpression(self)

    def __and__(self, other):
        raise NotImplementedError()

    def __or__(self, other):
        assert isinstance(other, AbstractDrs)
        return DrtOrExpression(self, other)

    def __gt__(self, other):
        assert isinstance(other, AbstractDrs)
        if isinstance(self, DRS):
            return DRS(self.refs, self.conds, other)
--
            return DrtConcatenation(self.first, self.second, other)
        raise Exception('Antecedent of implication must be a DRS')

    def equiv(self, other, prover=None):
        """
        Check for logical equivalence.
        Pass the expression (self <-> other) to the theorem prover.
--
        return f1.equiv(f2, prover)

    @property
    def type(self):
        raise AttributeError("'%s' object has no attribute 'type'" %
                             self.__class__.__name__)

    def typecheck(self, signature=None):
        raise NotImplementedError()

    def __add__(self, other):
        return DrtConcatenation(self, other, None)

    def get_refs(self, recursive=False):
        """
        Return the set of discourse referents in this DRS.
        :param recursive: bool Also find discourse referents in subterms?
--
        """
        raise NotImplementedError()

    def is_pronoun_function(self):
        """ Is self of the form "PRO(x)"? """
        return isinstance(self, DrtApplicationExpression) and \
               isinstance(self.function, DrtAbstractVariableExpression) and \
               self.function.variable.name == DrtTokens.PRONOUN and \
               isinstance(self.argument, DrtIndividualVariableExpression)

    def make_EqualityExpression(self, first, second):
        return DrtEqualityExpression(first, second)

    def make_VariableExpression(self, variable):
        return DrtVariableExpression(variable)

    def resolve_anaphora(self):
        return resolve_anaphora(self)

    def eliminate_equality(self):
        return self.visit_structured(lambda e: e.eliminate_equality(),
                                     self.__class__)

    def pprint(self):
        """
        Draw the DRS
        """
        print(self.pretty())

    def pretty(self):
        """
        Draw the DRS
        :return: the pretty print string
        """
        return '\n'.join(self._pretty())

    def draw(self):
        DrsDrawer(self).draw()


@python_2_unicode_compatible
class DRS(AbstractDrs, Expression):
    """A Discourse Representation Structure."""
    def __init__(self, refs, conds, consequent=None):
        """
        :param refs: list of ``DrtIndividualVariableExpression`` for the
        discourse referents
--
        self.conds = conds
        self.consequent = consequent

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """Replace all instances of variable v with expression E in self,
        where v is free in self."""
        if variable in self.refs:
--
                        for cond in self.conds],
                       consequent)

    def free(self):
        """:see: Expression.free()"""
        conds_free = reduce(operator.or_, [c.free() for c in self.conds], set())
        if self.consequent:
            conds_free.update(self.consequent.free())
        return conds_free - set(self.refs)

    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        if recursive:
            conds_refs = self.refs + sum((c.get_refs(True) for c in self.conds), [])
--
        else:
            return self.refs

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        parts = list(map(function, self.conds))
        if self.consequent:
            parts.append(function(self.consequent))
        return combinator(parts)

    def visit_structured(self, function, combinator):
        """:see: Expression.visit_structured()"""
        consequent = (function(self.consequent) if self.consequent else None)
        return combinator(self.refs, list(map(function, self.conds)), consequent)

    def eliminate_equality(self):
        drs = self
        i = 0
        while i < len(drs.conds):
--
        consequent = (drs.consequent.eliminate_equality() if drs.consequent else None)
        return DRS(drs.refs, conds, consequent)

    def fol(self):
        if self.consequent:
            accum = None
            if self.conds:
--
                accum = ExistsExpression(ref, accum)
            return accum

    def _pretty(self):
        refs_line = ' '.join(self._order_ref_strings(self.refs))

        cond_lines = [cond for cond_line in [filter(lambda s: s.strip(), cond._pretty())
--
                                                        self.consequent._pretty())
        return drs

    def _order_ref_strings(self, refs):
        strings = ["%s" % ref for ref in refs]
        ind_vars = []
        func_vars = []
--
               sorted(func_vars, key=lambda v: (v[0], int([v[1:],-1][len(v[1:])==0]))) + \
               sorted(ind_vars, key=lambda v: (v[0], int([v[1:],-1][len(v[1:])==0])))

    def __eq__(self, other):
        r"""Defines equality modulo alphabetic variance.
        If we are comparing \x.M  and \y.N, then check equality of M and N[x/y]."""
        if isinstance(other, DRS):
            if len(self.refs) == len(other.refs):
--
                    return True
        return False

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__

    def __str__(self):
        drs = '([%s],[%s])' % (','.join(self._order_ref_strings(self.refs)),
                               ', '.join("%s" % cond for cond in self.conds)) # map(str, self.conds)))
        if self.consequent:
--
        return drs


def DrtVariableExpression(variable):
    """
    This is a factory method that instantiates and returns a subtype of
    ``DrtAbstractVariableExpression`` appropriate for the given variable.
--


class DrtAbstractVariableExpression(AbstractDrs, AbstractVariableExpression):
    def fol(self):
        return self

    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        return []

    def _pretty(self):
        s = "%s" % self
        blank = ' '*len(s)
        return [blank, blank, s, blank]

    def eliminate_equality(self):
        return self

class DrtIndividualVariableExpression(DrtAbstractVariableExpression, IndividualVariableExpression):
--

@python_2_unicode_compatible
class DrtProposition(AbstractDrs, Expression):
    def __init__(self, variable, drs):
        self.variable = variable
        self.drs = drs

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        if self.variable == variable:
            assert isinstance(expression, DrtAbstractVariableExpression), "Can only replace a proposition label with a variable"
            return DrtProposition(expression.variable, self.drs.replace(variable, expression, replace_bound, alpha_convert))
        else:
            return DrtProposition(self.variable, self.drs.replace(variable, expression, replace_bound, alpha_convert))

    def eliminate_equality(self):
        return DrtProposition(self.variable, self.drs.eliminate_equality())

    def get_refs(self, recursive=False):
        return (self.drs.get_refs(True) if recursive else [])

    def __eq__(self, other):
        return self.__class__ == other.__class__ and \
               self.variable == other.variable and \
               self.drs == other.drs

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__

    def fol(self):
        return self.drs.fol()

    def _pretty(self):
        drs_s = self.drs._pretty()
        blank = ' ' * len("%s" % self.variable)
        return ([blank                + ' ' + line for line in drs_s[:1]] +
                ["%s" % self.variable + ':' + line for line in drs_s[1:2]] +
                [blank                + ' ' + line for line in drs_s[2:]])

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        return combinator([function(self.drs)])

    def visit_structured(self, function, combinator):
        """:see: Expression.visit_structured()"""
        return combinator(self.variable, function(self.drs))

    def __str__(self):
        return 'prop(%s, %s)' % (self.variable, self.drs)


class DrtNegatedExpression(AbstractDrs, NegatedExpression):
    def fol(self):
        return NegatedExpression(self.term.fol())

    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        return self.term.get_refs(recursive)

    def _pretty(self):
        term_lines = self.term._pretty()
        return (['    ' + line for line in term_lines[:2]] +
                ['__  ' + line for line in term_lines[2:3]] +
--
                ['    ' + line for line in term_lines[4:]])

class DrtLambdaExpression(AbstractDrs, LambdaExpression):
    def alpha_convert(self, newvar):
        """Rename all occurrences of the variable introduced by this variable
        binder in the expression to ``newvar``.
        :param newvar: ``Variable``, for the new variable
--
        return self.__class__(newvar, self.term.replace(self.variable,
                          DrtVariableExpression(newvar), True))

    def fol(self):
        return LambdaExpression(self.variable, self.term.fol())

    def _pretty(self):
        variables = [self.variable]
        term = self.term
        while term.__class__ == self.__class__:
--
                ['    ' + blank      + line for line in term_lines[3:]])

class DrtBinaryExpression(AbstractDrs, BinaryExpression):
    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        return self.first.get_refs(True) + self.second.get_refs(True) if recursive else []

    def _pretty(self):
        return DrtBinaryExpression._assemble_pretty(self._pretty_subex(self.first), self.getOp(), self._pretty_subex(self.second))

    @staticmethod
    def _assemble_pretty(first_lines, op, second_lines):
        max_lines = max(len(first_lines), len(second_lines))
        first_lines = _pad_vertically(first_lines, max_lines)
        second_lines = _pad_vertically(second_lines, max_lines)
--
                ['(' + first_line + ' ' + op    + ' ' + second_line + ')' for first_line, second_line in first_second_lines[2:3]] +
                [' ' + first_line + ' ' + blank + ' ' + second_line + ' ' for first_line, second_line in first_second_lines[3:]])

    def _pretty_subex(self, subex):
        return subex._pretty()

class DrtBooleanExpression(DrtBinaryExpression, BooleanExpression):
    pass

class DrtOrExpression(DrtBooleanExpression, OrExpression):
    def fol(self):
        return OrExpression(self.first.fol(), self.second.fol())

    def _pretty_subex(self, subex):
        if isinstance(subex, DrtOrExpression):
            return [line[1:-1] for line in subex._pretty()]
        return DrtBooleanExpression._pretty_subex(self, subex)

class DrtEqualityExpression(DrtBinaryExpression, EqualityExpression):
    def fol(self):
        return EqualityExpression(self.first.fol(), self.second.fol())

@python_2_unicode_compatible
class DrtConcatenation(DrtBooleanExpression):
    """DRS of the form '(DRS + DRS)'"""
    def __init__(self, first, second, consequent=None):
        DrtBooleanExpression.__init__(self, first, second)
        self.consequent = consequent

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """Replace all instances of variable v with expression E in self,
        where v is free in self."""
        first = self.first
--

        return self.__class__(first, second, consequent)

    def eliminate_equality(self):
        #TODO: at some point.  for now, simplify.
        drs = self.simplify()
        assert not isinstance(drs, DrtConcatenation)
        return drs.eliminate_equality()

    def simplify(self):
        first = self.first.simplify()
        second = self.second.simplify()
        consequent = (self.consequent.simplify() if self.consequent else None)
--
        else:
            return self.__class__(first, second, consequent)

    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        refs = self.first.get_refs(recursive) + self.second.get_refs(recursive)
        if self.consequent and recursive:
            refs.extend(self.consequent.get_refs(True))
        return refs

    def getOp(self):
        return DrtTokens.DRS_CONC

    def __eq__(self, other):
        r"""Defines equality modulo alphabetic variance.
        If we are comparing \x.M  and \y.N, then check equality of M and N[x/y]."""
        if isinstance(other, DrtConcatenation):
            self_refs = self.get_refs()
--
                        self.consequent == converted_other.consequent
        return False

    def __ne__(self, other):
        return not self == other

    __hash__ = DrtBooleanExpression.__hash__

    def fol(self):
        e = AndExpression(self.first.fol(), self.second.fol())
        if self.consequent:
            e = ImpExpression(e, self.consequent.fol())
        return e

    def _pretty(self):
        drs = DrtBinaryExpression._assemble_pretty(self._pretty_subex(self.first),
                                                   self.getOp(),
                                                   self._pretty_subex(self.second))
--
                                                       self._pretty(self.consequent))
        return drs

    def _pretty_subex(self, subex):
        if isinstance(subex, DrtConcatenation):
            return [line[1:-1] for line in subex._pretty()]
        return DrtBooleanExpression._pretty_subex(self, subex)


    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        if self.consequent:
            return combinator([function(self.first), function(self.second), function(self.consequent)])
        else:
            return combinator([function(self.first), function(self.second)])

    def __str__(self):
        first = self._str_subex(self.first)
        second = self._str_subex(self.second)
        drs = Tokens.OPEN + first + ' ' + self.getOp() \
--
                   "%s" % self.consequent + DrtTokens.CLOSE
        return drs

    def _str_subex(self, subex):
        s = "%s" % subex
        if isinstance(subex, DrtConcatenation) and subex.consequent is None:
            return s[1:-1]
--


class DrtApplicationExpression(AbstractDrs, ApplicationExpression):
    def fol(self):
        return ApplicationExpression(self.function.fol(), self.argument.fol())

    def get_refs(self, recursive=False):
        """:see: AbstractExpression.get_refs()"""
        return (self.function.get_refs(True) + self.argument.get_refs(True)
                if recursive else [])

    def _pretty(self):
        function, args = self.uncurry()
        function_lines = function._pretty()
        args_lines = [arg._pretty() for arg in args]
--
                [func_line + ' ' + ' '.join(args_line) + ' ' for func_line, args_line in func_args_lines[3:]])


def _pad_vertically(lines, max_lines):
    pad_line = [' ' * len(lines[0])]
    return lines + pad_line * (max_lines - len(lines))


@python_2_unicode_compatible
class PossibleAntecedents(list, AbstractDrs, Expression):
    def free(self):
        """Set of free variables."""
        return set(self)

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """Replace all instances of variable v with expression E in self,
        where v is free in self."""
        result = PossibleAntecedents()
--
                self.append(item)
        return result

    def _pretty(self):
        s = "%s" % self
        blank = ' ' * len(s)
        return [blank, blank, s]

    def __str__(self):
        return '[' + ','.join("%s" % it for it in self) + ']'


--
    pass


def resolve_anaphora(expression, trail=[]):
    if isinstance(expression, ApplicationExpression):
        if expression.is_pronoun_function():
            possible_antecedents = PossibleAntecedents()
--
    TOPSPACE = 10  #Space above whole DRS
    OUTERSPACE = 6 #Space to the left, right, and bottom of the whle DRS

    def __init__(self, drs, size_canvas=True, canvas=None):
        """
        :param drs: ``AbstractDrs``, The DRS to be drawn
        :param size_canvas: bool, True if the canvas size should be the exact size of the DRS
--
        self.drs = drs
        self.master = master

    def _get_text_height(self):
        """Get the height of a line of text"""
        return self.canvas.font.metrics("linespace")

    def draw(self, x=OUTERSPACE, y=TOPSPACE):
        """Draw the DRS"""
        self._handle(self.drs, self._draw_command, x, y)

--
        else:
            return self._visit(self.drs, x, y)

    def _visit(self, expression, x, y):
        """
        Return the bottom-rightmost point without actually drawing the item

--
        """
        return self._handle(expression, self._visit_command, x, y)

    def _draw_command(self, item, x, y):
        """
        Draw the given item at the given location

--

        return self._visit_command(item, x, y)

    def _visit_command(self, item, x, y):
        """
        Return the bottom-rightmost point without actually drawing the item

--
        elif isinstance(item, tuple):
            return item

    def _handle(self, expression, command, x=0, y=0):
        """
        :param expression: the expression to handle
        :param command: the function to apply, either _draw_command or _visit_command
--

        return (right, bottom)

    def _handle_VariableExpression(self, expression, command, x, y):
        return command("%s" % expression, x, y)

    def _handle_NegatedExpression(self, expression, command, x, y):
        # Find the width of the negation symbol
        right = self._visit_command(DrtTokens.NOT, x, y)[0]

--

        return (right, bottom)

    def _handle_DRS(self, expression, command, x, y):
        left = x + self.BUFFER #indent the left side
        bottom = y + self.BUFFER #indent the top

--
        max_right += self.BUFFER
        return command((max_right, bottom), x, y)

    def _handle_ApplicationExpression(self, expression, command, x, y):
        function, args = expression.uncurry()
        if not isinstance(function, DrtAbstractVariableExpression):
            #It's not a predicate expression ("P(x,y)"), so leave arguments curried
--

        return (right, max_bottom)

    def _handle_LambdaExpression(self, expression, command, x, y):
        # Find the width of the lambda symbol and abstracted variables
        variables = DrtTokens.LAMBDA + "%s" % expression.variable + DrtTokens.DOT
        right = self._visit_command(variables, x, y)[0]
--

        return (right, bottom)

    def _handle_BinaryExpression(self, expression, command, x, y):
        # Get the full height of the line, based on the operands
        first_height = self._visit(expression.first, 0, 0)[1]
        second_height = self._visit(expression.second, 0, 0)[1]
--

        return (right, max(first_bottom, second_bottom))

    def _handle_DrtProposition(self, expression, command, x, y):
        # Find the width of the negation symbol
        right = command(expression.variable, x, y)[0]

--

        return (right, bottom)

    def _get_centered_top(self, top, full_height, item_height):
        """Get the y-coordinate of the point that a figure should start at if
        its height is 'item_height' and it needs to be centered in an area that
        starts at 'top' and is 'full_height' tall."""
--

class DrtParser(LogicParser):
    """A lambda calculus expression parser."""
    def __init__(self):
        LogicParser.__init__(self)

        self.operator_precedence = dict(
--
                               [(x,8) for x in DrtTokens.IMP_LIST]                + \
                               [(None,9)])

    def get_all_symbols(self):
        """This method exists to be overridden"""
        return DrtTokens.SYMBOLS

    def isvariable(self, tok):
        return tok not in DrtTokens.TOKENS

    def handle(self, tok, context):
        """This method is intended to be overridden for logics that
        use different operators or expressions"""
        if tok in DrtTokens.NOT_LIST:
--
            else:
                return self.handle_variable(tok, context)

    def make_NegatedExpression(self, expression):
        return DrtNegatedExpression(expression)

    def handle_DRS(self, tok, context):
        # a DRS
        refs = self.handle_refs()
        if self.inRange(0) and self.token(0) == DrtTokens.COMMA: #if there is a comma (it's optional)
--
        self.assertNextToken(DrtTokens.CLOSE)
        return DRS(refs, conds, None)

    def handle_refs(self):
        self.assertNextToken(DrtTokens.OPEN_BRACKET)
        refs = []
        while self.inRange(0) and self.token(0) != DrtTokens.CLOSE_BRACKET:
--
        self.assertNextToken(DrtTokens.CLOSE_BRACKET)
        return refs

    def handle_conds(self, context):
        self.assertNextToken(DrtTokens.OPEN_BRACKET)
        conds = []
        while self.inRange(0) and self.token(0) != DrtTokens.CLOSE_BRACKET:
--
        self.assertNextToken(DrtTokens.CLOSE_BRACKET)
        return conds

    def handle_prop(self, tok, context):
        variable = self.make_VariableExpression(tok)
        self.assertNextToken(':')
        drs = self.parse_Expression(DrtTokens.COLON)
        return DrtProposition(variable, drs)

    def make_EqualityExpression(self, first, second):
        """This method serves as a hook for other logic parsers that
        have different equality expression classes"""
        return DrtEqualityExpression(first, second)

    def get_BooleanExpression_factory(self, tok):
        """This method serves as a hook for other logic parsers that
        have different boolean operators"""
        if tok == DrtTokens.DRS_CONC:
--
        elif tok in DrtTokens.OR_LIST:
            return DrtOrExpression
        elif tok in DrtTokens.IMP_LIST:
            def make_imp_expression(first, second):
                if isinstance(first, DRS):
                    return DRS(first.refs, first.conds, second)
                if isinstance(first, DrtConcatenation):
--
        else:
            return None

    def make_BooleanExpression(self, factory, first, second):
        return factory(first, second)

    def make_ApplicationExpression(self, function, argument):
        return DrtApplicationExpression(function, argument)

    def make_VariableExpression(self, name):
        return DrtVariableExpression(Variable(name))

    def make_LambdaExpression(self, variables, term):
        return DrtLambdaExpression(variables, term)


def demo():
    print('='*20 + 'TEST PARSE' + '='*20)
    parser = DrtParser()
    print(parser.parse(r'([x,y],[sees(x,y)])'))
--
    parser.parse(r"\P.\Q.(([x],[]) + P(x) + Q(x))(\x.([],[dog(x)]))").pprint()


def test_draw():
    expressions = [
            r'x',
            r'([],[])',
--
## Utility functions for connecting parse output to semantics
##############################################################

def batch_parse(inputs, grammar, trace=0):
    """
    Convert input sentences into syntactic trees.

--
        parses.append(syntrees)
    return parses

def root_semrep(syntree, semkey='SEM'):
    """
    Find the semantic representation at the root of a tree.

--
        print("has no specification for the feature %s" % semkey)
    raise

def batch_interpret(inputs, grammar, semkey='SEM', trace=0):
    """
    Add the semantic representation to each syntactic parse tree
    of each input sentence.
--
    return [[(syn, root_semrep(syn, semkey)) for syn in syntrees]
            for syntrees in batch_parse(inputs, grammar, trace=trace)]

def batch_evaluate(inputs, grammar, model, assignment, trace=0):
    """
    Add the truth-in-a-model value to each semantic representation
    for each syntactic parse of each input sentences.
--
                                (\([^)]+\))  # tuple-expression
                                \s*""", re.VERBOSE)

def parse_valuation_line(s, encoding=None):
    """
    Parse a line in a valuation file.

--
        value = set(set_elements)
    return symbol, value

def parse_valuation(s, encoding=None):
    """
    Convert a valuation file into a valuation.

--
    return val


def demo_model0():
    global m0, g0
    #Initialize a valuation of non-logical constants."""
    v = [('john', 'b1'),
--
    g0 = evaluate.Assignment(dom)


def read_sents(filename, encoding='utf8'):
    with codecs.open(filename, 'r', encoding) as fp:
        sents = [l.rstrip() for l in fp]

--
    sents = [l for l in sents if not l[0] == '#']
    return sents

def demo_legacy_grammar():
    """
    Check that batch_interpret() is compatible with legacy grammars that use
    a lowercase 'sem' feature.

    Define 'test.fcfg' to be the following

    """
    from nltk.grammar import parse_fcfg
--
        print()
        print("output: ", sem)

def demo():
    import sys
    from optparse import OptionParser
    description = \
--

    opts = OptionParser(description=description)

    opts.set_defaults(evaluate=True, beta=True, syntrace=0,
                      semtrace=0, demo='default', grammar='', sentences='')

    opts.add_option("-d", "--demo", dest="demo",
                    help="choose demo D; omit this for the default demo, or specify 'chat80'", metavar="D")
    opts.add_option("-g", "--gram", dest="grammar",
                    help="read in grammar G", metavar="G")
    opts.add_option("-m", "--model", dest="model",
--

import re
import operator
from collections import defaultdict
from functools import reduce

from nltk.internals import Counter
--
    SYMBOLS = [x for x in TOKENS if re.match(r'^[-\\.(),!&^|>=<]*$', x)]


def boolean_ops():
    """
    Boolean operators
    """
--
    for pair in zip(names, [Tokens.NOT, Tokens.AND, Tokens.OR, Tokens.IMP, Tokens.IFF]):
        print("%-15s\t%s" %  pair)

def equality_preds():
    """
    Equality predicates
    """
--
    for pair in zip(names, [Tokens.EQ, Tokens.NEQ]):
        print("%-15s\t%s" %  pair)

def binding_ops():
    """
    Binding operators
    """
--
@total_ordering
@python_2_unicode_compatible
class Variable(object):
    def __init__(self, name):
        """
        :param name: the name of the variable
        """
        assert isinstance(name, string_types), "%s is not a string" % name
        self.name = name

    def __eq__(self, other):
        return isinstance(other, Variable) and self.name == other.name

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, Variable):
            raise TypeError
        return self.name < other.name

    def substitute_bindings(self, bindings):
        return bindings.get(self, self)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.name

    def __repr__(self):
        return "Variable('%s')" % self.name


def unique_variable(pattern=None, ignore=None):
    """
    Return a new, unique variable.

--
        v = Variable("%s%s" % (prefix, _counter.get()))
    return v

def skolem_function(univ_scope=None):
    """
    Return a skolem function over the variables in univ_scope
    param univ_scope
--

@python_2_unicode_compatible
class Type(object):
    def __repr__(self):
        return "%s" % self

    def __hash__(self):
        return hash("%s" % self)

@python_2_unicode_compatible
class ComplexType(Type):
    def __init__(self, first, second):
        assert(isinstance(first, Type)), "%s is not a Type" % first
        assert(isinstance(second, Type)), "%s is not a Type" % second
        self.first = first
        self.second = second

    def __eq__(self, other):
        return isinstance(other, ComplexType) and \
               self.first == other.first and \
               self.second == other.second

    def __ne__(self, other):
        return not self == other

    __hash__ = Type.__hash__

    def matches(self, other):
        if isinstance(other, ComplexType):
            return self.first.matches(other.first) and \
                   self.second.matches(other.second)
        else:
            return self == ANY_TYPE

    def resolve(self, other):
        if other == ANY_TYPE:
            return self
        elif isinstance(other, ComplexType):
--
        else:
            return None

    def __str__(self):
        if self == ANY_TYPE:
            return "%s" % ANY_TYPE
        else:
            return '<%s,%s>' % (self.first, self.second)

    def str(self):
        if self == ANY_TYPE:
            return ANY_TYPE.str()
        else:
            return '(%s -> %s)' % (self.first.str(), self.second.str())

class BasicType(Type):
    def __eq__(self, other):
        return isinstance(other, BasicType) and ("%s" % self) == ("%s" % other)

    def __ne__(self, other):
        return not self == other

    __hash__ = Type.__hash__

    def matches(self, other):
        return other == ANY_TYPE or self == other

    def resolve(self, other):
        if self.matches(other):
            return self
        else:
--

@python_2_unicode_compatible
class EntityType(BasicType):
    def __str__(self):
        return 'e'

    def str(self):
        return 'IND'

@python_2_unicode_compatible
class TruthValueType(BasicType):
    def __str__(self):
        return 't'

    def str(self):
        return 'BOOL'

@python_2_unicode_compatible
class EventType(BasicType):
    def __str__(self):
        return 'v'

    def str(self):
        return 'EVENT'

@python_2_unicode_compatible
class AnyType(BasicType, ComplexType):
    def __init__(self):
        pass

    @property
    def first(self): return self

    @property
    def second(self): return self

    def __eq__(self, other):
        return isinstance(other, AnyType) or other.__eq__(self)

    def __ne__(self, other):
        return not self == other

    __hash__ = Type.__hash__

    def matches(self, other):
        return True

    def resolve(self, other):
        return other

    def __str__(self):
        return '?'

    def str(self):
        return 'ANY'


--
ANY_TYPE = AnyType()


def parse_type(type_string):
    assert isinstance(type_string, string_types)
    type_string = type_string.replace(' ', '') #remove spaces

--


class TypeException(Exception):
    def __init__(self, msg):
        Exception.__init__(self, msg)

class InconsistentTypeHierarchyException(TypeException):
    def __init__(self, variable, expression=None):
        if expression:
            msg = "The variable '%s' was found in multiple places with different"\
                " types in '%s'." % (variable, expression)
--
        Exception.__init__(self, msg)

class TypeResolutionException(TypeException):
    def __init__(self, expression, other_type):
        Exception.__init__(self, "The type of '%s', '%s', cannot be "
                           "resolved with type '%s'" % \
                           (expression, expression.type, other_type))

class IllegalTypeException(TypeException):
    def __init__(self, expression, other_type, allowed_type):
        Exception.__init__(self, "Cannot set type of %s '%s' to '%s'; "
                           "must match type '%s'." %
                           (expression.__class__.__name__, expression,
                            other_type, allowed_type))


def typecheck(expressions, signature=None):
    """
    Ensure correct typing across a collection of ``Expression`` objects.
    :param expressions: a collection of expressions
--
    An interface for classes that can perform substitutions for
    variables.
    """
    def substitute_bindings(self, bindings):
        """
        :return: The object that is obtained by replacing
            each variable bound by ``bindings`` with its values.
--
        """
        raise NotImplementedError()

    def variables(self):
        """
        :return: A list of all variables in this object.
        """
--
class Expression(SubstituteBindingsI):
    """This is the base abstract object for all logical expressions"""

    def __call__(self, other, *additional):
        accum = self.applyto(other)
        for a in additional:
            accum = accum(a)
        return accum

    def applyto(self, other):
        assert isinstance(other, Expression), "%s is not an Expression" % other
        return ApplicationExpression(self, other)

    def __neg__(self):
        return NegatedExpression(self)

    def negate(self):
        """If this is a negated expression, remove the negation.
        Otherwise add a negation."""
        return -self

    def __and__(self, other):
        if not isinstance(other, Expression):
            raise TypeError("%s is not an Expression" % other)
        return AndExpression(self, other)

    def __or__(self, other):
        if not isinstance(other, Expression):
            raise TypeError("%s is not an Expression" % other)
        return OrExpression(self, other)

    def __gt__(self, other):
        if not isinstance(other, Expression):
            raise TypeError("%s is not an Expression" % other)
        return ImpExpression(self, other)

    def __lt__(self, other):
        if not isinstance(other, Expression):
            raise TypeError("%s is not an Expression" % other)
        return IffExpression(self, other)

    def __eq__(self, other):
        raise NotImplementedError()

    def __ne__(self, other):
        return not self == other

    def equiv(self, other, prover=None):
        """
        Check for logical equivalence.
        Pass the expression (self <-> other) to the theorem prover.
--
        bicond = IffExpression(self.simplify(), other.simplify())
        return prover.prove(bicond)

    def __hash__(self):
        return hash(repr(self))

    def substitute_bindings(self, bindings):
        expr = self
        for var in expr.variables():
            if var in bindings:
--
                expr = expr.replace(var, val)
        return expr.simplify()

    def typecheck(self, signature=None):
        """
        Infer and check types.  Raise exceptions if necessary.

--
            representations of types)
        :return: the signature, plus any additional type mappings
        """
        sig = defaultdict(list)
        if signature:
            for key in signature:
                val = signature[key]
--

        return dict((key, sig[key][0].type) for key in sig)

    def findtype(self, variable):
        """
        Find the type of the given variable as it is used in this expression.
        For example, finding the type of "P" in "P(x) & Q(x,y)" yields "<e,t>"
--
        """
        raise NotImplementedError()

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """
        Set the type of this expression to be the given type.  Raise type
        exceptions where applicable.
--
        """
        raise NotImplementedError()

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """
        Replace every instance of 'variable' with 'expression'
        :param variable: ``Variable`` The variable to replace
--
                                                         replace_bound, alpha_convert),
                                     self.__class__)

    def normalize(self):
        """Rename auto-generated unique variables"""
        def get_indiv_vars(e):
            if isinstance(e, IndividualVariableExpression):
                return set([e])
            elif isinstance(e, AbstractVariableExpression):
--
            result = result.replace(e.variable, newVar, True)
        return result

    def visit(self, function, combinator):
        """
        Recursively visit subexpressions.  Apply 'function' to each
        subexpression and pass the result of each function application
--
        """
        raise NotImplementedError()

    def visit_structured(self, function, combinator):
        """
        Recursively visit subexpressions.  Apply 'function' to each
        subexpression and pass the result of each function application
--
        """
        return self.visit(function, lambda parts: combinator(*parts))

    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__, self)

    def __str__(self):
        return self.str()

    def variables(self):
        """
        Return a set of all the variables for binding substitution.
        The variables returned include all free (non-bound) individual
--
        return self.free() | set(p for p in self.predicates()|self.constants()
                                 if re.match('^[?@]', p.name))

    def free(self):
        """
        Return a set of all the free (non-bound) variables.  This includes
        both individual and predicate variables, but not constants.
--
        return self.visit(lambda e: e.free(),
                          lambda parts: reduce(operator.or_, parts, set()))

    def constants(self):
        """
        Return a set of individual constants (non-predicates).
        :return: set of ``Variable`` objects
--
        return self.visit(lambda e: e.constants(),
                          lambda parts: reduce(operator.or_, parts, set()))

    def predicates(self):
        """
        Return a set of predicates (constants, not variables).
        :return: set of ``Variable`` objects
--
        return self.visit(lambda e: e.predicates(),
                          lambda parts: reduce(operator.or_, parts, set()))

    def simplify(self):
        """
        :return: beta-converted version of this expression
        """
        return self.visit_structured(lambda e: e.simplify(), self.__class__)

    def make_VariableExpression(self, variable):
        return VariableExpression(variable)


--
    ``AbstractVariableExpression``).  This means that the example from above
    will be returned as "(\x y.see(x,y)(john))(mary)".
    """
    def __init__(self, function, argument):
        """
        :param function: ``Expression``, for the function expression
        :param argument: ``Expression``, for the argument
--
        self.function = function
        self.argument = argument

    def simplify(self):
        function = self.function.simplify()
        argument = self.argument.simplify()
        if isinstance(function, LambdaExpression):
--
            return self.__class__(function, argument)

    @property
    def type(self):
        if isinstance(self.function.type, ComplexType):
            return self.function.type.second
        else:
            return ANY_TYPE

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        self.argument._set_type(ANY_TYPE, signature)
        try:
--
                    % (self.function, self.function.type, self.argument,
                       self.argument.type, self.function.type.first))

    def findtype(self, variable):
        """:see Expression.findtype()"""
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        if self.is_atom():
--
        else:
            return ANY_TYPE

    def constants(self):
        """:see: Expression.constants()"""
        if isinstance(self.function, AbstractVariableExpression):
            function_constants = set()
--
            function_constants = self.function.constants()
        return function_constants | self.argument.constants()

    def predicates(self):
        """:see: Expression.predicates()"""
        if isinstance(self.function, ConstantExpression):
            function_preds = set([self.function.variable])
--
            function_preds = self.function.predicates()
        return function_preds | self.argument.predicates()

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        return combinator([function(self.function), function(self.argument)])

    def __eq__(self, other):
        return isinstance(other, ApplicationExpression) and \
                self.function == other.function and \
                self.argument == other.argument

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__

    def __str__(self):
        # uncurry the arguments and find the base function
        if self.is_atom():
            function, args = self.uncurry()
--

        return function_str + Tokens.OPEN + arg_str + Tokens.CLOSE

    def uncurry(self):
        """
        Uncurry this application expression

--
        return (function, args)

    @property
    def pred(self):
        """
        Return uncurried base-function.
        If this is an atom, then the result will be a variable expression.
--
        return self.uncurry()[0]

    @property
    def args(self):
        """
        Return uncurried arg-list
        """
        return self.uncurry()[1]

    def is_atom(self):
        """
        Is this expression an atom (as opposed to a lambda expression applied
        to a term)?
--
@python_2_unicode_compatible
class AbstractVariableExpression(Expression):
    """This class represents a variable to be used as a predicate or entity"""
    def __init__(self, variable):
        """
        :param variable: ``Variable``, for the variable
        """
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        self.variable = variable

    def simplify(self):
        return self

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """:see: Expression.replace()"""
        assert isinstance(variable, Variable), "%s is not an Variable" % variable
        assert isinstance(expression, Expression), "%s is not an Expression" % expression
--
        else:
            return self

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        resolution = other_type
        for varEx in signature[self.variable.name]:
--
        for varEx in signature[self.variable.name]:
            varEx.type = resolution

    def findtype(self, variable):
        """:see Expression.findtype()"""
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        if self.variable == variable:
--
        else:
            return ANY_TYPE

    def predicates(self):
        """:see: Expression.predicates()"""
        return set()

    def __eq__(self, other):
        """Allow equality between instances of ``AbstractVariableExpression``
        subtypes."""
        return isinstance(other, AbstractVariableExpression) and \
               self.variable == other.variable

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, AbstractVariableExpression):
            raise TypeError
        return self.variable < other.variable

    __hash__ = Expression.__hash__

    def __str__(self):
        return "%s" % self.variable

class IndividualVariableExpression(AbstractVariableExpression):
    """This class represents variables that take the form of a single lowercase
    character (other than 'e') followed by zero or more digits."""
    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if not other_type.matches(ENTITY_TYPE):
            raise IllegalTypeException(self, other_type, ENTITY_TYPE)

        signature[self.variable.name].append(self)

    def _get_type(self): return ENTITY_TYPE
    type = property(_get_type, _set_type)

    def free(self):
        """:see: Expression.free()"""
        return set([self.variable])

    def constants(self):
        """:see: Expression.constants()"""
        return set()

--
    character followed by zero or more digits."""
    type = ANY_TYPE

    def free(self):
        """:see: Expression.free()"""
        return set([self.variable])

    def constants(self):
        """:see: Expression.constants()"""
        return set()

--
    character followed by zero or more digits."""
    type = ENTITY_TYPE

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if other_type == ANY_TYPE:
            #entity type by default, for individuals
            resolution = ENTITY_TYPE
        else:
            resolution = other_type
--
        for varEx in signature[self.variable.name]:
            varEx.type = resolution

    def free(self):
        """:see: Expression.free()"""
        return set()

    def constants(self):
        """:see: Expression.constants()"""
        return set([self.variable])


def VariableExpression(variable):
    """
    This is a factory method that instantiates and returns a subtype of
    ``AbstractVariableExpression`` appropriate for the given variable.
--
class VariableBinderExpression(Expression):
    """This an abstract class for any Expression that binds a variable in an
    Expression.  This includes LambdaExpressions and Quantified Expressions"""
    def __init__(self, variable, term):
        """
        :param variable: ``Variable``, for the variable
        :param term: ``Expression``, for the term
--
        self.variable = variable
        self.term = term

    def replace(self, variable, expression, replace_bound=False, alpha_convert=True):
        """:see: Expression.replace()"""
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        assert isinstance(expression, Expression), "%s is not an Expression" % expression
--
            return self.__class__(self.variable,
                                  self.term.replace(variable, expression, replace_bound, alpha_convert))

    def alpha_convert(self, newvar):
        """Rename all occurrences of the variable introduced by this variable
        binder in the expression to ``newvar``.
        :param newvar: ``Variable``, for the new variable
--
                                                VariableExpression(newvar),
                                                True))

    def free(self):
        """:see: Expression.free()"""
        return self.term.free() - set([self.variable])

    def findtype(self, variable):
        """:see Expression.findtype()"""
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        if variable == self.variable:
--
        else:
            return self.term.findtype(variable)

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        return combinator([function(self.term)])

    def visit_structured(self, function, combinator):
        """:see: Expression.visit_structured()"""
        return combinator(self.variable, function(self.term))

    def __eq__(self, other):
        r"""Defines equality modulo alphabetic variance.  If we are comparing
        \x.M  and \y.N, then check equality of M and N[x/y]."""
        if isinstance(self, other.__class__) or \
           isinstance(other, self.__class__):
--
        else:
            return False

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__
--
@python_2_unicode_compatible
class LambdaExpression(VariableBinderExpression):
    @property
    def type(self):
        return ComplexType(self.term.findtype(self.variable),
                           self.term.type)

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        self.term._set_type(other_type.second, signature)
        if not self.type.resolve(other_type):
            raise TypeResolutionException(self, other_type)

    def __str__(self):
        variables = [self.variable]
        term = self.term
        while term.__class__ == self.__class__:
--
@python_2_unicode_compatible
class QuantifiedExpression(VariableBinderExpression):
    @property
    def type(self): return TRUTH_TYPE

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if not other_type.matches(TRUTH_TYPE):
            raise IllegalTypeException(self, other_type, TRUTH_TYPE)
        self.term._set_type(TRUTH_TYPE, signature)

    def __str__(self):
        variables = [self.variable]
        term = self.term
        while term.__class__ == self.__class__:
--
               Tokens.DOT + "%s" % term

class ExistsExpression(QuantifiedExpression):
    def getQuantifier(self):
        return Tokens.EXISTS

class AllExpression(QuantifiedExpression):
    def getQuantifier(self):
        return Tokens.ALL


@python_2_unicode_compatible
class NegatedExpression(Expression):
    def __init__(self, term):
        assert isinstance(term, Expression), "%s is not an Expression" % term
        self.term = term

    @property
    def type(self): return TRUTH_TYPE

    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if not other_type.matches(TRUTH_TYPE):
            raise IllegalTypeException(self, other_type, TRUTH_TYPE)
        self.term._set_type(TRUTH_TYPE, signature)

    def findtype(self, variable):
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        return self.term.findtype(variable)

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        return combinator([function(self.term)])

    def negate(self):
        """:see: Expression.negate()"""
        return self.term

    def __eq__(self, other):
        return isinstance(other, NegatedExpression) and self.term == other.term

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__

    def __str__(self):
        return Tokens.NOT + "%s" % self.term


@python_2_unicode_compatible
class BinaryExpression(Expression):
    def __init__(self, first, second):
        assert isinstance(first, Expression), "%s is not an Expression" % first
        assert isinstance(second, Expression), "%s is not an Expression" % second
        self.first = first
        self.second = second

    @property
    def type(self): return TRUTH_TYPE

    def findtype(self, variable):
        """:see Expression.findtype()"""
        assert isinstance(variable, Variable), "%s is not a Variable" % variable
        f = self.first.findtype(variable)
--
        else:
            return ANY_TYPE

    def visit(self, function, combinator):
        """:see: Expression.visit()"""
        return combinator([function(self.first), function(self.second)])

    def __eq__(self, other):
        return (isinstance(self, other.__class__) or \
                isinstance(other, self.__class__)) and \
               self.first == other.first and self.second == other.second

    def __ne__(self, other):
        return not self == other

    __hash__ = Expression.__hash__

    def __str__(self):
        first = self._str_subex(self.first)
        second = self._str_subex(self.second)
        return Tokens.OPEN + first + ' ' + self.getOp() \
                + ' ' + second + Tokens.CLOSE

    def _str_subex(self, subex):
        return "%s" % subex


class BooleanExpression(BinaryExpression):
    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if not other_type.matches(TRUTH_TYPE):
            raise IllegalTypeException(self, other_type, TRUTH_TYPE)
--

class AndExpression(BooleanExpression):
    """This class represents conjunctions"""
    def getOp(self):
        return Tokens.AND

    def _str_subex(self, subex):
        s = "%s" % subex
        if isinstance(subex, AndExpression):
            return s[1:-1]
--

class OrExpression(BooleanExpression):
    """This class represents disjunctions"""
    def getOp(self):
        return Tokens.OR

    def _str_subex(self, subex):
        s = "%s" % subex
        if isinstance(subex, OrExpression):
            return s[1:-1]
--

class ImpExpression(BooleanExpression):
    """This class represents implications"""
    def getOp(self):
        return Tokens.IMP

class IffExpression(BooleanExpression):
    """This class represents biconditionals"""
    def getOp(self):
        return Tokens.IFF


class EqualityExpression(BinaryExpression):
    """This class represents equality expressions like "(x = y)"."""
    def _set_type(self, other_type=ANY_TYPE, signature=None):
        """:see Expression._set_type()"""
        assert isinstance(other_type, Type)

        if signature is None:
            signature = defaultdict(list)

        if not other_type.matches(TRUTH_TYPE):
            raise IllegalTypeException(self, other_type, TRUTH_TYPE)
        self.first._set_type(ENTITY_TYPE, signature)
        self.second._set_type(ENTITY_TYPE, signature)

    def getOp(self):
        return Tokens.EQ


--
class LogicParser(object):
    """A lambda calculus expression parser."""

    def __init__(self, type_check=False):
        """
        :param type_check: bool should type checking be performed?
        to their types.
--
                           [(None,10)])
        self.right_associated_operations = [APP]

    def parse(self, data, signature=None):
        """
        Parse the expression.

--

        return result

    def process(self, data):
        """Split the data into tokens"""
        out = []
        mapping = {}
--
        mapping[len(out)+1] = len(data)+1
        return out, mapping

    def process_quoted_token(self, data_idx, data):
        token = ''
        c = data[data_idx]
        i = data_idx
--
                break
        return token, i

    def get_all_symbols(self):
        """This method exists to be overridden"""
        return Tokens.SYMBOLS

    def inRange(self, location):
        """Return TRUE if the given location is within the buffer"""
        return self._currentIndex+location < len(self._buffer)

    def token(self, location=None):
        """Get the next waiting token.  If a location is given, then
        return the token at currentIndex+location without advancing
        currentIndex; setting it gives lookahead/lookback capability."""
--
        except IndexError:
            raise ExpectedMoreTokensException(self._currentIndex+1)

    def isvariable(self, tok):
        return tok not in Tokens.TOKENS

    def parse_Expression(self, context):
        """Parse the next complete expression from the stream and return it."""
        try:
            tok = self.token()
--

        return self.attempt_adjuncts(accum, context)

    def handle(self, tok, context):
        """This method is intended to be overridden for logics that
        use different operators or expressions"""
        if self.isvariable(tok):
--
        elif tok == Tokens.OPEN:
            return self.handle_open(tok, context)

    def attempt_adjuncts(self, expression, context):
        cur_idx = None
        while cur_idx != self._currentIndex: #while adjuncts are added
            cur_idx = self._currentIndex
--
            expression = self.attempt_BooleanExpression(expression, context)
        return expression

    def handle_negation(self, tok, context):
        return self.make_NegatedExpression(self.parse_Expression(Tokens.NOT))

    def make_NegatedExpression(self, expression):
        return NegatedExpression(expression)

    def handle_variable(self, tok, context):
        #It's either: 1) a predicate expression: sees(x,y)
        #             2) an application expression: P(x)
        #             3) a solo variable: john OR x
--
            self.assertNextToken(Tokens.CLOSE)
        return accum

    def get_next_token_variable(self, description):
        try:
            tok = self.token()
        except ExpectedMoreTokensException as e:
--
                                 "Constants may not be %s." % (tok, description))
        return Variable(tok)

    def handle_lambda(self, tok, context):
        # Expression is a lambda expression
        if not self.inRange(0):
            raise ExpectedMoreTokensException(self._currentIndex+2,
--
            accum = self.make_LambdaExpression(vars.pop(), accum)
        return accum

    def handle_quant(self, tok, context):
        # Expression is a quantified expression: some x.M
        factory = self.get_QuantifiedExpression_factory(tok)

--
            accum = self.make_QuanifiedExpression(factory, vars.pop(), accum)
        return accum

    def get_QuantifiedExpression_factory(self, tok):
        """This method serves as a hook for other logic parsers that
        have different quantifiers"""
        if tok in Tokens.EXISTS_LIST:
--
        else:
            self.assertToken(tok, Tokens.QUANTS)

    def make_QuanifiedExpression(self, factory, variable, term):
        return factory(variable, term)

    def handle_open(self, tok, context):
        #Expression is in parens
        accum = self.parse_Expression(None)
        self.assertNextToken(Tokens.CLOSE)
        return accum

    def attempt_EqualityExpression(self, expression, context):
        """Attempt to make an equality expression.  If the next token is an
        equality operator, then an EqualityExpression will be returned.
        Otherwise, the parameter will be returned."""
--
                    expression = self.make_NegatedExpression(expression)
        return expression

    def make_EqualityExpression(self, first, second):
        """This method serves as a hook for other logic parsers that
        have different equality expression classes"""
        return EqualityExpression(first, second)

    def attempt_BooleanExpression(self, expression, context):
        """Attempt to make a boolean expression.  If the next token is a boolean
        operator, then a BooleanExpression will be returned.  Otherwise, the
        parameter will be returned."""
--
                break
        return expression

    def get_BooleanExpression_factory(self, tok):
        """This method serves as a hook for other logic parsers that
        have different boolean operators"""
        if tok in Tokens.AND_LIST:
--
        else:
            return None

    def make_BooleanExpression(self, factory, first, second):
        return factory(first, second)

    def attempt_ApplicationExpression(self, expression, context):
        """Attempt to make an application expression.  The next tokens are
        a list of arguments in parens, then the argument expression is a
        function being applied to the arguments.  Otherwise, return the
--
                return accum
        return expression

    def make_ApplicationExpression(self, function, argument):
        return ApplicationExpression(function, argument)

    def make_VariableExpression(self, name):
        return VariableExpression(Variable(name))

    def make_LambdaExpression(self, variable, term):
        return LambdaExpression(variable, term)

    def has_priority(self, operation, context):
        return self.operator_precedence[operation] < self.operator_precedence[context] or \
               (operation in self.right_associated_operations and \
                self.operator_precedence[operation] == self.operator_precedence[context])

    def assertNextToken(self, expected):
        try:
            tok = self.token()
        except ExpectedMoreTokensException as e:
--
            if tok != expected:
                raise UnexpectedTokenException(self._currentIndex, tok, expected)

    def assertToken(self, tok, expected):
        if isinstance(expected, list):
            if tok not in expected:
                raise UnexpectedTokenException(self._currentIndex, tok, expected)
--
            if tok != expected:
                raise UnexpectedTokenException(self._currentIndex, tok, expected)

    def __repr__(self):
        if self.inRange(0):
            msg = 'Next token: ' + self.token(0)
        else:
--
        return '<' + self.__class__.__name__ + ': ' + msg + '>'


def parse_logic(s, logic_parser=None, encoding=None):
    """
    Convert a file of First Order Formulas into a list of {Expression}s.

--
    return statements


class StringTrie(defaultdict):
    LEAF = "<leaf>"

    def __init__(self, strings=None):
        defaultdict.__init__(self, StringTrie)
        if strings:
            for string in strings:
                self.insert(string)

    def insert(self, string):
        if len(string):
            self[string[0]].insert(string[1:])
        else:
--


class ParseException(Exception):
    def __init__(self, index, message):
        self.index = index
        Exception.__init__(self, message)

class UnexpectedTokenException(ParseException):
    def __init__(self, index, unexpected=None, expected=None, message=None):
        if unexpected and expected:
            msg = "Unexpected token: '%s'.  " \
                  "Expected token '%s'." % (unexpected, expected)
--
        ParseException.__init__(self, index, msg)

class ExpectedMoreTokensException(ParseException):
    def __init__(self, index, message=None):
        if not message:
            message = 'More tokens expected.'
        ParseException.__init__(self, index, 'End of input found.  ' + message)


def is_indvar(expr):
    """
    An individual variable must be a single lowercase character other than 'e',
    followed by zero or more digits.
--
    assert isinstance(expr, string_types), "%s is not a string" % expr
    return re.match(r'^[a-df-z]\d*$', expr) is not None

def is_funcvar(expr):
    """
    A function variable must be a single uppercase character followed by
    zero or more digits.
--
    assert isinstance(expr, string_types), "%s is not a string" % expr
    return re.match(r'^[A-Z]\d*$', expr) is not None

def is_eventvar(expr):
    """
    An event variable must be a single lowercase 'e' character followed by
    zero or more digits.
--
    return re.match(r'^e\d*$', expr) is not None


def demo():
    p = LogicParser().parse
    print('='*20 + 'Test parser' + '='*20)
    print(p(r'john'))
--
    print(e2)
    print(e1 == e2)

def demo_errors():
    print('='*20 + 'Test parser errors' + '='*20)
    demoException('(P(x) & Q(x)')
    demoException('((P(x) &) & Q(x))')
--
    demoException('(P(x)Q(x)')
    demoException('exists x -> y')

def demoException(s):
    try:
        LogicParser().parse(s)
    except ParseException as e:
        print("%s: %s" % (e.__class__.__name__, e))

def printtype(ex):
    print("%s : %s" % (ex.str(), ex.type))

if __name__ == '__main__':
--

# todo: get a more general solution to canonicalized symbols for clauses -- maybe use xmlcharrefs?

from collections import defaultdict
import re
from nltk.compat import htmlentitydefs

# Dictionary that associates corpora with NE classes
NE_CLASSES = {
--
long2short = dict(LOCATION ='LOC', ORGANIZATION = 'ORG', PERSON = 'PER')


def _expand(type):
    """
    Expand an NE class name.
    :type type: str
--
    except KeyError:
        return type

def class_abbrev(type):
    """
    Abbreviate an NE class name.
    :type type: str
--
        return type


def _join(lst, sep=' ', untag=False):
    """
    Join a list into a string, turning tags tuples into tag strings or just words.
    :param untag: if ``True``, omit the tag from tagged input strings.
--
        from nltk.tag import tuple2str
        return sep.join(tuple2str(tup) for tup in lst)

def descape_entity(m, defs=htmlentitydefs.entitydefs):
    """
    Translate one entity to its ISO Latin value.
    Inspired by example from effbot.org
--
    #s = pattern.sub(descape_entity, s)
    #print s, new
    try:
        return defs[m.group(1)]

    except KeyError:
        return m.group(0) # use as is

def list2sym(lst):
    """
    Convert a list of strings into a canonical symbol.
    :type lst: list
--
    sym = sym.replace('.', '')
    return sym

def mk_pairs(tree):
    """
    Group a chunk structure into a list of pairs of the form (list(str), ``Tree``)

--
    return pairs


def mk_reldicts(pairs, window=5, trace=0):
    """
    Converts the pairs generated by ``mk_pairs`` into a 'reldict': a dictionary which
    stores information about the subject and object NEs plus the filler between them.
--
    :param window: a threshold for the number of items to include in the left and right context
    :type window: int
    :return: 'relation' dictionaries whose keys are 'lcon', 'subjclass', 'subjtext', 'subjsym', 'filler', objclass', objtext', 'objsym' and 'rcon'
    :rtype: list(defaultdict)
    """
    result = []
    while len(pairs) > 2:
        reldict = defaultdict(str)
        reldict['lcon'] = _join(pairs[0][0][-window:])
        reldict['subjclass'] = pairs[0][1].node
        reldict['subjtext'] = _join(pairs[0][1].leaves())
--
        pairs = pairs[1:]
    return result

def extract_rels(subjclass, objclass, doc, corpus='ace', pattern=None, window=10):
    """
    Filter the output of ``mk_reldicts`` according to specified NE classes and a filler pattern.

--
    :param window: filters out fillers which exceed this threshold
    :type window: int
    :return: see ``mk_reldicts``
    :rtype: list(defaultdict)
    """

    if subjclass and subjclass not in NE_CLASSES[corpus]:
--
    return list(filter(relfilter, reldicts))


def show_raw_rtuple(reldict, lcon=False, rcon=False):
    """
    Pretty print the reldict as an rtuple.
    :param reldict: a relation dictionary
    :type reldict: defaultdict
    """
    items = [class_abbrev(reldict['subjclass']), reldict['subjtext'], reldict['filler'], class_abbrev(reldict['objclass']), reldict['objtext']]
    format = '[%s: %r] %r [%s: %r]'
--
    printargs = tuple(items)
    return format % printargs

def show_clause(reldict, relsym):
    """
    Print the relation in clausal form.
    :param reldict: a relation dictionary
    :type reldict: defaultdict
    :param relsym: a label for the relation
    :type relsym: str
    """
--
############################################
# Example of in(ORG, LOC)
############################################
def in_demo(trace=0, sql=True):
    """
    Select pairs of organizations and locations whose mentions occur with an
    intervening occurrence of the preposition "in".
--
# Example of has_role(PER, LOC)
############################################

def roles_demo(trace=0):
    from nltk.corpus import ieer
    roles = """
    (.*(                   # assorted roles
--
##############################################


def ieer_headlines():

    from nltk.corpus import ieer
    from nltk.tree import Tree
--
## Dutch CONLL2002: take_on_role(PER, ORG
#############################################

def conllned(trace=1):
    """
    Find the copula+'van' relation ('of') in the Dutch tagged training corpus
    from CoNLL 2002.
--
## Spanish CONLL2002: (PER, ORG)
#############################################

def conllesp():
    from nltk.corpus import conll2002

    de = """
--
    print()


def ne_chunked():
    IN = re.compile(r'.*\bin\b(?!\b.+ing)')
    rels = []
    for sent in nltk.corpus.treebank.tagged_sents()[:100]:
--

class Error(Exception): pass

class Undefined(Error):  pass

def trace(f, *args, **kw):
    argspec = inspect.getargspec(f)
    d = dict(zip(argspec[0], args))
    if d.pop('trace', None):
--
            print("%s => %s" % item)
    return f(*args, **kw)

def is_rel(s):
    """
    Check whether a set represents a relation (of any arity).

--
    else:
        raise ValueError("Set %r contains sequences of different lengths" % s)

def set2rel(s):
    """
    Convert a set containing individuals (strings or numbers) into a set of
    unary tuples. Any tuples of strings already in the set are passed through
--
            new.add(elem)
    return new

def arity(rel):
    """
    Check the arity of a relation.
    :type rel: set of tuples
--
    just behave like a standard  dictionary) if indexed with an expression that
    is not in its list of symbols.
    """
    def __init__(self, iter):
        """
        :param iter: a list of (symbol, value) pairs.
        """
--

                raise ValueError(msg)

    def __getitem__(self, key):
        if key in self:
            return dict.__getitem__(self, key)
        else:
            raise Undefined("Unknown expression: '%s'" % key)

    def __str__(self):
        return pformat(self)

    @property
    def domain(self):
        """Set-theoretic domain of the value-space of a Valuation."""
        dom = []
        for val in self.values():
--
        return set(dom)

    @property
    def symbols(self):
        """The non-logical constants which the Valuation recognizes."""
        return sorted(self.keys())

--
    *g*. *g* only assigns values to individual variables (i.e.,
    members of the class ``IndividualVariableExpression`` in the ``logic``
    module. If a variable is not assigned a value by *g*, it will raise
    an ``Undefined`` exception.

    A variable *Assignment* is a mapping from individual variables to
    entities in the domain. Individual variables are usually indicated
--
    :type assign: list
    """

    def __init__(self, domain, assign=None):
        dict.__init__(self)
        self.domain = domain
        if assign:
--
                self[var] = val
        self._addvariant()

    def __getitem__(self, key):
        if key in self:
            return dict.__getitem__(self, key)
        else:
            raise Undefined("Not recognized as a variable: '%s'" % key)

    def copy(self):
        new = Assignment(self.domain)
        new.update(self)
        return new

    def purge(self, var=None):
        """
        Remove one or all keys (i.e. logic variables) from an
        assignment, and update ``self.variant``.
--
        self._addvariant()
        return None

    def __str__(self):
        """
        Pretty printing for assignments. {'x', 'u'} appears as 'g[u/x]'
        """
--
            gstring += "[%s/%s]" % (val, var)
        return gstring

    def _addvariant(self):
        """
        Create a more pretty-printable version of the assignment.
        """
--
        self.variant = list
        return None

    def add(self, var, val):
        """
        Add a new variable-value pair to the assignment, and update
        ``self.variant``.
--
    model and don't require the domain of *V* to be subset of *D*.
    """

    def __init__(self, domain, valuation):
        assert isinstance(domain, set)
        self.domain = domain
        self.valuation = valuation
--
            raise Error("The valuation domain, %s, must be a subset of the model's domain, %s"\
                  % (valuation.domain, domain))

    def __repr__(self):
        return "(%r, %r)" % (self.domain, self.valuation)

    def __str__(self):
        return "Domain = %s,\nValuation = \n%s" % (self.domain, self.valuation)

    def evaluate(self, expr, g, trace=None):
        """
        Call the ``LogicParser`` to parse input expressions, and
        provide a handler for ``satisfy``
        that blocks further propagation of the ``Undefined`` error.
        :param expr: An ``Expression`` of ``logic``.
        :type g: Assignment
        :param g: an assignment to individual variables.
        :rtype: bool or 'Undefined'
        """
        try:
            lp = LogicParser()
--
                print()
                print("'%s' evaluates to %s under M, %s" %  (expr, value, g))
            return value
        except Undefined:
            if trace:
                print()
                print("'%s' is undefined under M, %s" %  (expr, g))
            return 'Undefined'


    def satisfy(self, parsed, g, trace=None):
        """
        Recursive interpretation function for a formula of first-order logic.

        Raises an ``Undefined`` error when ``parsed`` is an atomic string
        but is not a symbol or an individual variable.

        :return: Returns a truth value or ``Undefined`` if ``parsed`` is\
        complex, and calls the interpretation function ``i`` if ``parsed``\
        is atomic.

--
            return self.i(parsed, g, trace)

    #@decorator(trace_eval)
    def i(self, parsed, g, trace=False):
        """
        An interpretation function.

--

        - if ``parsed`` is a non-logical constant, calls the valuation *V*
        - else if ``parsed`` is an individual variable, calls assignment *g*
        - else returns ``Undefined``.

        :param parsed: an ``Expression`` of ``logic``.
        :type g: Assignment
--
            return g[parsed.variable.name]

        else:
            raise Undefined("Can't find a value for %s" % parsed)

    def satisfiers(self, parsed, varex, g, trace=None, nesting=0):
        """
        Generate the entities from the model's domain that satisfy an open formula.

--
            result = set(c for c in candidates)
        # var isn't free in parsed
        else:
            raise Undefined("%s is not free in %s" % (var.name, parsed))

        return result

--

# Demo 1: Propositional Logic
#################
def propdemo(trace=None):
    """Example of a propositional model."""

    global val1, dom1, m1, g1
--
# Demo 2: FOL Model
#############

def folmodel(quiet=False, trace=None):
    """Example of a first-order model."""

    global val2, v2, dom2, m2, g2
--
        for parsed in parsed_exprs:
            try:
                print("The interpretation of '%s' in m2 is %s" % (parsed, m2.i(parsed, g2)))
            except Undefined:
                print("The interpretation of '%s' in m2 is Undefined" % parsed)


        applications = [('boy', ('adam')), ('walks', ('adam',)), ('love', ('adam', 'y')), ('love', ('y', 'adam'))]
--
                funval = m2.i(lp.parse(fun), g2)
                argsval = tuple(m2.i(lp.parse(arg), g2) for arg in args)
                print("%s(%s) evaluates to %s" % (fun, args, argsval in funval))
            except Undefined:
                print("%s(%s) evaluates to Undefined" % (fun, args))

# Demo 3: FOL
#########

def foldemo(trace=None):
    """
    Interpretation of closed expressions in a first-order model.
    """
--
# Demo 3: Satisfaction
#############

def satdemo(trace=None):
    """Satisfiers of an open formula in a first order model."""

    print()
--
        print("The satisfiers of '%s' are: %s" % (p, m2.satisfiers(p, 'x', g2, trace)))


def demo(num=0, trace=None):
    """
    Run exists demos.

--
    """
    A container for handling quantifier ambiguity via Cooper storage.
    """
    def __init__(self, featstruct):
        """
        :param featstruct: The value of the ``sem`` node in a tree from
            ``parse_with_bindops()``
--
        except KeyError:
            print("%s is not a Cooper storage structure" % featstruct)

    def _permute(self, lst):
        """
        :return: An iterator over the permutations of the input list
        :type lst: list
--
                    yield (x,)+y
        else: yield ()

    def s_retrieve(self, trace=False):
        """
        Carry out S-Retrieval of binding operators in store. If hack=True,
        serialize the bindop and core as strings and reparse. Ugh.
--
            self.readings.append(term)


def parse_with_bindops(sentence, grammar=None, trace=0):
    """
    Use a grammar with Binding Operators to parse a sentence.
    """
--
    return parser.nbest_parse(tokens)


def demo():
    from nltk.sem import cooper_storage as cs
    sentence = "every girl chases a dog"
    #sentence = "a man gives a bone to every dog"
--

@python_2_unicode_compatible
class Expression(object):
    def applyto(self, other, other_indices=None):
        return ApplicationExpression(self, other, other_indices)

    def __call__(self, other):
        return self.applyto(other)

    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__, self)


@python_2_unicode_compatible
class AtomicExpression(Expression):
    def __init__(self, name, dependencies=None):
        """
        :param name: str for the constant name
        :param dependencies: list of int for the indices on which this atom is dependent
--
            dependencies = []
        self.dependencies = dependencies

    def simplify(self, bindings=None):
        """
        If 'self' is bound by 'bindings', return the atomic to which it is bound.
        Otherwise, return self.
--
        else:
            return self

    def compile_pos(self, index_counter, glueFormulaFactory):
        """
        From Iddo Lev's PhD Dissertation p108-109

--
        self.dependencies = []
        return (self, [])

    def compile_neg(self, index_counter, glueFormulaFactory):
        """
        From Iddo Lev's PhD Dissertation p108-109

--
        self.dependencies = []
        return (self, [])

    def initialize_labels(self, fstruct):
        self.name = fstruct.initialize_label(self.name.lower())

    def __eq__(self, other):
        return self.__class__ == other.__class__ and self.name == other.name

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        accum = self.name
        if self.dependencies:
            accum += "%s" % self.dependencies
        return accum

    def __hash__(self):
        return hash(self.name)

class ConstantExpression(AtomicExpression):
    def unify(self, other, bindings):
        """
        If 'other' is a constant, then it must be equal to 'self'.  If 'other' is a variable,
        then it must not be bound to anything other than 'self'.
--
        raise UnificationException(self, other, bindings)

class VariableExpression(AtomicExpression):
    def unify(self, other, bindings):
        """
        'self' must not be bound to anything other than 'other'.

--

@python_2_unicode_compatible
class ImpExpression(Expression):
    def __init__(self, antecedent, consequent):
        """
        :param antecedent: ``Expression`` for the antecedent
        :param consequent: ``Expression`` for the consequent
--
        self.antecedent = antecedent
        self.consequent = consequent

    def simplify(self, bindings=None):
        return self.__class__(self.antecedent.simplify(bindings), self.consequent.simplify(bindings))

    def unify(self, other, bindings):
        """
        Both the antecedent and consequent of 'self' and 'other' must unify.

--
        except VariableBindingException:
            raise UnificationException(self, other, bindings)

    def compile_pos(self, index_counter, glueFormulaFactory):
        """
        From Iddo Lev's PhD Dissertation p108-109

--
        (c, c_new) = self.consequent.compile_pos(index_counter, glueFormulaFactory)
        return (ImpExpression(a,c), a_new + c_new)

    def compile_neg(self, index_counter, glueFormulaFactory):
        """
        From Iddo Lev's PhD Dissertation p108-109

--
        new_v = glueFormulaFactory('v%s' % fresh_index, a, set([fresh_index]))
        return (c, a_new + c_new + [new_v])

    def initialize_labels(self, fstruct):
        self.antecedent.initialize_labels(fstruct)
        self.consequent.initialize_labels(fstruct)

    def __eq__(self, other):
        return self.__class__ == other.__class__ and \
                self.antecedent == other.antecedent and self.consequent == other.consequent

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        return "%s%s %s %s%s" % (
            Tokens.OPEN, self.antecedent, Tokens.IMP, self.consequent, Tokens.CLOSE)

    def __hash__(self):
        return hash('%s%s%s' % (hash(self.antecedent), Tokens.IMP, hash(self.consequent)))

@python_2_unicode_compatible
class ApplicationExpression(Expression):
    def __init__(self, function, argument, argument_indices=None):
        """
        :param function: ``Expression`` for the function
        :param argument: ``Expression`` for the argument
--
        self.argument = argument
        self.bindings = bindings

    def simplify(self, bindings=None):
        """
        Since function is an implication, return its consequent.  There should be
        no need to check that the application is valid since the checking is done
--

        return self.function.simplify(bindings).consequent

    def __eq__(self, other):
        return self.__class__ == other.__class__ and \
                self.function == other.function and self.argument == other.argument

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        return "%s" % self.function + Tokens.OPEN + "%s" % self.argument + Tokens.CLOSE

    def __hash__(self):
        return hash('%s%s%s' % (hash(self.antecedent), Tokens.OPEN, hash(self.consequent)))

@python_2_unicode_compatible
class BindingDict(object):
    def __init__(self, bindings=None):
        """
        :param bindings:
            list [(``VariableExpression``, ``AtomicExpression``)] to initialize the dictionary
--
            for (v, b) in bindings:
                self[v] = b

    def __setitem__(self, variable, binding):
        """
        A binding is consistent with the dict if its variable is not already bound, OR if its
        variable is already bound to its argument.
--
        else:
            raise VariableBindingException('Variable %s already bound to another value' % (variable))

    def __getitem__(self, variable):
        """
        Return the expression to which 'variable' is bound
        """
--
            except KeyError:
                return intermediate

    def __contains__(self, item):
        return item in self.d

    def __add__(self, other):
        """
        :param other: ``BindingDict`` The dict with which to combine self
        :return: ``BindingDict`` A new dict containing all the elements of both parameters
--
            raise VariableBindingException('Attempting to add two contradicting'\
                        ' VariableBindingsLists: %s, %s' % (self, other))

    def __ne__(self, other):
        return not self == other

    def __eq__(self, other):
        if not isinstance(other, BindingDict):
            raise TypeError
        return self.d == other.d

    def __str__(self):
        return '{' + ', '.join('%s: %s' % (v, self.d[v]) for v in self.d) + '}'

    def __repr__(self):
        return 'BindingDict: %s' % self

class VariableBindingException(Exception):
    pass

class UnificationException(Exception):
    def __init__(self, a, b, bindings):
        Exception.__init__(self, 'Cannot unify %s with %s given %s' % (a, b, bindings))

class LinearLogicApplicationException(Exception):
--

class LinearLogicParser(LogicParser):
    """A linear logic expression parser."""
    def __init__(self):
        LogicParser.__init__(self)

        self.operator_precedence = {APP: 1, Tokens.IMP: 2, None: 3}
        self.right_associated_operations += [Tokens.IMP]

    def get_all_symbols(self):
        return Tokens.TOKENS

    def handle(self, tok, context):
        if tok not in Tokens.TOKENS:
            return self.handle_variable(tok, context)
        elif tok == Tokens.OPEN:
            return self.handle_open(tok, context)

    def get_BooleanExpression_factory(self, tok):
        if tok == Tokens.IMP:
            return ImpExpression
        else:
            return None

    def make_BooleanExpression(self, factory, first, second):
        return factory(first, second)

    def attempt_ApplicationExpression(self, expression, context):
        """Attempt to make an application expression.  If the next tokens
        are an argument in parens, then the argument expression is a
        function being applied to the arguments.  Otherwise, return the
--
                expression = ApplicationExpression(expression, argument, None)
        return expression

    def make_VariableExpression(self, name):
        if name[0].isupper():
            return VariableExpression(name)
        else:
            return ConstantExpression(name)

def demo():
    llp = LinearLogicParser()

    print(llp.parse(r'f'))
--
SPEC_SEMTYPES = {'a'       : 'ex_quant',
                 'an'      : 'ex_quant',
                 'every'   : 'univ_quant',
                 'the'     : 'def_art',
                 'no'      : 'no_quant',
                 'default' : 'ex_quant'}

OPTIONAL_RELATIONSHIPS = ['nmod', 'vmod', 'punct']

@python_2_unicode_compatible
class GlueFormula(object):
    def __init__(self, meaning, glue, indices=None):
        if not indices:
            indices = set()

--

        self.indices = indices

    def applyto(self, arg):
        """ self = (\\x.(walk x), (subj -o f))
            arg  = (john        ,  subj)
            returns ((walk john),          f)
--

        return self.__class__(return_meaning, return_glue, return_indices)

    def make_VariableExpression(self, name):
        return VariableExpression(name)

    def make_LambdaExpression(self, variable, term):
        return LambdaExpression(variable, term)

    def lambda_abstract(self, other):
        assert isinstance(other, GlueFormula)
        assert isinstance(other.meaning, AbstractVariableExpression)
        return self.__class__(self.make_LambdaExpression(other.meaning.variable,
                                                         self.meaning),
                              linearlogic.ImpExpression(other.glue, self.glue))

    def compile(self, counter=None):
        """From Iddo Lev's PhD Dissertation p108-109"""
        if not counter:
            counter = Counter()
        (compiled_glue, new_forms) = self.glue.simplify().compile_pos(counter, self.__class__)
        return new_forms + [self.__class__(self.meaning, compiled_glue, set([counter.get()]))]

    def simplify(self):
        return self.__class__(self.meaning.simplify(), self.glue.simplify(), self.indices)

    def __eq__(self, other):
        return self.__class__ == other.__class__ and self.meaning == other.meaning and self.glue == other.glue

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        assert isinstance(self.indices, set)
        accum = '%s : %s' % (self.meaning, self.glue)
        if self.indices:
            accum += ' : {' + ', '.join(str(index) for index in self.indices) + '}'
        return accum

    def __repr__(self):
        return "%s" % self

@python_2_unicode_compatible
class GlueDict(dict):
    def __init__(self, filename, encoding=None):
        self.filename = filename
        self.file_encoding = encoding
        self.read_file()

    def read_file(self, empty_first=True):
        if empty_first:
            self.clear()

--
                self[sem][relationships].extend(glue_formulas) # add the glue entry to the dictionary


    def __str__(self):
        accum = ''
        for pos in self:
            str_pos = "%s" % pos
--
                    i += 1
        return accum

    def to_glueformula_list(self, depgraph, node=None, counter=None, verbose=False):
        if node is None:
            top = depgraph.nodelist[0]
            root = depgraph.nodelist[top['deps'][0]]
--
            glueformulas.extend(self.to_glueformula_list(depgraph, dep, counter, verbose))
        return glueformulas

    def lookup(self, node, depgraph, counter):
        semtype_names = self.get_semtypes(node)

        semtype = None
--

        return self.get_glueformulas_from_semtype_entry(lookup, node['word'], node, depgraph, counter)

    def add_missing_dependencies(self, node, depgraph):
        rel = node['rel'].lower()

        if rel == 'main':
--
            subj = self.lookup_unique('subj', headnode, depgraph)
            node['deps'].append(subj['address'])

    def _lookup_semtype_option(self, semtype, node, depgraph):
        relationships = frozenset(depgraph.nodelist[dep]['rel'].lower()
                                   for dep in node['deps']
                                   if depgraph.nodelist[dep]['rel'].lower()
--
            lookup = semtype[relationships]
        except KeyError:
            # An exact match is not found, so find the best match where
            # 'best' is defined as the glue entry whose relationship set has the
            # most relations of any possible relationship set that is a subset
            # of the actual depgraph
            best_match = frozenset()
--

        return lookup

    def get_semtypes(self, node):
        """
        Based on the node, return a list of plausible semtypes in order of
        plausibility.
--
            if word in SPEC_SEMTYPES:
                return [SPEC_SEMTYPES[word]]
            else:
                return [SPEC_SEMTYPES['default']]
        elif rel in ['nmod', 'vmod']:
            return [node['tag'], rel]
        else:
            return [node['tag']]

    def get_glueformulas_from_semtype_entry(self, lookup, word, node, depgraph, counter):
        glueformulas = []

        glueFormulaFactory = self.get_GlueFormula_factory()
--
            glueformulas.append(gf)
        return glueformulas

    def get_meaning_formula(self, generic, word):
        """
        :param generic: A meaning formula string containing the
        parameter "<word>"
--
        word = word.replace('.', '')
        return generic.replace('<word>', word)

    def initialize_labels(self, expr, node, depgraph, unique_index):
        if isinstance(expr, linearlogic.AtomicExpression):
            name = self.find_label_name(expr.name.lower(), node, depgraph, unique_index)
            if name[0].isupper():
--
                       self.initialize_labels(expr.antecedent, node, depgraph, unique_index),
                       self.initialize_labels(expr.consequent, node, depgraph, unique_index))

    def find_label_name(self, name, node, depgraph, unique_index):
        try:
            dot = name.index('.')

--
            elif name=='b':     return self.get_label(self.lookup_unique('conjb', node, depgraph))
            else:               return self.get_label(self.lookup_unique(name, node, depgraph))

    def get_label(self, node):
        """
        Pick an alphabetic character as identifier for an entity in the model.

--
        else:
            return letter

    def lookup_unique(self, rel, node, depgraph):
        """
        Lookup 'key'. There should be exactly one item in the associated relation.
        """
--
        else:
            return deps[0]

    def get_GlueFormula_factory(self):
        return GlueFormula

class Glue(object):
    def __init__(self, semtype_file=None, remove_duplicates=False,
                 depparser=None, verbose=False):
        self.verbose = verbose
        self.remove_duplicates = remove_duplicates
--
        else:
            self.semtype_file = 'glue.semtype'

    def train_depparser(self, depgraphs=None):
        if depgraphs:
            self.depparser.train(depgraphs)
        else:
--
                os.path.join('grammars', 'sample_grammars',
                             'glue_train.conll')))

    def parse_to_meaning(self, sentence):
        readings = []
        for agenda in self.parse_to_compiled(sentence):
            readings.extend(self.get_readings(agenda))
        return readings

    def get_readings(self, agenda):
        readings = []
        agenda_length = len(agenda)
        atomics = dict()
--
                    self._add_to_reading_list(gf, readings)
        return readings

    def _add_to_reading_list(self, glueformula, reading_list):
        add_reading = True
        if self.remove_duplicates:
            for reading in reading_list:
--
        if add_reading:
            reading_list.append(glueformula.meaning)

    def parse_to_compiled(self, sentence='a man sees Mary'.split()):
        gfls = [self.depgraph_to_glue(dg) for dg in self.dep_parse(sentence)]
        return [self.gfl_to_compiled(gfl) for gfl in gfls]

    def dep_parse(self, sentence='every cat leaves'.split()):
        #Lazy-initialize the depparser
        if self.depparser is None:
            from nltk.parse import MaltParser
--

        return [self.depparser.parse(sentence, verbose=self.verbose)]

    def depgraph_to_glue(self, depgraph):
        return self.get_glue_dict().to_glueformula_list(depgraph)

    def get_glue_dict(self):
        return GlueDict(self.semtype_file)

    def gfl_to_compiled(self, gfl):
        index_counter = Counter()
        return_list = []
        for gf in gfl:
--

        return return_list

    def get_pos_tagger(self):
        regexp_tagger = RegexpTagger(
            [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers
             (r'(The|the|A|a|An|an)$', 'AT'),   # articles
--
             (r'.*s$', 'NNS'),                  # plural nouns
             (r'.*ing$', 'VBG'),                # gerunds
             (r'.*ed$', 'VBD'),                 # past tense verbs
             (r'.*', 'NN')                      # nouns (default)
        ])
        brown_train = brown.tagged_sents(categories='news')
        unigram_tagger = UnigramTagger(brown_train, backoff=regexp_tagger)
--


class DrtGlueFormula(GlueFormula):
    def __init__(self, meaning, glue, indices=None):
        if not indices:
            indices = set()

--

        self.indices = indices

    def make_VariableExpression(self, name):
        return drt.DrtVariableExpression(name)

    def make_LambdaExpression(self, variable, term):
        return drt.DrtLambdaExpression(variable, term)

class DrtGlueDict(GlueDict):
    def get_GlueFormula_factory(self):
        return DrtGlueFormula

class DrtGlue(Glue):
    def __init__(self, semtype_file=None, remove_duplicates=False,
                 depparser=None, verbose=False):
        if not semtype_file:
            semtype_file = 'drt_glue.semtype'
        Glue.__init__(self, semtype_file, remove_duplicates, depparser, verbose)

    def get_glue_dict(self):
        return DrtGlueDict(self.semtype_file)


def demo(show_example=-1):
    from nltk.parse import MaltParser
    examples = ['David sees Mary',
                'David eats a sandwich',
--
from nltk.sem.glue import DrtGlue

class DrtGlueDemo(object):
    def __init__(self, examples):
        # Set up the main window.
        self._top = Tk()
        self._top.title('DRT Glue Demo')
--
    ##  Initialization Helpers
    #########################################

    def _init_glue(self):
        tagger = RegexpTagger(
            [('^(David|Mary|John)$', 'NNP'),
             ('^(walks|sees|eats|chases|believes|gives|sleeps|chases|persuades|tries|seems|leaves)$', 'VB'),
--
        depparser = MaltParser(tagger=tagger)
        self._glue = DrtGlue(depparser=depparser, remove_duplicates=False)

    def _init_fonts(self, root):
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = Font(font=Button()["font"])
        root.option_add("*Font", self._sysfont)

        # TWhat's our font size (default=same as sysfont)
        self._size = IntVar(root)
        self._size.set(self._sysfont.cget('size'))

--
        self._bigfont = Font(family='helvetica', weight='bold',
                                    size=big)

    def _init_exampleListbox(self, parent):
        self._exampleFrame = listframe = Frame(parent)
        self._exampleFrame.pack(fill='both', side='left', padx=2)
        self._exampleList_label = Label(self._exampleFrame, font=self._boldfont,
--
        # If they select a example, apply it.
        self._exampleList.bind('<<ListboxSelect>>', self._exampleList_select)

    def _init_readingListbox(self, parent):
        self._readingFrame = listframe = Frame(parent)
        self._readingFrame.pack(fill='both', side='left', padx=2)
        self._readingList_label = Label(self._readingFrame, font=self._boldfont,
--

        self._populate_readingListbox()

    def _populate_readingListbox(self):
        # Populate the listbox with integers
        self._readingList.delete(0, 'end')
        for i in range(len(self._readings)):
--
        # If they select a example, apply it.
        self._readingList.bind('<<ListboxSelect>>', self._readingList_select)

    def _init_bindings(self):
        # Key bindings are a good thing.
        self._top.bind('<Control-q>', self.destroy)
        self._top.bind('<Control-x>', self.destroy)
--
        self._top.bind('p', self.prev)
        self._top.bind('<BackSpace>', self.prev)

    def _init_buttons(self, parent):
        # Set up the frames.
        self._buttonframe = buttonframe = Frame(parent)
        buttonframe.pack(fill='none', side='bottom', padx=3, pady=2)
--
               background='#90c0d0', foreground='black',
               command=self.next,).pack(side='left')

    def _configure(self, event):
        self._autostep = 0
        (x1, y1, x2, y2) = self._cframe.scrollregion()
        y2 = event.height - 6
        self._canvas['scrollregion'] = '%d %d %d %d' % (x1,y1,x2,y2)
        self._redraw()

    def _init_canvas(self, parent):
        self._cframe = CanvasFrame(parent, background='white',
                                   #width=525, height=250,
                                   closeenough=10,
--
        self._textwidgets = []
        self._textline = None

    def _init_menubar(self, parent):
        menubar = Menu(parent)

        filemenu = Menu(menubar, tearoff=0)
--
    ##  Main draw procedure
    #########################################

    def _redraw(self):
        canvas = self._canvas

        # Delete the old DRS, widgets, etc.
--
    ##  Button Callbacks
    #########################################

    def destroy(self, *e):
        self._autostep = 0
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def prev(self, *e):
        selection = self._readingList.curselection()
        readingListSize = self._readingList.size()

--
            self._select_previous_example()


    def _select_previous_example(self):
        #if the current example is not the first example
        if self._curExample > 0:
            self._exampleList_store_selection(self._curExample-1)
--
            #go to the last example
            self._exampleList_store_selection(len(self._examples)-1)

    def next(self, *e):
        selection = self._readingList.curselection()
        readingListSize = self._readingList.size()

--
        else:
            self._select_next_example()

    def _select_next_example(self):
        #if the current example is not the last example
        if self._curExample < len(self._examples)-1:
            self._exampleList_store_selection(self._curExample+1)
--
            self._exampleList_store_selection(0)


    def about(self, *e):
        ABOUT = ("NLTK Discourse Representation Theory (DRT) Glue Semantics Demo\n"+
                 "Written by Daniel H. Garrette")
        TITLE = 'About: NLTK DRT Glue Demo'
--
        except:
            ShowText(self._top, TITLE, ABOUT)

    def postscript(self, *e):
        self._autostep = 0
        self._cframe.print_to_file()

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
        if in_idle(): return
        self._top.mainloop(*args, **kwargs)

    def resize(self, size=None):
        if size is not None: self._size.set(size)
        size = self._size.get()
        self._font.configure(size=-(abs(size)))
--
        self._bigfont.configure(size=-(abs(size+2)))
        self._redraw()

    def _toggle_remove_duplicates(self):
        self._glue.remove_duplicates = not self._glue.remove_duplicates

        self._exampleList.selection_clear(0, 'end')
--
        self._redraw()


    def _exampleList_select(self, event):
        selection = self._exampleList.curselection()
        if len(selection) != 1: return
        self._exampleList_store_selection(int(selection[0]))

    def _exampleList_store_selection(self, index):
        self._curExample = index
        example = self._examples[index]

--
            self._redraw()


    def _readingList_select(self, event):
        selection = self._readingList.curselection()
        if len(selection) != 1: return
        self._readingList_store_selection(int(selection[0]))

    def _readingList_store_selection(self, index):
        reading = self._readings[index]

        self._readingList.selection_clear(0, 'end')
--


class DrsWidget(object):
    def __init__(self, canvas, drs, **attribs):
        self._drs = drs
        self._canvas = canvas
        canvas.font = Font(font=canvas.itemcget(canvas.create_text(0, 0, text=''), 'font'))
        canvas._BUFFER = 3
        self.bbox = (0, 0, 0, 0)

    def draw(self):
        (right, bottom) = DrsDrawer(self._drs, canvas=self._canvas).draw()
        self.bbox = (0, 0, right+1, bottom+1)

    def clear(self):
        self._canvas.create_rectangle(self.bbox, fill="white", width="0" )

def demo():
    examples = ['John walks',
                'David sees Mary',
                'David eats a sandwich',
--

This package implement a wrapper around scikit-learn classifiers. To use this
wrapper, construct a scikit-learn estimator object, then use that to construct
a SklearnClassifier. E.g., to wrap a linear SVM with default settings:

>>> from sklearn.svm import LinearSVC
>>> from nltk.classify.scikitlearn import SklearnClassifier
--
class SklearnClassifier(ClassifierI):
    """Wrapper for scikit-learn classifiers."""

    def __init__(self, estimator, dtype=float, sparse=True):
        """
        :param estimator: scikit-learn classifier object.

        :param dtype: data type used when building feature array.
            scikit-learn estimators work exclusively on numeric data. The
            default value should be fine for almost all situations.

        :param sparse: Whether to use sparse matrices internally.
            The estimator must support these; not all scikit-learn classifiers
            do (see their respective documentation and look for "sparse
            matrix"). The default value is True, since most NLP problems
            involve sparse feature sets. Setting this to False may take a
            great amount of memory.
        :type sparse: boolean.
--
        self._encoder = LabelEncoder()
        self._vectorizer = DictVectorizer(dtype=dtype, sparse=sparse)

    def __repr__(self):
        return "<SklearnClassifier(%r)>" % self._clf

    def batch_classify(self, featuresets):
        """Classify a batch of samples.

        :param featuresets: An iterable over featuresets, each a dict mapping
--
        classes = self._encoder.classes_
        return [classes[i] for i in self._clf.predict(X)]

    def batch_prob_classify(self, featuresets):
        """Compute per-class probabilities for a batch of samples.

        :param featuresets: An iterable over featuresets, each a dict mapping
--
        y_proba_list = self._clf.predict_proba(X)
        return [self._make_probdist(y_proba) for y_proba in y_proba_list]

    def labels(self):
        """The class labels used by this classifier.

        :rtype: list
        """
        return list(self._encoder.classes_)

    def train(self, labeled_featuresets):
        """
        Train (fit) the scikit-learn estimator.

--

        return self

    def _make_probdist(self, y_proba):
        classes = self._encoder.classes_
        return DictionaryProbDist(dict((classes[i], p)
                                       for i, p in enumerate(y_proba)))


# skip doctests if scikit-learn is not installed
def setup_module(module):
    from nose import SkipTest
    try:
        import sklearn
--
(stored as a list of words) to a featureset describing the set of
words included in the document:

    >>> # Define a feature detector function.
    >>> def document_features(document):
    ...     return dict([('contains-word(%s)' % w, True) for w in document])

Feature detectors are typically applied to each token before it is fed
--
for WSD includes features describing the left and right contexts of
the target word:

    >>> def wsd_features(sentence, index):
    ...     featureset = {}
    ...     for i in range(max(0, index-3), index):
    ...         featureset['left-context(%s)' % sentence[i]] = True
--

The features of a sentence are simply the words it contains:

    >>> def features(sentence):
    ...     words = sentence.lower().split()
    ...     return dict(('contains(%s)' % w, True) for w in words)

--
    True
"""

from collections import defaultdict

from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist

--

class PositiveNaiveBayesClassifier(NaiveBayesClassifier):
    @staticmethod
    def train(positive_featuresets, unlabeled_featuresets, positive_prob_prior=0.5,
              estimator=ELEProbDist):
        """
        :param positive_featuresets: A list of featuresets that are known as positive
--
        :param unlabeled_featuresets: A list of featuresets whose label is unknown.

        :param positive_prob_prior: A prior estimate of the probability of the label
            ``True`` (default 0.5).
        """
        positive_feature_freqdist = defaultdict(FreqDist)
        unlabeled_feature_freqdist = defaultdict(FreqDist)
        feature_values = defaultdict(set)
        fnames = set()

        # Count up how many times each feature value occurred in positive examples.
--
##  Demo
##//////////////////////////////////////////////////////

def demo():
    from nltk.classify.util import partial_names_demo
    classifier = partial_names_demo(PositiveNaiveBayesClassifier.train)
    classifier.show_most_informative_features()
--
(or `scikit-learn <http://scikit-learn.org>`_ directly).
"""
class SvmClassifier(object):
    def __init__(self, *args, **kwargs):
        raise NotImplementedError(__doc__)
--
======================
The term *feature* is usually used to refer to some property of an
unlabeled token.  For example, when performing word sense
disambiguation, we might define a ``'prevword'`` feature whose value is
the word preceding the target word.  However, in the context of
maxent modeling, the term *feature* is typically used to refer to a
property of a "labeled" token.  In order to prevent confusion, we
--
import tempfile
import os
import gzip
from collections import defaultdict

from nltk import compat
from nltk.data import gzip_open_unicode
--

      dotprod(a,b) = sum(x*y for (x,y) in zip(a,b))
    """
    def __init__(self, encoding, weights, logarithmic=True):
        """
        Construct a new maxent classifier model.  Typically, new
        classifier models are created using the ``train()`` method.
--
        #self._logarithmic = False
        assert encoding.length() == len(weights)

    def labels(self):
        return self._encoding.labels()

    def set_weights(self, new_weights):
        """
        Set the feature weight vector for this classifier.
        :param new_weights: The new feature weight vector.
--
        self._weights = new_weights
        assert (self._encoding.length() == len(new_weights))

    def weights(self):
        """
        :return: The feature weight vector for this classifier.
        :rtype: list of float
        """
        return self._weights

    def classify(self, featureset):
        return self.prob_classify(featureset).max()

    def prob_classify(self, featureset):
        prob_dict = {}
        for label in self._encoding.labels():
            feature_vector = self._encoding.encode(featureset, label)
--
        return DictionaryProbDist(prob_dict, log=self._logarithmic,
                                  normalize=True)

    def explain(self, featureset, columns=4):
        """
        Print a table showing the effect of each of the features in
        the given feature set, and how they combine to determine the
--
        print('  Feature'.ljust(descr_width)+''.join(
            '%8s' % (("%s" % l)[:7]) for l in labels))
        print('  '+'-'*(descr_width-2+8*len(labels)))
        sums = defaultdict(int)
        for i, label in enumerate(labels):
            feature_vector = self._encoding.encode(featureset, label)
            feature_vector.sort(key=lambda fid__: abs(self._weights[fid__[0]]),
--
        print('  PROBS:'.ljust(descr_width)+''.join(
            '%8.3f' % pdist.prob(l) for l in labels))

    def show_most_informative_features(self, n=10, show='all'):
        """
        :param show: all, neg, or pos (for negative-only or positive-only)
        """
--
            print('%8.3f %s' % (self._weights[fid],
                                self._encoding.describe(fid)))

    def __repr__(self):
        return ('<ConditionalExponentialClassifier: %d labels, %d features>' %
                (len(self._encoding.labels()), self._encoding.length()))

--
    ALGORITHMS = ['GIS', 'IIS', 'MEGAM', 'TADM']

    @classmethod
    def train(cls, train_toks, algorithm=None, trace=3, encoding=None,
              labels=None, sparse=None, gaussian_prior_sigma=0, **cutoffs):
        """
        Train a new maxent classifier based on the given corpus of
--
            - External Libraries (requiring megam):
              LM-BFGS algorithm, with training performed by Megam (``'megam'``)

            The default algorithm is ``'IIS'``.

        :type trace: int
        :param trace: The level of diagnostic tracing output to produce.
--
    input-feature values and labels that are present in a given
    corpus.
    """
    def encode(self, featureset, label):
        """
        Given a (featureset, label) pair, return the corresponding
        vector of joint-feature values.  This vector is represented as
--
        """
        raise NotImplementedError()

    def length(self):
        """
        :return: The size of the fixed-length joint-feature vectors
            that are generated by this encoding.
--
        """
        raise NotImplementedError()

    def labels(self):
        """
        :return: A list of the \"known labels\" -- i.e., all labels
            ``l`` such that ``self.encode(fs,l)`` can be a nonzero
--
        """
        raise NotImplementedError()

    def describe(self, fid):
        """
        :return: A string describing the value of the joint-feature
            whose index in the generated feature vectors is ``fid``.
--
        """
        raise NotImplementedError()

    def train(cls, train_toks):
        """
        Construct and return new feature encoding, based on a given
        training corpus ``train_toks``.
--
    A feature encoding that calls a user-supplied function to map a
    given featureset/label pair to a sparse joint-feature vector.
    """
    def __init__(self, func, length, labels):
        """
        Construct a new feature encoding based on the given function.

--
        self._func = func
        self._labels = labels

    def encode(self, featureset, label):
        return self._func(featureset, label)

    def length(self):
        return self._length

    def labels(self):
        return self._labels

    def describe(self, fid):
        return 'no description available'

class BinaryMaxentFeatureEncoding(MaxentFeatureEncodingI):
--
    These always-on features allow the maxent model to directly model
    the prior probabilities of each label.
    """
    def __init__(self, labels, mapping, unseen_features=False,
                 alwayson_features=False):
        """
        :param labels: A list of the \"known labels\" for this encoding.
--
                                 for (i, fname) in enumerate(fnames))
            self._length += len(fnames)

    def encode(self, featureset, label):
        # Inherit docs.
        encoding = []

--

        return encoding

    def describe(self, f_id):
        # Inherit docs.
        if not isinstance(f_id, compat.integer_types):
            raise TypeError('describe() expected an int')
--
        else:
            raise ValueError('Bad feature id')

    def labels(self):
        # Inherit docs.
        return self._labels

    def length(self):
        # Inherit docs.
        return self._length

    @classmethod
    def train(cls, train_toks, count_cutoff=0, labels=None, **options):
        """
        Construct and return new feature encoding, based on a given
        training corpus ``train_toks``.  See the class description
--
        """
        mapping = {}              # maps (fname, fval, label) -> fid
        seen_labels = set()       # The set of labels we've encountered
        count = defaultdict(int)  # maps (fname, fval) -> count

        for (tok, label) in train_toks:
            if labels and label not in labels:
--
class GISEncoding(BinaryMaxentFeatureEncoding):
    """
    A binary feature encoding which adds one new joint-feature to the
    joint-features defined by ``BinaryMaxentFeatureEncoding``: a
    correction feature, whose value is chosen to ensure that the
    sparse vector always sums to a constant non-negative number.  This
    new feature is used to ensure two preconditions for the GIS
--
      - The feature vector must sum to a constant non-negative number
        for every token.
    """
    def __init__(self, labels, mapping, unseen_features=False,
                 alwayson_features=False, C=None):
        """
        :param C: The correction constant.  The value of the correction
--
        self._C = C

    @property
    def C(self):
        """The non-negative constant that all encoded feature vectors
        will sum to."""
        return self._C

    def encode(self, featureset, label):
        # Get the basic encoding.
        encoding = BinaryMaxentFeatureEncoding.encode(self, featureset, label)
        base_length = BinaryMaxentFeatureEncoding.length(self)
--
        # Return the result
        return encoding

    def length(self):
        return BinaryMaxentFeatureEncoding.length(self) + 1

    def describe(self, f_id):
        if f_id == BinaryMaxentFeatureEncoding.length(self):
            return 'Correction feature (%s)' % self._C
        else:
--


class TadmEventMaxentFeatureEncoding(BinaryMaxentFeatureEncoding):
    def __init__(self, labels, mapping, unseen_features=False,
                       alwayson_features=False):
        self._mapping = OrderedDict(mapping)
        self._label_mapping = OrderedDict()
--
                                                   unseen_features,
                                                   alwayson_features)

    def encode(self, featureset, label):
        encoding = []
        for feature, value in featureset.items():
            if (feature, label) not in self._mapping:
--
                             self._label_mapping[value]))
        return encoding

    def labels(self):
        return self._labels

    def describe(self, fid):
        for (feature, label) in self._mapping:
            if self._mapping[(feature, label)] == fid:
                return (feature, label)

    def length(self):
        return len(self._mapping)

    @classmethod
    def train(cls, train_toks, count_cutoff=0, labels=None, **options):
        mapping = OrderedDict()
        if not labels:
            labels = []
--
    These always-on features allow the maxent model to directly model
    the prior probabilities of each label.
    """
    def __init__(self, labels, mapping, unseen_features=False,
                 alwayson_features=False):
        """
        :param labels: A list of the \"known labels\" for this encoding.
--
                                for (i, fname) in enumerate(fnames))
            self._length += len(fnames)

    def encode(self, featureset, label):
        # Inherit docs.
        encoding = []

--

        return encoding

    def describe(self, f_id):
        # Inherit docs.
        if not isinstance(f_id, compat.integer_types):
            raise TypeError('describe() expected an int')
--
        else:
            raise ValueError('Bad feature id')

    def labels(self):
        # Inherit docs.
        return self._labels

    def length(self):
        # Inherit docs.
        return self._length

    @classmethod
    def train(cls, train_toks, count_cutoff=0, labels=None, **options):
        """
        Construct and return new feature encoding, based on a given
        training corpus ``train_toks``.  See the class description
--
        """
        mapping = {}              # maps (fname, fval, label) -> fid
        seen_labels = set()       # The set of labels we've encountered
        count = defaultdict(int)  # maps (fname, fval) -> count

        for (tok, label) in train_toks:
            if labels and label not in labels:
--
#{ Classifier Trainer: Generalized Iterative Scaling
######################################################################

def train_maxent_classifier_with_gis(train_toks, trace=3, encoding=None,
                                     labels=None, **cutoffs):
    """
    Train a new ``ConditionalExponentialClassifier``, using the given
--

    :see: ``train_maxent_classifier()`` for parameter descriptions.
    """
    cutoffs.setdefault('max_iter', 100)
    cutoffchecker = CutoffChecker(cutoffs)

    # Construct an encoding from the training data.
--

    if not hasattr(encoding, 'C'):
        raise TypeError('The GIS algorithm requires an encoding that '
                        'defines C (e.g., GISEncoding).')

    # Cinv is the inverse of the sum of each joint feature vector.
    # This controls the learning rate: higher Cinv (or lower C) gives
--
# Return the classifier.
    return classifier

def calculate_empirical_fcount(train_toks, encoding):
    fcount = numpy.zeros(encoding.length(), 'd')

    for tok, label in train_toks:
--

    return fcount

def calculate_estimated_fcount(classifier, train_toks, encoding):
    fcount = numpy.zeros(encoding.length(), 'd')

    for tok, label in train_toks:
--
#{ Classifier Trainer: Improved Iterative Scaling
######################################################################

def train_maxent_classifier_with_iis(train_toks, trace=3, encoding=None,
                                     labels=None, **cutoffs):
    """
    Train a new ``ConditionalExponentialClassifier``, using the given
--

    :see: ``train_maxent_classifier()`` for parameter descriptions.
    """
    cutoffs.setdefault('max_iter', 100)
    cutoffchecker = CutoffChecker(cutoffs)

    # Construct an encoding from the training data.
--
    # Return the classifier.
    return classifier

def calculate_nfmap(train_toks, encoding):
    """
    Construct a map that can be used to compress ``nf`` (which is
    typically sparse).
--
            nfset.add(sum(val for (id,val) in encoding.encode(tok,label)))
    return dict((nf, i) for (i, nf) in enumerate(nfset))

def calculate_deltas(train_toks, classifier, unattested, ffreq_empirical,
                     nfmap, nfarray, nftranspose, encoding):
    """
    Calculate the update values for the classifier weights for
--

    | delta[i] -= (ffreq_empirical[i] - sum1[i])/(-sum2[i])

    until convergence, where *sum1* and *sum2* are defined as:

    |    sum1[i](delta) = SUM[fs,l] f[i](fs,l,delta)
    |    sum2[i](delta) = SUM[fs,l] (f[i](fs,l,delta).nf(feature_vector(fs,l)))
--
# this would need to put requirements on what encoding is used.  But
# we may need this for other maxent classifier trainers that require
# implicit formats anyway.
def train_maxent_classifier_with_megam(train_toks, trace=3, encoding=None,
                                       labels=None, gaussian_prior_sigma=0,
                                       **kwargs):
    """
--

class TadmMaxentClassifier(MaxentClassifier):
    @classmethod
    def train(cls, train_toks, **kwargs):
        algorithm = kwargs.get('algorithm', 'tao_lmvm')
        trace = kwargs.get('trace', 3)
        encoding = kwargs.get('encoding', None)
--
######################################################################
#{ Demo
######################################################################
def demo():
    from nltk.classify.util import names_demo
    classifier = names_demo(MaxentClassifier.train)

--
######################################################################

_megam_bin = None
def config_megam(bin=None):
    """
    Configure NLTK's interface to the ``megam`` maxent optimization
    package.
--
#{ Megam Interface Functions
######################################################################

def write_megam_file(train_toks, encoding, stream,
                     bernoulli=True, explicit=True):
    """
    Generate an input file for ``megam`` based on the given corpus of
--
        # End of the instance.
        stream.write('\n')

def parse_megam_weights(s, features_count, explicit=True):
    """
    Given the stdout output generated by ``megam`` when training a
    model, return a ``numpy`` array containing the corresponding weight
--
            weights[int(fid)] = float(weight)
    return weights

def _write_megam_features(vector, stream, bernoulli):
    if not vector:
        raise ValueError('MEGAM classifier requires the use of an '
                         'always-on feature.')
--
        else:
            stream.write(' %s %s' % (fid, fval))

def call_megam(args):
    """
    Call the ``megam`` binary with the given arguments.
    """
--
    numpy = None

_tadm_bin = None
def config_tadm(bin=None):
    global _tadm_bin
    _tadm_bin = find_binary(
        'tadm', bin,
--
        binary_names=['tadm'],
        url='http://tadm.sf.net')

def write_tadm_file(train_toks, encoding, stream):
    """
    Generate an input file for ``tadm`` based on the given corpus of
    classified tokens.
--
            )
            stream.write(line)

def parse_tadm_weights(paramfile):
    """
    Given the stdout output generated by ``tadm`` when training a
    model, return a ``numpy`` array containing the corresponding weight
--
        weights.append(float(line.strip()))
    return numpy.array(weights, 'd')

def call_tadm(args):
    """
    Call the ``tadm`` binary with the given arguments.
    """
--
        print(stderr)
        raise OSError('tadm command failed!')

def names_demo():
    from nltk.classify.util import names_demo
    from nltk.classify.maxent import TadmMaxentClassifier
    classifier = names_demo(TadmMaxentClassifier.train)

def encoding_demo():
    import sys
    from nltk.classify.maxent import TadmEventMaxentFeatureEncoding
    tokens = [({'f0':1, 'f1':1, 'f3':1}, 'A'),
--
"""
from __future__ import print_function, unicode_literals

from collections import defaultdict

from nltk.probability import FreqDist, DictionaryProbDist, ELEProbDist, sum_logs
from .api import ClassifierI
--
    you generally should not use 'None' as a feature value for one of
    your own features.
    """
    def __init__(self, label_probdist, feature_probdist):
        """
        :param label_probdist: P(label), the probability distribution
            over labels.  It is expressed as a ``ProbDistI`` whose
--
        self._feature_probdist = feature_probdist
        self._labels = list(label_probdist.samples())

    def labels(self):
        return self._labels

    def classify(self, featureset):
        return self.prob_classify(featureset).max()

    def prob_classify(self, featureset):
        # Discard any feature names that we've never seen before.
        # Otherwise, we'll just assign a probability of 0 to
        # everything.
--

        return DictionaryProbDist(logprob, normalize=True, log=True)

    def show_most_informative_features(self, n=10):
        # Determine the most relevant features, and display them.
        cpdist = self._feature_probdist
        print('Most Informative Features')

        for (fname, fval) in self.most_informative_features(n):
            def labelprob(l):
                return cpdist[l,fname].prob(fval)
            labels = sorted([l for l in self._labels
                             if fval in cpdist[l,fname].samples()],
--
            print(('%24s = %-14r %6s : %-6s = %s : 1.0' %
                   (fname, fval, ("%s" % l1)[:6], ("%s" % l0)[:6], ratio)))

    def most_informative_features(self, n=100):
        """
        Return a list of the 'most informative' features used by this
        classifier.  For the purpose of this function, the
--
        features = set()
        # The max & min probability associated w/ each (fname, fval)
        # pair.  Maps (fname,fval) -> float.
        maxprob = defaultdict(lambda: 0.0)
        minprob = defaultdict(lambda: 1.0)

        for (label, fname), probdist in self._feature_probdist.items():
            for fval in probdist.samples():
--
        return features[:n]

    @staticmethod
    def train(labeled_featuresets, estimator=ELEProbDist):
        """
        :param labeled_featuresets: A list of classified featuresets,
            i.e., a list of tuples ``(featureset, label)``.
        """
        label_freqdist = FreqDist()
        feature_freqdist = defaultdict(FreqDist)
        feature_values = defaultdict(set)
        fnames = set()

        # Count up how many times each feature value occurred, given
--
##  Demo
##//////////////////////////////////////////////////////

def demo():
    from nltk.classify.util import names_demo
    classifier = names_demo(NaiveBayesClassifier.train)
    classifier.show_most_informative_features()
--
# alternative name possibility: 'detect_features()'?
# alternative name possibility: 'map_featuredetect()'?
# or.. just have users use LazyMap directly?
def apply_features(feature_func, toks, labeled=None):
    """
    Use the ``LazyMap`` class to construct a lazy list-like
    object that is analogous to ``map(feature_func, toks)``.  In
--
        then the list elements should be tuples ``(tok,label)``, and
        ``tok`` will be passed to ``feature_func()``.
    :param labeled: If true, then ``toks`` contains labeled tokens --
        i.e., tuples of the form ``(tok, label)``.  (Default:
        auto-detect based on types.)
    """
    if labeled is None:
        labeled = toks and isinstance(toks[0], (tuple, list))
    if labeled:
        def lazy_func(labeled_token):
            return (feature_func(labeled_token[0]), labeled_token[1])
        return LazyMap(lazy_func, toks)
    else:
        return LazyMap(feature_func, toks)

def attested_labels(tokens):
    """
    :return: A list of all labels that are attested in the given list
        of tokens.
--
    """
    return tuple(set(label for (tok,label) in tokens))

def log_likelihood(classifier, gold):
    results = classifier.batch_prob_classify([fs for (fs,l) in gold])
    ll = [pdist.prob(l) for ((fs,l), pdist) in zip(gold, results)]
    return math.log(float(sum(ll))/len(ll))

def accuracy(classifier, gold):
    results = classifier.batch_classify([fs for (fs,l) in gold])
    correct = [l==r for ((fs,l), r) in zip(gold, results)]
    if correct:
--
    Accuracy cutoffs are also implemented, but they're almost never
    a good idea to use.
    """
    def __init__(self, cutoffs):
        self.cutoffs = cutoffs.copy()
        if 'min_ll' in cutoffs:
            cutoffs['min_ll'] = -abs(cutoffs['min_ll'])
--
        self.acc = None
        self.iter = 1

    def check(self, classifier, train_toks):
        cutoffs = self.cutoffs
        self.iter += 1
        if 'max_iter' in cutoffs and self.iter >= cutoffs['max_iter']:
--
#{ Demos
######################################################################

def names_demo_features(name):
    features = {}
    features['alwayson'] = True
    features['startswith'] = name[0].lower()
    features['endswith'] = name[-1].lower()
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        features['count(%s)' % letter] = name.lower().count(letter)
        features['has(%s)' % letter] = letter in name.lower()
    return features

def binary_names_demo_features(name):
    features = {}
    features['alwayson'] = True
    features['startswith(vowel)'] = name[0].lower() in 'aeiouy'
    features['endswith(vowel)'] = name[-1].lower() in 'aeiouy'
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        features['count(%s)' % letter] = name.lower().count(letter)
        features['has(%s)' % letter] = letter in name.lower()
        features['startswith(%s)' % letter] = (letter==name[0].lower())
        features['endswith(%s)' % letter] = (letter==name[-1].lower())
    return features

def names_demo(trainer, features=names_demo_features):
    from nltk.corpus import names
    import random

--
    # Return the classifier
    return classifier

def partial_names_demo(trainer, features=names_demo_features):
    from nltk.corpus import names
    import random

--
    return classifier

_inst_cache = {}
def wsd_demo(trainer, word, features, n=1000):
    from nltk.corpus import senseval
    import random

--
"""
from __future__ import print_function, unicode_literals

from collections import defaultdict

from nltk.probability import FreqDist, MLEProbDist, entropy
from nltk.classify.api import ClassifierI
--

@python_2_unicode_compatible
class DecisionTreeClassifier(ClassifierI):
    def __init__(self, label, feature_name=None, decisions=None, default=None):
        """
        :param label: The most likely label for tokens that reach
            this node in the decision tree.  If this decision tree
--
        :param decisions: A dictionary mapping from feature values
            for the feature identified by ``feature_name`` to
            child decision trees.
        :param default: The child that will be used if the value of
            feature ``feature_name`` does not match any of the keys in
            ``decisions``.  This is used when constructing binary
            decision trees.
--
        self._label = label
        self._fname = feature_name
        self._decisions = decisions
        self._default = default

    def labels(self):
        labels = [self._label]
        if self._decisions is not None:
            for dt in self._decisions.values():
                labels.extend(dt.labels())
        if self._default is not None:
            labels.extend(self._default.labels())
        return list(set(labels))

    def classify(self, featureset):
        # Decision leaf:
        if self._fname is None:
            return self._label
--
        fval = featureset.get(self._fname)
        if fval in self._decisions:
            return self._decisions[fval].classify(featureset)
        elif self._default is not None:
            return self._default.classify(featureset)
        else:
            return self._label

    def error(self, labeled_featuresets):
        errors = 0
        for featureset, label in labeled_featuresets:
            if self.classify(featureset) != label:
                errors += 1
        return float(errors)/len(labeled_featuresets)

    def pp(self, width=70, prefix='', depth=4):
        """
        Return a string containing a pretty-printed version of this
        decision tree.  Each line in this string corresponds to a
        single decision tree node or leaf, and indentation is used to
        display the structure of the decision tree.
        """
        # [xx] display default!!
        if self._fname is None:
            n = width-len(prefix)-15
            return '%s%s %s\n' % (prefix, '.'*n, self._label)
--
            s += '%s%s %s\n' % (hdr, '.'*(n), result._label)
            if result._fname is not None and depth>1:
                s += result.pp(width, prefix+'  ', depth-1)
        if self._default is not None:
            n = width-len(prefix)-21
            s += '%selse: %s %s\n' % (prefix, '.'*n, self._default._label)
            if self._default._fname is not None and depth>1:
                s += self._default.pp(width, prefix+'  ', depth-1)
        return s

    def pseudocode(self, prefix='', depth=4):
        """
        Return a string representation of this decision tree that
        expresses the decisions it makes as a nested set of pseudocode
--
                s += '\n'+result.pseudocode(prefix+'  ', depth-1)
            else:
                s += 'return %r\n' % result._label
        if self._default is not None:
            if len(self._decisions) == 1:
                s += '%sif %s != %r: '% (prefix, self._fname,
                                         list(self._decisions.keys())[0])
            else:
                s += '%selse: ' % (prefix,)
            if self._default._fname is not None and depth>1:
                s += '\n'+self._default.pseudocode(prefix+'  ', depth-1)
            else:
                s += 'return %r\n' % self._default._label
        return s

    def __str__(self):
        return self.pp()

    @staticmethod
    def train(labeled_featuresets, entropy_cutoff=0.05, depth_cutoff=100,
              support_cutoff=10, binary=False, feature_values=None,
              verbose=False):
        """
--

        # Collect a list of the values each feature can take.
        if feature_values is None and binary:
            feature_values = defaultdict(set)
            for featureset, label in labeled_featuresets:
                for fname, fval in featureset.items():
                    feature_values[fname].add(fval)
--
        return tree

    @staticmethod
    def leaf(labeled_featuresets):
        label = FreqDist(label for (featureset,label)
                         in labeled_featuresets).max()
        return DecisionTreeClassifier(label)

    @staticmethod
    def stump(feature_name, labeled_featuresets):
        label = FreqDist(label for (featureset,label)
                         in labeled_featuresets).max()

        # Find the best label for each value.
        freqs = defaultdict(FreqDist) # freq(label|value)
        for featureset, label in labeled_featuresets:
            feature_value = featureset.get(feature_name)
            freqs[feature_value].inc(label)
--
                         for val in freqs)
        return DecisionTreeClassifier(label, feature_name, decisions)

    def refine(self, labeled_featuresets, entropy_cutoff, depth_cutoff,
               support_cutoff, binary=False, feature_values=None,
               verbose=False):
        if len(labeled_featuresets) <= support_cutoff: return
--
                self._decisions[fval] = DecisionTreeClassifier.train(
                    fval_featuresets, entropy_cutoff, depth_cutoff,
                    support_cutoff, binary, feature_values, verbose)
        if self._default is not None:
            default_featuresets = [(featureset, label) for (featureset, label)
                                   in labeled_featuresets
                                   if featureset.get(self._fname) not in
                                   self._decisions]
            label_freqs = FreqDist(label for (featureset,label)
                                   in default_featuresets)
            if entropy(MLEProbDist(label_freqs)) > entropy_cutoff:
                self._default = DecisionTreeClassifier.train(
                    default_featuresets, entropy_cutoff, depth_cutoff,
                    support_cutoff, binary, feature_values, verbose)

    @staticmethod
    def best_stump(feature_names, labeled_featuresets, verbose=False):
        best_stump = DecisionTreeClassifier.leaf(labeled_featuresets)
        best_error = best_stump.error(labeled_featuresets)
        for fname in feature_names:
--
        return best_stump

    @staticmethod
    def binary_stump(feature_name, feature_value, labeled_featuresets):
        label = FreqDist(label for (featureset, label)
                         in labeled_featuresets).max()

--


        decisions = {}
        default = label
        # But hopefully we have observations!
        if pos_fdist.N() > 0:
            decisions = {feature_value: DecisionTreeClassifier(pos_fdist.max())}
        if neg_fdist.N() > 0:
            default = DecisionTreeClassifier(neg_fdist.max())

        return DecisionTreeClassifier(label, feature_name, decisions, default)

    @staticmethod
    def best_binary_stump(feature_names, labeled_featuresets, feature_values,
                          verbose=False):
        best_stump = DecisionTreeClassifier.leaf(labeled_featuresets)
        best_error = best_stump.error(labeled_featuresets)
--
            descr = '%s=%s' % (best_stump._fname,
                               list(best_stump._decisions.keys())[0])
        else:
            descr = '(default)'
        if verbose:
            print(('best stump for %6d toks uses %-20s err=%6.4f' %
                   (len(labeled_featuresets), descr, best_error)))
--
##  Demo
##//////////////////////////////////////////////////////

def f(x):
    return DecisionTreeClassifier.train(x, binary=True, verbose=True)

def demo():
    from nltk.classify.util import names_demo, binary_names_demo_features
    classifier = names_demo(f, #DecisionTreeClassifier.train,
                            binary_names_demo_features)
--
                '/usr/local/share/weka',
                '/usr/lib/weka',
                '/usr/local/lib/weka',]
def config_weka(classpath=None):
    global _weka_classpath

    # Make sure java's configured first.
--
                          'For more information about Weka, please see '
                          'http://www.cs.waikato.ac.nz/ml/weka/')

def _check_weka_version(jar):
    try:
        zf = zipfile.ZipFile(jar)
    except SystemExit as KeyboardInterrupt:
--
        zf.close()

class WekaClassifier(ClassifierI):
    def __init__(self, formatter, model_filename):
        self._formatter = formatter
        self._model = model_filename

    def batch_prob_classify(self, featuresets):
        return self._batch_classify(featuresets, ['-p', '0', '-distribution'])

    def batch_classify(self, featuresets):
        return self._batch_classify(featuresets, ['-p', '0'])

    def _batch_classify(self, featuresets, options):
        # Make sure we can find java & weka.
        config_weka()

--
                os.remove(os.path.join(temp_dir, f))
            os.rmdir(temp_dir)

    def parse_weka_distribution(self, s):
        probs = [float(v) for v in re.split('[*,]+', s) if v.strip()]
        probs = dict(zip(self._formatter.labels(), probs))
        return DictionaryProbDist(probs)

    def parse_weka_output(self, lines):
        # Strip unwanted text from stdout
        for i,line in enumerate(lines):
            if line.strip().startswith("inst#"):
--
        'ripper': 'weka.classifiers.rules.JRip',
        }
    @classmethod
    def train(cls, model_filename, featuresets,
              classifier='naivebayes', options=[], quiet=True):
        # Make sure we can find java & weka.
        config_weka()
--
    be determined from data using ``from_train``.
    """

    def __init__(self, labels, features):
        """
        :param labels: A list of all class labels that can be generated.
        :param features: A list of feature specifications, where
--
        self._labels = labels
        self._features = features

    def format(self, tokens):
        """Returns a string representation of ARFF output for the given data."""
        return self.header_section() + self.data_section(tokens)

    def labels(self):
        """Returns the list of classes."""
        return list(self._labels)

    def write(self, outfile, tokens):
        """Writes ARFF data to a file for the given data."""
        if not hasattr(outfile, 'write'):
            outfile = open(outfile, 'w')
--
        outfile.close()

    @staticmethod
    def from_train(tokens):
        """
        Constructs an ARFF_Formatter instance with class labels and feature
        types determined from the given data. Handles boolean, numeric and
--

        return ARFF_Formatter(labels, features)

    def header_section(self):
        """Returns an ARFF header as a string."""
        # Header comment.
        s = ('% Weka ARFF file\n' +
--

        return s

    def data_section(self, tokens, labeled=None):
        """
        Returns the ARFF data section for the given data.

--

        return s

    def _fmt_arff_val(self, fval):
        if fval is None:
            return '?'
        elif isinstance(fval, (bool, compat.integer_types)):
--

if __name__ == '__main__':
    from nltk.classify.util import names_demo, binary_names_demo_features
    def make_classifier(featuresets):
        return WekaClassifier.train('/tmp/name.model', featuresets,
                                    'C4.5')
    classifier = names_demo(make_classifier, binary_names_demo_features)
--

_mallet_home = None
_mallet_classpath = None
def config_mallet(mallet_home=None):
    """
    Configure NLTK's interface to the Mallet machine learning package.

--
                                  if filename.endswith('.jar'))


def call_mallet(cmd, classpath=None, stdin=None, stdout=None, stderr=None,
                blocking=True):
    """
    Call `nltk.internals.java` with the given command, and with the classpath
    modified to include both ``nltk.jar`` and all the ``.jar`` files defined by
    Mallet.

    See `nltk.internals.java` for parameter and return value descriptions.
--
    ints, but can be any immutable type.  The set of labels
    that the classifier chooses from must be fixed and finite.

    Subclasses must define:
      - ``labels()``
      - either ``classify()`` or ``batch_classify()`` (or both)

    Subclasses may define:
      - either ``prob_classify()`` or ``batch_prob_classify()`` (or both)
    """
    def labels(self):
        """
        :return: the list of category labels used by this classifier.
        :rtype: list of (immutable)
        """
        raise NotImplementedError()

    def classify(self, featureset):
        """
        :return: the most appropriate label for the given featureset.
        :rtype: label
--
        else:
            raise NotImplementedError()

    def prob_classify(self, featureset):
        """
        :return: a probability distribution over labels for the given
            featureset.
--
        else:
            raise NotImplementedError()

    def batch_classify(self, featuresets):
        """
        Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:

--
        """
        return [self.classify(fs) for fs in featuresets]

    def batch_prob_classify(self, featuresets):
        """
        Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:

--
    or ints, but can be any immutable type.  The set of labels
    that the multi-classifier chooses from must be fixed and finite.

    Subclasses must define:
      - ``labels()``
      - either ``classify()`` or ``batch_classify()`` (or both)

    Subclasses may define:
      - either ``prob_classify()`` or ``batch_prob_classify()`` (or both)
    """
    def labels(self):
        """
        :return: the list of category labels used by this classifier.
        :rtype: list of (immutable)
        """
        raise NotImplementedError()

    def classify(self, featureset):
        """
        :return: the most appropriate set of labels for the given featureset.
        :rtype: set(label)
--
        else:
            raise NotImplementedError()

    def prob_classify(self, featureset):
        """
        :return: a probability distribution over sets of labels for the
            given featureset.
--
        else:
            raise NotImplementedError()

    def batch_classify(self, featuresets):
        """
        Apply ``self.classify()`` to each element of ``featuresets``.  I.e.:

--
        """
        return [self.classify(fs) for fs in featuresets]

    def batch_prob_classify(self, featuresets):
        """
        Apply ``self.prob_classify()`` to each element of ``featuresets``.  I.e.:

--
#     of labels that the classifier chooses from must be fixed and
#     finite.
#     """
#     def labels(self):
#         """
#         :return: the list of category labels used by this classifier.
#         :rtype: list of (immutable)
#         """
#         raise NotImplementedError()

#     def prob_classify(self, featureset):
#         """
#         Return a probability distribution over labels for the given
#         featureset.
--
#         """
#         raise NotImplementedError()

#     def classify(self, featureset):
#         """
#         Return the most appropriate label for the given featureset.

--
import nltk
from .util import accuracy

def ne(token):
    """
    This just assumes that words in all caps or titles are
    named entities.
--
        return True
    return False

def lemmatize(word):
    """
    Use morphy from WordNet to find the base form of verbs.
    """
--
    This builds a bag of words for both the text and the hypothesis after
    throwing away some stopwords, then calculates overlap and difference.
    """
    def __init__(self, rtepair, stop=True, lemmatize=False):
        """
        :param rtepair: a ``RTEPair`` from which features should be extracted
        :param stop: if ``True``, stopwords are thrown away.
--
        self._txt_extra = self.text_words - self.hyp_words


    def overlap(self, toktype, debug=False):
        """
        Compute the overlap between text and hypothesis.

--
        else:
            raise ValueError("Type not recognized:'%s'" % toktype)

    def hyp_extra(self, toktype, debug=True):
        """
        Compute the extraneous material in the hypothesis.

--
            raise ValueError("Type not recognized: '%s'" % toktype)


def rte_features(rtepair):
    extractor = RTEFeatureExtractor(rtepair)
    features = {}
    features['alwayson'] = True
--
    return features


def rte_classifier(trainer, features=rte_features):
    """
    Classify RTEPairs
    """
--
    return classifier


def demo_features():
    pairs = nltk.corpus.rte.pairs(['rte1_dev.xml'])[:6]
    for pair in pairs:
        print()
--
            print("%-15s => %s" % (key, rte_features(pair)[key]))


def demo_feature_extractor():
    rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]
    extractor = RTEFeatureExtractor(rtepair)
    print(extractor.hyp_words)
--
    print(extractor.hyp_extra('word'))


def demo():
    import nltk
    try:
        nltk.config_megam('/usr/local/bin/megam')
--
    # Configuration
    #/////////////////////////////////////////////////////////////////

    #: Default configuration values for the frame.
    FRAME_CONFIG = dict(background='#888',
                        takefocus=True,
                        highlightthickness=1)

    #: Default configurations for the column labels.
    LABEL_CONFIG = dict(borderwidth=1, relief='raised',
                        font='helvetica -16 bold',
                      background='#444', foreground='white')

    #: Default configuration for the column listboxes.
    LISTBOX_CONFIG = dict(borderwidth=1,
                          selectborderwidth=0,
                          highlightthickness=0,
--
    # Constructor
    #/////////////////////////////////////////////////////////////////

    def __init__(self, master, columns, column_weights=None, cnf={}, **kw):
        """
        Construct a new multi-column listbox widget.

--
        self._listboxes = []
        self._labels = []

        # Pick a default value for column_weights, if none was specified.
        if column_weights is None:
            column_weights = [1] * len(columns)
        elif len(column_weights) != len(columns):
--
            lb.bind('<Button-2>', lambda e: self.scan_mark(e.x, e.y))
            lb.bind('<B2-Motion>', lambda e: self.scan_dragto(e.x, e.y))
            # Dragging outside the window has no effect (diable
            # the default listbox behavior, which scrolls):
            lb.bind('<B1-Leave>', lambda e: 'break')
            # Columns can be resized by dragging them:
            l.bind('<Button-1>', self._resize_column)
--
    # Column Resizing
    #/////////////////////////////////////////////////////////////////

    def _resize_column(self, event):
        """
        Callback used to resize a column of the table.  Return ``True``
        if the column is actually getting resized (if the user clicked
--
        else:
            return False

    def _resize_column_motion_cb(self, event):
        lb = self._listboxes[self._resize_column_index]
        charwidth = lb.winfo_width() / float(lb['width'])

--

        lb['width'] = max(3, lb['width'] + int((x1-x2)/charwidth))

    def _resize_column_buttonrelease_cb(self, event):
        event.widget.unbind('<ButtonRelease-%d>' % event.num)
        event.widget.unbind('<Motion>')

--
    #/////////////////////////////////////////////////////////////////

    @property
    def column_names(self):
        """
        A tuple containing the names of the columns used by this
        multi-column listbox.
--
        return self._column_names

    @property
    def column_labels(self):
        """
        A tuple containing the ``Tkinter.Label`` widgets used to
        display the label of each column.  If this multi-column
--
        tuple.  These widgets will all be augmented with a
        ``column_index`` attribute, which can be used to determine
        which column they correspond to.  This can be convenient,
        e.g., when defining callbacks for bound events.
        """
        return tuple(self._labels)

    @property
    def listboxes(self):
        """
        A tuple containing the ``Tkinter.Listbox`` widgets used to
        display individual columns.  These widgets will all be
        augmented with a ``column_index`` attribute, which can be used
        to determine which column they correspond to.  This can be
        convenient, e.g., when defining callbacks for bound events.
        """
        return tuple(self._listboxes)

--
    # Mouse & Keyboard Callback Functions
    #/////////////////////////////////////////////////////////////////

    def _select(self, e):
        i = e.widget.nearest(e.y)
        self.selection_clear(0, 'end')
        self.selection_set(i)
        self.activate(i)
        self.focus()

    def _scroll(self, delta):
        for lb in self._listboxes:
            lb.yview_scroll(delta, 'unit')
        return 'break'

    def _pagesize(self):
        """:return: The number of rows that makes up one page"""
        return int(self.index('@0,1000000')) - int(self.index('@0,0'))

--
    # Row selection
    #/////////////////////////////////////////////////////////////////

    def select(self, index=None, delta=None, see=True):
        """
        Set the selected row.  If ``index`` is specified, then select
        row ``index``.  Otherwise, if ``delta`` is specified, then move
--
    # Configuration
    #/////////////////////////////////////////////////////////////////

    def configure(self, cnf={}, **kw):
        """
        Configure this widget.  Use ``label_*`` to configure all
        labels; and ``listbox_*`` to configure all listboxes.  E.g.:
--
            else:
                Frame.configure(self, {key:val})

    def __setitem__(self, key, val):
        """
        Configure this widget.  This is equivalent to
        ``self.configure({key,val``)}.  See ``configure()``.
        """
        self.configure({key:val})

    def rowconfigure(self, row_index, cnf={}, **kw):
        """
        Configure all table cells in the given row.  Valid keyword
        arguments are: ``background``, ``bg``, ``foreground``, ``fg``,
--
        """
        for lb in self._listboxes: lb.itemconfigure(row_index, cnf, **kw)

    def columnconfigure(self, col_index, cnf={}, **kw):
        """
        Configure all table cells in the given column.  Valid keyword
        arguments are: ``background``, ``bg``, ``foreground``, ``fg``,
--
            else:
                lb.configure({key:val})

    def itemconfigure(self, row_index, col_index, cnf=None, **kw):
        """
        Configure the table cell at the given row and column.  Valid
        keyword arguments are: ``background``, ``bg``, ``foreground``,
--
    # Value Access
    #/////////////////////////////////////////////////////////////////

    def insert(self, index, *rows):
        """
        Insert the given row or rows into the table, at the given
        index.  Each row value should be a tuple of cell values, one
--
        for (lb,elts) in zip(self._listboxes, list(zip(*rows))):
            lb.insert(index, *elts)

    def get(self, first, last=None):
        """
        Return the value(s) of the specified row(s).  If ``last`` is
        not specified, then return a single row value; otherwise,
--
        else:
            return tuple(values)

    def bbox(self, row, col):
        """
        Return the bounding box for the given table cell, relative to
        this widget's top-left corner.  The bounding box is a tuple
--
    # Hide/Show Columns
    #/////////////////////////////////////////////////////////////////

    def hide_column(self, col_index):
        """
        Hide the given column.  The column's state is still
        maintained: its values will still be returned by ``get()``, and
--
        self.listboxes[col_index].grid_forget()
        self.grid_columnconfigure(col_index, weight=0)

    def show_column(self, col_index):
        """
        Display a column that has been hidden using ``hide_column()``.
        It is safe to call this on a column that is not hidden.
--
    # Binding Methods
    #/////////////////////////////////////////////////////////////////

    def bind_to_labels(self, sequence=None, func=None, add=None):
        """
        Add a binding to each ``Tkinter.Label`` widget in this
        mult-column listbox that will call ``func`` in response to the
--
        return [label.bind(sequence, func, add)
                for label in self.column_labels]

    def bind_to_listboxes(self, sequence=None, func=None, add=None):
        """
        Add a binding to each ``Tkinter.Listbox`` widget in this
        mult-column listbox that will call ``func`` in response to the
--
        for listbox in self.listboxes:
            listbox.bind(sequence, func, add)

    def bind_to_columns(self, sequence=None, func=None, add=None):
        """
        Add a binding to each ``Tkinter.Label`` and ``Tkinter.Listbox``
        widget in this mult-column listbox that will call ``func`` in
--
    #/////////////////////////////////////////////////////////////////

    # These methods delegate to the first listbox:
    def curselection(self, *args, **kwargs):
        return self._listboxes[0].curselection(*args, **kwargs)
    def selection_includes(self, *args, **kwargs):
        return self._listboxes[0].selection_includes(*args, **kwargs)
    def itemcget(self, *args, **kwargs):
        return self._listboxes[0].itemcget(*args, **kwargs)
    def size(self, *args, **kwargs):
        return self._listboxes[0].size(*args, **kwargs)
    def index(self, *args, **kwargs):
        return self._listboxes[0].index(*args, **kwargs)
    def nearest(self, *args, **kwargs):
        return self._listboxes[0].nearest(*args, **kwargs)

    # These methods delegate to each listbox (and return None):
    def activate(self, *args, **kwargs):
        for lb in self._listboxes: lb.activate(*args, **kwargs)
    def delete(self, *args, **kwargs):
        for lb in self._listboxes: lb.delete(*args, **kwargs)
    def scan_mark(self, *args, **kwargs):
        for lb in self._listboxes: lb.scan_mark(*args, **kwargs)
    def scan_dragto(self, *args, **kwargs):
        for lb in self._listboxes: lb.scan_dragto(*args, **kwargs)
    def see(self, *args, **kwargs):
        for lb in self._listboxes: lb.see(*args, **kwargs)
    def selection_anchor(self, *args, **kwargs):
        for lb in self._listboxes: lb.selection_anchor(*args, **kwargs)
    def selection_clear(self, *args, **kwargs):
        for lb in self._listboxes: lb.selection_clear(*args, **kwargs)
    def selection_set(self, *args, **kwargs):
        for lb in self._listboxes: lb.selection_set(*args, **kwargs)
    def yview(self, *args, **kwargs):
        for lb in self._listboxes: v = lb.yview(*args, **kwargs)
        return v # if called with no arguments
    def yview_moveto(self, *args, **kwargs):
        for lb in self._listboxes: lb.yview_moveto(*args, **kwargs)
    def yview_scroll(self, *args, **kwargs):
        for lb in self._listboxes: lb.yview_scroll(*args, **kwargs)

    #/////////////////////////////////////////////////////////////////
--
    select_set = selection_set

    #/////////////////////////////////////////////////////////////////
    # These listbox methods are not defined for multi-listbox
    #/////////////////////////////////////////////////////////////////
    # def xview(self, *what): pass
    # def xview_moveto(self, fraction): pass
    # def xview_scroll(self, number, what): pass

######################################################################
# Table
--
    applied to the new row.

    Note: Although ``Table`` acts like a widget in some ways (e.g., it
    defines ``grid()``, ``pack()``, and ``bind()``), it is not itself a
    widget; it just contains one.  This is because widgets need to
    define ``__getitem__()``, ``__setitem__()``, and ``__nonzero__()`` in
    a way that's incompatible with the fact that ``Table`` behaves as a
    list-of-lists.

--
        table.  Each element of _rows is a row value, i.e., a list of
        cell values, one for each column in the row.
    """
    def __init__(self, master, column_names, rows=None,
                 column_weights=None,
                 scrollbar=True, click_to_sort=True,
                 reprfunc=None, cnf={}, **kw):
--
    #/////////////////////////////////////////////////////////////////
    # These all just delegate to either our frame or our MLB.

    def pack(self, *args, **kwargs):
        """Position this table's main frame widget in its parent
        widget.  See ``Tkinter.Frame.pack()`` for more info."""
        self._frame.pack(*args, **kwargs)

    def grid(self, *args, **kwargs):
        """Position this table's main frame widget in its parent
        widget.  See ``Tkinter.Frame.grid()`` for more info."""
        self._frame.grid(*args, **kwargs)

    def focus(self):
        """Direct (keyboard) input foxus to this widget."""
        self._mlb.focus()

    def bind(self, sequence=None, func=None, add=None):
        """Add a binding to this table's main frame that will call
        ``func`` in response to the event sequence."""
        self._mlb.bind(sequence, func, add)

    def rowconfigure(self, row_index, cnf={}, **kw):
        """:see: ``MultiListbox.rowconfigure()``"""
        self._mlb.rowconfigure(row_index, cnf, **kw)

    def columnconfigure(self, col_index, cnf={}, **kw):
        """:see: ``MultiListbox.columnconfigure()``"""
        col_index = self.column_index(col_index)
        self._mlb.columnconfigure(col_index, cnf, **kw)

    def itemconfigure(self, row_index, col_index, cnf=None, **kw):
        """:see: ``MultiListbox.itemconfigure()``"""
        col_index = self.column_index(col_index)
        return self._mlb.itemconfigure(row_index, col_index, cnf, **kw)

    def bind_to_labels(self, sequence=None, func=None, add=None):
        """:see: ``MultiListbox.bind_to_labels()``"""
        return self._mlb.bind_to_labels(sequence, func, add)

    def bind_to_listboxes(self, sequence=None, func=None, add=None):
        """:see: ``MultiListbox.bind_to_listboxes()``"""
        return self._mlb.bind_to_listboxes(sequence, func, add)

    def bind_to_columns(self, sequence=None, func=None, add=None):
        """:see: ``MultiListbox.bind_to_columns()``"""
        return self._mlb.bind_to_columns(sequence, func, add)

--
    #{ Table as list-of-lists
    #/////////////////////////////////////////////////////////////////

    def insert(self, row_index, rowvalue):
        """
        Insert a new row into the table, so that its row index will be
        ``row_index``.  If the table contains any rows whose row index
--
        self._mlb.insert(row_index, rowvalue)
        if self._DEBUG: self._check_table_vs_mlb()

    def extend(self, rowvalues):
        """
        Add new rows at the end of the table.

--
        for rowvalue in rowvalues: self.append(rowvalue)
        if self._DEBUG: self._check_table_vs_mlb()

    def append(self, rowvalue):
        """
        Add a new row to the end of the table.

--
        self.insert(len(self._rows), rowvalue)
        if self._DEBUG: self._check_table_vs_mlb()

    def clear(self):
        """
        Delete all rows in this table.
        """
--
        self._mlb.delete(0, 'end')
        if self._DEBUG: self._check_table_vs_mlb()

    def __getitem__(self, index):
        """
        Return the value of a row or a cell in this table.  If
        ``index`` is an integer, then the row value for the ``index``th
--
        else:
            return tuple(self._rows[index])

    def __setitem__(self, index, val):
        """
        Replace the value of a row or a cell in this table with
        ``val``.
--
            self._mlb.delete(index+1)
            self._restore_config_info(config_cookie)

    def __delitem__(self, row_index):
        """
        Delete the ``row_index``th row from this table.
        """
--
        self._mlb.delete(row_index)
        if self._DEBUG: self._check_table_vs_mlb()

    def __len__(self):
        """
        :return: the number of rows in this table.
        """
        return len(self._rows)

    def _checkrow(self, rowvalue):
        """
        Helper function: check that a given row value has the correct
        number of elements; and if not, raise an exception.
--
    #/////////////////////////////////////////////////////////////////

    @property
    def column_names(self):
        """A list of the names of the columns in this table."""
        return self._mlb.column_names

    def column_index(self, i):
        """
        If ``i`` is a valid column index integer, then return it as is.
        Otherwise, check if ``i`` is used as the name for any column;
--
            # This raises a key error if the column is not found.
            return self._column_name_to_index[i]

    def hide_column(self, column_index):
        """:see: ``MultiListbox.hide_column()``"""
        self._mlb.hide_column(self.column_index(column_index))

    def show_column(self, column_index):
        """:see: ``MultiListbox.show_column()``"""
        self._mlb.show_column(self.column_index(column_index))

--
    # Selection
    #/////////////////////////////////////////////////////////////////

    def selected_row(self):
        """
        Return the index of the currently selected row, or None if
        no row is selected.  To get the row value itself, use
--
        if sel: return int(sel[0])
        else: return None

    def select(self, index=None, delta=None, see=True):
        """:see: ``MultiListbox.select()``"""
        self._mlb.select(index, delta, see)

--
    # Sorting
    #/////////////////////////////////////////////////////////////////

    def sort_by(self, column_index, order='toggle'):
        """
        Sort the rows in this table, using the specified column's
        values as a sort key.
--
        self._restore_config_info(config_cookie, index_by_id=True, see=True)
        if self._DEBUG: self._check_table_vs_mlb()

    def _sort(self, event):
        """Event handler for clicking on a column label -- sort by
        that column."""
        column_index = event.widget.column_index
--
    #{ Table Drawing Helpers
    #/////////////////////////////////////////////////////////////////

    def _fill_table(self, save_config=True):
        """
        Re-draw the table from scratch, by clearing out the table's
        multi-column listbox; and then filling it in with values from
--
                row = [self._reprfunc(i,j,v) for (j,v) in enumerate(row)]
            self._mlb.insert('end', row)

    def _get_itemconfig(self, r, c):
        return dict( (k, self._mlb.itemconfig(r, c, k)[-1])
                     for k in ('foreground', 'selectforeground',
                               'background', 'selectbackground') )

    def _save_config_info(self, row_indices=None, index_by_id=False):
        """
        Return a 'cookie' containing information about which row is
        selected, and what color configurations have been applied.
--
        ``index_by_id=False``, then it is assumed that all rows will be
        in the same order when ``_restore_config_info()`` is called.
        """
        # Default value for row_indices is all rows.
        if row_indices is None:
            row_indices = list(range(len(self._rows)))

--

        return selection, config

    def _restore_config_info(self, cookie, index_by_id=False, see=False):
        """
        Restore selection & color configuration information that was
        saved using ``_save_config_info``.
--
    """If true, then run ``_check_table_vs_mlb()`` after any operation
       that modifies the table."""

    def _check_table_vs_mlb(self):
        """
        Verify that the contents of the table's ``_rows`` variable match
        the contents of its multi-listbox (``_mlb``).  This is just
--
######################################################################

# update this to use new WordNet API
def demo():
    root = Tk()
    root.bind('<Control-q>', lambda e: root.destroy())

--
            hyper = (synset.hypernyms()+[''])[0]
            hypo = (synset.hyponyms()+[''])[0]
            table.append([word,
                          getattr(synset, 'definition', '*none*'),
                          getattr(hyper, 'definition', '*none*'),
                          getattr(hypo, 'definition', '*none*')])

    table.columnconfig('Word', background='#afa')
    table.columnconfig('Synset', background='#efe')
--
    from .table import Table

# skip doctests from this package
def setup_module(module):
    from nose import SkipTest
    raise SkipTest("nltk.draw examples are not doctests")
--
A utility for displaying lexical dispersion.
"""

def dispersion_plot(text, words, ignore_case=False):
    """
    Generate a lexical dispersion plot.

--

"""
Tools for graphically displaying and interacting with the objects and
processing classes defined by the Toolkit.  These tools are primarily
intended to help students visualize the objects that they create.

The graphical tools are typically built using "canvas widgets", each
of which encapsulates the graphical elements and bindings used to
display a complex object on a Tkinter ``Canvas``.  For example, NLTK
defines canvas widgets for displaying trees and directed graphs, as
well as a number of simpler widgets.  These canvas widgets make it
easier to build new graphical tools and demos.  See the class
documentation for ``CanvasWidget`` for more information.

The ``nltk.draw`` module defines the abstract ``CanvasWidget`` base
class, and a number of simple canvas widgets.  The remaining canvas
widgets are defined by submodules, such as ``nltk.draw.tree``.

The ``nltk.draw`` module also defines ``CanvasFrame``, which
encapsulates a ``Canvas`` and its scrollbars.  It uses a
``ScrollWatcherWidget`` to ensure that all canvas widgets contained on
its canvas are within the scroll region.
--
    Each canvas widget can support a variety of "attributes", which
    control how the canvas widget is displayed.  Some typical examples
    attributes are ``color``, ``font``, and ``radius``.  Each attribute
    has a default value.  This default value can be overridden in the
    constructor, using keyword arguments of the form
    ``attribute=value``:

--
    Interaction
    ===========
    The attribute ``'draggable'`` controls whether the user can drag a
    canvas widget around the canvas.  By default, canvas widgets
    are not draggable.

    ``CanvasWidget`` provides callback support for two types of user
--
      - ``_update``: Update this canvas widget in response to a
        change in a single child.

    For a ``CanvasWidget`` with no child widgets, the default
    definitions for ``_manage`` and ``_update`` may be used.

    If a subclass defines any attributes, then it should implement
    ``__getitem__`` and ``__setitem__``.  If either of these methods is
    called with an unknown attribute, then they should propagate the
    request to ``CanvasWidget``.
--
        argument, which is the ``CanvasWidget`` that triggered the
        callback.
    """
    def __init__(self, canvas, parent=None, **attribs):
        """
        Create a new canvas widget.  This constructor should only be
        called by subclass constructors; and it should be called only
--
    ##  Inherited methods.
    ##//////////////////////////////////////////////////////

    def bbox(self):
        """
        :return: A bounding box for this ``CanvasWidget``. The bounding
            box is a tuple of four coordinates, *(xmin, ymin, xmax, ymax)*,
--
        if len(self.tags()) == 0: raise ValueError('No tags')
        return self.__canvas.bbox(*self.tags())

    def width(self):
        """
        :return: The width of this canvas widget's bounding box, in
            its ``Canvas``'s coordinate space.
--
        bbox = self.__canvas.bbox(*self.tags())
        return bbox[2]-bbox[0]

    def height(self):
        """
        :return: The height of this canvas widget's bounding box, in
            its ``Canvas``'s coordinate space.
--
        bbox = self.__canvas.bbox(*self.tags())
        return bbox[3]-bbox[1]

    def parent(self):
        """
        :return: The hierarchical parent of this canvas widget.
            ``self`` is considered a subpart of its parent for
--
        """
        return self.__parent

    def child_widgets(self):
        """
        :return: A list of the hierarchical children of this canvas
            widget.  These children are considered part of ``self``
--
        """
        return self.__children

    def canvas(self):
        """
        :return: The canvas that this canvas widget is bound to.
        :rtype: Tkinter.Canvas
        """
        return self.__canvas

    def move(self, dx, dy):
        """
        Move this canvas widget by a given distance.  In particular,
        shift the canvas widget right by ``dx`` pixels, and down by
--
            self.__canvas.move(tag, dx, dy)
        if self.__parent: self.__parent.update(self)

    def moveto(self, x, y, anchor='NW'):
        """
        Move this canvas widget to the given location.  In particular,
        shift the canvas widget such that the corner or side of the
--
        if anchor == 'SW': self.move(x-x1,        y-y2)
        if anchor == 'W':  self.move(x-x1,        y-y1/2-y2/2)

    def destroy(self):
        """
        Remove this ``CanvasWidget`` from its ``Canvas``.  After a
        ``CanvasWidget`` has been destroyed, it should not be accessed.
--
        self.__canvas.delete(*self.tags())
        self.__canvas = None

    def update(self, child):
        """
        Update the graphical display of this canvas widget, and all of
        its ancestors, in response to a change in one of this canvas
--
        # We're done updating.
        self.__updating = 0

    def manage(self):
        """
        Arrange this canvas widget and all of its descendants.

--
        for child in self.__children: child.manage()
        self._manage()

    def tags(self):
        """
        :return: a list of the canvas tags for all graphical
            elements managed by this canvas widget, including
--
            tags += child.tags()
        return tags

    def __setitem__(self, attr, value):
        """
        Set the value of the attribute ``attr`` to ``value``.  See the
        class documentation for a list of attributes supported by this
--
        else:
            raise ValueError('Unknown attribute %r' % attr)

    def __getitem__(self, attr):
        """
        :return: the value of the attribute ``attr``.  See the class
            documentation for a list of attributes supported by this
--
        else:
            raise ValueError('Unknown attribute %r' % attr)

    def __repr__(self):
        """
        :return: a string representation of this canvas widget.
        :rtype: str
        """
        return '<%s>' % self.__class__.__name__

    def hide(self):
        """
        Temporarily hide this canvas widget.

--
        for tag in self.tags():
            self.__canvas.itemconfig(tag, state='hidden')

    def show(self):
        """
        Show a hidden canvas widget.

--
        for tag in self.tags():
            self.__canvas.itemconfig(tag, state='normal')

    def hidden(self):
        """
        :return: True if this canvas widget is hidden.
        :rtype: bool
--
    ##  Callback interface
    ##//////////////////////////////////////////////////////

    def bind_click(self, callback, button=1):
        """
        Register a new callback that will be called whenever this
        ``CanvasWidget`` is clicked on.
--
        """
        self.__callbacks[button] = callback

    def bind_drag(self, callback):
        """
        Register a new callback that will be called after this
        ``CanvasWidget`` is dragged.  This implicitly makes this
--
        self.__draggable = 1
        self.__callbacks['drag'] = callback

    def unbind_click(self, button=1):
        """
        Remove a callback that was registered with ``bind_click``.

--
        try: del self.__callbacks[button]
        except: pass

    def unbind_drag(self):
        """
        Remove a callback that was registered with ``bind_drag``.
        """
--
    ##  Callback internals
    ##//////////////////////////////////////////////////////

    def __press_cb(self, event):
        """
        Handle a button-press event:
          - record the button press event in ``self.__press``
--
        self.__canvas.bind('<ButtonRelease-%d>' % event.num,
                          self.__release_cb)

    def __start_drag(self, event):
        """
        Begin dragging this object:
          - register a motion callback
--
        self.__drag_x = event.x
        self.__drag_y = event.y

    def __motion_cb(self, event):
        """
        Handle a motion event:
          - move this object to the new location
--
        self.__drag_x = event.x
        self.__drag_y = event.y

    def __release_cb(self, event):
        """
        Handle a release callback:
          - unregister motion & button release callbacks.
--

        self.__press = None

    def __drag(self):
        """
        If this ``CanvasWidget`` has a drag callback, then call it;
        otherwise, find the closest ancestor with a drag callback, and
--
        elif self.__parent is not None:
            self.__parent.__drag()

    def __click(self, button):
        """
        If this ``CanvasWidget`` has a drag callback, then call it;
        otherwise, find the closest ancestor with a click callback, and
--
    ##  Child/parent Handling
    ##//////////////////////////////////////////////////////

    def _add_child_widget(self, child):
        """
        Register a hierarchical child widget.  The child will be
        considered part of this canvas widget for purposes of user
--
        child.__parent = self
        self.__children.append(child)

    def _remove_child_widget(self, child):
        """
        Remove a hierarchical child widget.  This child will no longer
        be considered part of this canvas widget for purposes of user
--
        child.__parent = None

    ##//////////////////////////////////////////////////////
    ##  Defined by subclass
    ##//////////////////////////////////////////////////////

    def _tags(self):
        """
        :return: a list of canvas tags for all graphical elements
            managed by this canvas widget, not including graphical
--
        """
        raise NotImplementedError()

    def _manage(self):
        """
        Arrange the child widgets of this canvas widget.  This method
        is called when the canvas widget is initially created.  It is
--
        """
        pass

    def _update(self, child):
        """
        Update this canvas widget in response to a change in one of
        its children.
--
        this width, it will be line-wrapped at whitespace.
      - ``draggable``: whether the text can be dragged by the user.
    """
    def __init__(self, canvas, text, **attribs):
        """
        Create a new text widget.

--
        self._tag = canvas.create_text(1, 1, text=text)
        CanvasWidget.__init__(self, canvas, **attribs)

    def __setitem__(self, attr, value):
        if attr in ('color', 'font', 'justify', 'width'):
            if attr == 'color': attr = 'fill'
            self.canvas().itemconfig(self._tag, {attr:value})
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'width':
            return int(self.canvas().itemcget(self._tag, attr))
        elif attr in ('color', 'font', 'justify'):
--
        else:
            return CanvasWidget.__getitem__(self, attr)

    def _tags(self): return [self._tag]

    def text(self):
        """
        :return: The text displayed by this text widget.
        :rtype: str
        """
        return self.canvas().itemcget(self._tag, 'TEXT')

    def set_text(self, text):
        """
        Change the text that is displayed by this text widget.

--
        if self.parent() is not None:
            self.parent().update(self)

    def __repr__(self):
        return '[Text: %r]' % self._text

class SymbolWidget(TextWidget):
    """
    A canvas widget that displays special symbols, such as the
    negation sign and the exists operator.  Symbols are specified by
    name.  Currently, the following symbol names are defined: ``neg``,
    ``disj``, ``conj``, ``lambda``, ``merge``, ``forall``, ``exists``,
    ``subseteq``, ``subset``, ``notsubset``, ``emptyset``, ``imp``,
    ``rightarrow``, ``equal``, ``notequal``, ``epsilon``.
--
               'epsilon': 'e',
               }

    def __init__(self, canvas, symbol, **attribs):
        """
        Create a new symbol widget.

--
        TextWidget.__init__(self, canvas, '', **attribs)
        self.set_symbol(symbol)

    def symbol(self):
        """
        :return: the name of the symbol that is displayed by this
            symbol widget.
--
        """
        return self._symbol

    def set_symbol(self, symbol):
        """
        Change the symbol that is displayed by this symbol widget.

--
        self._symbol = symbol
        self.set_text(SymbolWidget.SYMBOLS[symbol])

    def __repr__(self):
        return '[Symbol: %r]' % self._symbol

    @staticmethod
    def symbolsheet(size=20):
        """
        Open a new Tkinter window that displays the entire alphabet
        for the symbol font.  This is useful for constructing the
        ``SymbolWidget.SYMBOLS`` dictionary.
        """
        top = Tk()
        def destroy(e, top=top): top.destroy()
        top.bind('q', destroy)
        Button(top, text='Quit', command=top.destroy).pack(side='bottom')
        text = Text(top, font=('helvetica', -size), width=20, height=30)
--
class AbstractContainerWidget(CanvasWidget):
    """
    An abstract class for canvas widgets that contain a single child,
    such as ``BoxWidget`` and ``OvalWidget``.  Subclasses must define
    a constructor, which should create any new graphical elements and
    then call the ``AbstractCanvasContainer`` constructor.  Subclasses
    must also define the ``_update`` method and the ``_tags`` method;
    and any subclasses that define attributes should define
    ``__setitem__`` and ``__getitem__``.
    """
    def __init__(self, canvas, child, **attribs):
        """
        Create a new container widget.  This constructor should only
        be called by subclass constructors.
--
        self._add_child_widget(child)
        CanvasWidget.__init__(self, canvas, **attribs)

    def _manage(self):
        self._update(self._child)

    def child(self):
        """
        :return: The child widget contained by this container widget.
        :rtype: CanvasWidget
        """
        return self._child

    def set_child(self, child):
        """
        Change the child widget contained by this container widget.

--
        self._child = child
        self.update(child)

    def __repr__(self):
        name = self.__class__.__name__
        if name[-6:] == 'Widget': name = name[:-6]
        return '[%s: %r]' % (name, self._child)
--
        and the box.
      - ``draggable``: whether the text can be dragged by the user.
    """
    def __init__(self, canvas, child, **attribs):
        """
        Create a new box widget.

--
        canvas.tag_lower(self._box)
        AbstractContainerWidget.__init__(self, canvas, child, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'margin': self._margin = value
        elif attr in ('outline', 'fill', 'width'):
            self.canvas().itemconfig(self._box, {attr:value})
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'margin': return self._margin
        elif attr == 'width':
            return float(self.canvas().itemcget(self._box, attr))
--
        else:
            return CanvasWidget.__getitem__(self, attr)

    def _update(self, child):
        (x1, y1, x2, y2) = child.bbox()
        margin = self._margin + self['width']/2
        self.canvas().coords(self._box, x1-margin, y1-margin,
                             x2+margin, y2+margin)

    def _tags(self): return [self._box]

class OvalWidget(AbstractContainerWidget):
    """
--
      - ``draggable``: whether the text can be dragged by the user.
      - ``double``: If true, then a double-oval is drawn.
    """
    def __init__(self, canvas, child, **attribs):
        """
        Create a new oval widget.

--
        canvas.tag_lower(self._oval)
        AbstractContainerWidget.__init__(self, canvas, child, **attribs)

    def __setitem__(self, attr, value):
        c = self.canvas()
        if attr == 'margin': self._margin = value
        elif attr == 'double':
--
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'margin': return self._margin
        elif attr == 'double': return self._double is not None
        elif attr == 'width':
--
    # The ratio between inscribed & circumscribed ovals
    RATIO = 1.4142135623730949

    def _update(self, child):
        R = OvalWidget.RATIO
        (x1, y1, x2, y2) = child.bbox()
        margin = self._margin
--
            self.canvas().coords(self._oval2, left-margin+2, top-margin+2,
                                 right+margin-2, bot+margin-2)

    def _tags(self):
        if self._oval2 is None:
            return [self._oval]
        else:
--
      - ``width``: The width of the parenthases.
      - ``draggable``: whether the text can be dragged by the user.
    """
    def __init__(self, canvas, child, **attribs):
        """
        Create a new parenthasis widget.

--
                                         start=-90, extent=180)
        AbstractContainerWidget.__init__(self, canvas, child, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'color':
            self.canvas().itemconfig(self._oparen, outline=value)
            self.canvas().itemconfig(self._cparen, outline=value)
--
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'color':
            return self.canvas().itemcget(self._oparen, 'outline')
        elif attr == 'width':
--
        else:
            return CanvasWidget.__getitem__(self, attr)

    def _update(self, child):
        (x1, y1, x2, y2) = child.bbox()
        width = max((y2-y1)/6, 4)
        self.canvas().coords(self._oparen, x1-width, y1, x1+width, y2)
        self.canvas().coords(self._cparen, x2-width, y1, x2+width, y2)

    def _tags(self): return [self._oparen, self._cparen]

class BracketWidget(AbstractContainerWidget):
    """
--
      - ``width``: The width of the brackets.
      - ``draggable``: whether the text can be dragged by the user.
    """
    def __init__(self, canvas, child, **attribs):
        """
        Create a new bracket widget.

--
        self._cbrack = canvas.create_line(1,1,1,1,1,1,1,1)
        AbstractContainerWidget.__init__(self, canvas, child, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'color':
            self.canvas().itemconfig(self._obrack, fill=value)
            self.canvas().itemconfig(self._cbrack, fill=value)
--
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'color':
            return self.canvas().itemcget(self._obrack, 'outline')
        elif attr == 'width':
--
        else:
            return CanvasWidget.__getitem__(self, attr)

    def _update(self, child):
        (x1, y1, x2, y2) = child.bbox()
        width = max((y2-y1)/8, 2)
        self.canvas().coords(self._obrack, x1, y1, x1-width, y1,
--
        self.canvas().coords(self._cbrack, x2, y1, x2+width, y1,
                             x2+width, y2, x2, y2)

    def _tags(self): return [self._obrack, self._cbrack]

class SequenceWidget(CanvasWidget):
    """
--
    Attributes:
      - ``align``: The vertical alignment of the children.  Possible
        values are ``'top'``, ``'center'``, and ``'bottom'``.  By
        default, children are center-aligned.
      - ``space``: The amount of horizontal space to place between
        children.  By default, one pixel of space is used.
      - ``ordered``: If true, then keep the children in their
        original order.
    """
    def __init__(self, canvas, *children, **attribs):
        """
        Create a new sequence widget.

--
        for child in children: self._add_child_widget(child)
        CanvasWidget.__init__(self, canvas, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'align':
            if value not in ('top', 'bottom', 'center'):
                raise ValueError('Bad alignment: %r' % value)
--
        elif attr == 'ordered': self._ordered = value
        else: CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'align': return value
        elif attr == 'space': return self._space
        elif attr == 'ordered': return self._ordered
        else: return CanvasWidget.__getitem__(self, attr)

    def _tags(self): return []

    def _yalign(self, top, bot):
        if self._align == 'top': return top
        if self._align == 'bottom': return bot
        if self._align == 'center': return (top+bot)/2

    def _update(self, child):
        # Align all children with child.
        (left, top, right, bot) = child.bbox()
        y = self._yalign(top, bot)
--
                    self._children[i].move(x-x2, 0)
                    x -= x2-x1 + self._space

    def _manage(self):
        if len(self._children) == 0: return
        child = self._children[0]

--
            self._children[i].move(x-x2, y-self._yalign(y1,y2))
            x -= x2-x1 + self._space

    def __repr__(self):
        return '[Sequence: ' + repr(self._children)[1:-1]+']'

    # Provide an alias for the child_widgets() member.
    children = CanvasWidget.child_widgets

    def replace_child(self, oldchild, newchild):
        """
        Replace the child canvas widget ``oldchild`` with ``newchild``.
        ``newchild`` must not have a parent.  ``oldchild``'s parent will
--
        self._add_child_widget(newchild)
        self.update(newchild)

    def remove_child(self, child):
        """
        Remove the given child canvas widget.  ``child``'s parent will
        be set ot None.
--
        if len(self._children) > 0:
            self.update(self._children[0])

    def insert_child(self, index, child):
        """
        Insert a child canvas widget before a given index.

--
    Attributes:
      - ``align``: The horizontal alignment of the children.  Possible
        values are ``'left'``, ``'center'``, and ``'right'``.  By
        default, children are center-aligned.
      - ``space``: The amount of vertical space to place between
        children.  By default, one pixel of space is used.
      - ``ordered``: If true, then keep the children in their
        original order.
    """
    def __init__(self, canvas, *children, **attribs):
        """
        Create a new stack widget.

--
        for child in children: self._add_child_widget(child)
        CanvasWidget.__init__(self, canvas, **attribs)

    def __setitem__(self, attr, value):
        if attr == 'align':
            if value not in ('left', 'right', 'center'):
                raise ValueError('Bad alignment: %r' % value)
--
        elif attr == 'ordered': self._ordered = value
        else: CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'align': return value
        elif attr == 'space': return self._space
        elif attr == 'ordered': return self._ordered
        else: return CanvasWidget.__getitem__(self, attr)

    def _tags(self): return []

    def _xalign(self, left, right):
        if self._align == 'left': return left
        if self._align == 'right': return right
        if self._align == 'center': return (left+right)/2

    def _update(self, child):
        # Align all children with child.
        (left, top, right, bot) = child.bbox()
        x = self._xalign(left, right)
--
                    self._children[i].move(0, y-y2)
                    y -= y2-y1 + self._space

    def _manage(self):
        if len(self._children) == 0: return
        child = self._children[0]

--
            self._children[i].move(x-self._xalign(x1,x2), y-y2)
            y -= y2-y1 + self._space

    def __repr__(self):
        return '[Stack: ' + repr(self._children)[1:-1]+']'

    # Provide an alias for the child_widgets() member.
    children = CanvasWidget.child_widgets

    def replace_child(self, oldchild, newchild):
        """
        Replace the child canvas widget ``oldchild`` with ``newchild``.
        ``newchild`` must not have a parent.  ``oldchild``'s parent will
--
        self._add_child_widget(newchild)
        self.update(newchild)

    def remove_child(self, child):
        """
        Remove the given child canvas widget.  ``child``'s parent will
        be set ot None.
--
        if len(self._children) > 0:
            self.update(self._children[0])

    def insert_child(self, index, child):
        """
        Insert a child canvas widget before a given index.

--
    height of zero; and if you wish to only create vertical space, use
    a width of zero.
    """
    def __init__(self, canvas, width, height, **attribs):
        """
        Create a new space widget.

--
        self._tag = canvas.create_line(1, 1, width, height, fill='')
        CanvasWidget.__init__(self, canvas, **attribs)

    # note: width() and height() are already defined by CanvasWidget.
    def set_width(self, width):
        """
        Change the width of this space widget.

--
        [x1, y1, x2, y2] = self.bbox()
        self.canvas().coords(self._tag, x1, y1, x1+width, y2)

    def set_height(self, height):
        """
        Change the height of this space widget.

--
        [x1, y1, x2, y2] = self.bbox()
        self.canvas().coords(self._tag, x1, y1, x2, y1+height)

    def _tags(self): return [self._tag]

    def __repr__(self): return '[Space]'

class ScrollWatcherWidget(CanvasWidget):
    """
--
    scroll-watcher widget will only increase the size of the
    ``Canvas``'s scrollregion; it will never decrease it.
    """
    def __init__(self, canvas, *children, **attribs):
        """
        Create a new scroll-watcher widget.

--
        for child in children: self._add_child_widget(child)
        CanvasWidget.__init__(self, canvas, **attribs)

    def add_child(self, canvaswidget):
        """
        Add a new canvas widget to the scroll-watcher.  The
        scroll-watcher will ensure that the new canvas widget is
--
        self._add_child_widget(canvaswidget)
        self.update(canvaswidget)

    def remove_child(self, canvaswidget):
        """
        Remove a canvas widget from the scroll-watcher.  The
        scroll-watcher will no longer ensure that the new canvas
--
        """
        self._remove_child_widget(canvaswidget)

    def _tags(self): return []

    def _update(self, child):
        self._adjust_scrollregion()

    def _adjust_scrollregion(self):
        """
        Adjust the scrollregion of this scroll-watcher's ``Canvas`` to
        include the bounding boxes of all of its children.
--
    its own main window, including a "Done" button and a "Print"
    button.
    """
    def __init__(self, parent=None, **kw):
        """
        Create a new ``CanvasFrame``.

--
            self.pack(expand=1, fill='both')
            self._init_menubar()

    def _init_menubar(self):
        menubar = Menu(self._parent)

        filemenu = Menu(menubar, tearoff=0)
--

        self._parent.config(menu=menubar)

    def print_to_file(self, filename=None):
        """
        Print the contents of this ``CanvasFrame`` to a postscript
        file.  If no filename is given, then prompt the user for one.
--
            ftypes = [('Postscript files', '.ps'),
                      ('All files', '*')]
            filename = asksaveasfilename(filetypes=ftypes,
                                         defaultextension='.ps')
            if not filename: return
        (x0, y0, w, h) = self.scrollregion()
        self._canvas.postscript(file=filename, x=x0, y=y0,
--
                                pageheight=h+2, # points = 1/72 inch
                                pagex=0, pagey=0)

    def scrollregion(self):
        """
        :return: The current scroll region for the canvas managed by
            this ``CanvasFrame``.
--
        (x1, y1, x2, y2) = self._canvas['scrollregion'].split()
        return (int(x1), int(y1), int(x2), int(y2))

    def canvas(self):
        """
        :return: The canvas managed by this ``CanvasFrame``.
        :rtype: Tkinter.Canvas
        """
        return self._canvas

    def add_widget(self, canvaswidget, x=None, y=None):
        """
        Register a canvas widget with this ``CanvasFrame``.  The
        ``CanvasFrame`` will ensure that this canvas widget is always
--
        # Register with scrollwatcher.
        self._scrollwatcher.add_child(canvaswidget)

    def _find_room(self, widget, desired_x, desired_y):
        """
        Try to find a space for a given widget.
        """
--
                    return (x,y)
        return (0,0)

    def destroy_widget(self, canvaswidget):
        """
        Remove a canvas widget from this ``CanvasFrame``.  This
        deregisters the canvas widget, and destroys it.
--
        self.remove_widget(canvaswidget)
        canvaswidget.destroy()

    def remove_widget(self, canvaswidget):
        # Deregister with scrollwatcher.
        self._scrollwatcher.remove_child(canvaswidget)

    def pack(self, cnf={}, **kw):
        """
        Pack this ``CanvasFrame``.  See the documentation for
        ``Tkinter.Pack`` for more information.
--
        self._frame.pack(cnf, **kw)
        # Adjust to be big enough for kids?

    def destroy(self, *e):
        """
        Destroy this ``CanvasFrame``.  If this ``CanvasFrame`` created a
        top-level window, then this will close that window.
--
        self._parent.destroy()
        self._parent = None

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this frame is created from a non-interactive program (e.g.
--
    typically used by graphical tools to display help text, or similar
    information.
    """
    def __init__(self, root, title, text, width=None, height=None,
                 **textbox_options):
        if width is None or height is None:
            (width, height) = self.find_dimentions(text, width, height)
--
        # Focus the scrollbar, so they can use up/down, etc.
        scrollbar.focus()

    def find_dimentions(self, text, width, height):
        lines = text.split('\n')
        if width is None:
            maxwidth = max(len(line) for line in lines)
--

        return (width, height)

    def destroy(self, *e):
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this window is created from a non-interactive program (e.g.
--
    """
    A dialog box for entering
    """
    def __init__(self, parent, original_text='', instructions='',
                 set_callback=None, title=None):
        self._parent = parent
        self._original_text = original_text
--
        b = Button(buttons, text='Cancel', command=self._cancel, width=8)
        b.pack(side='right', padx=5)
        b = Button(buttons, text='Ok', command=self._ok,
                   width=8, default='active')
        b.pack(side='left', padx=5)
        b = Button(buttons, text='Apply', command=self._apply, width=8)
        b.pack(side='left')
--

        self._entry.focus()

    def _reset(self, *e):
        self._entry.delete(0,'end')
        self._entry.insert(0, self._original_text)
        if self._set_callback:
            self._set_callback(self._original_text)

    def _cancel(self, *e):
        try: self._reset()
        except: pass
        self._destroy()

    def _ok(self, *e):
        self._apply()
        self._destroy()

    def _apply(self, *e):
        if self._set_callback:
            self._set_callback(self._entry.get())

    def _destroy(self, *e):
        if self._top is None: return
        self._top.destroy()
        self._top = None
--
class ColorizedList(object):
    """
    An abstract base class for displaying a colorized list of items.
    Subclasses should define:
      - ``_init_colortags``, which sets up Text color tags that
        will be used by the list.
      - ``_item_repr``, which returns a list of (text,colortag)
--
    :note: Typically, you will want to register a callback for
        ``'select'`` that calls ``mark`` on the given item.
    """
    def __init__(self, parent, items=[], **options):
        """
        Construct a new list.

--
    # Abstract methods
    #////////////////////////////////////////////////////////////

    def _init_colortags(self, textwidget, options):
        """
        Set up any colortags that will be used by this colorized list.
        E.g.:
--
        """
        raise NotImplementedError()

    def _item_repr(self, item):
        """
        Return a list of (text, colortag) tuples that make up the
        colorized representation of the item.  Colorized
--
    # Item Access
    #////////////////////////////////////////////////////////////

    def get(self, index=None):
        """
        :return: A list of the items contained by this list.
        """
--
        else:
            return self._items[index]

    def set(self, items):
        """
        Modify the list of items contained by this list.
        """
--
        # Clear all marks
        self._marks.clear()

    def unmark(self, item=None):
        """
        Remove highlighting from the given item; or from every item,
        if no item is given.
--
            (start, end) = ('%d.0' % (index+1), '%d.0' % (index+2))
            self._textwidget.tag_remove('highlight', start, end)

    def mark(self, item):
        """
        Highlight the given item.
        :raise ValueError: If ``item`` is not contained in the list.
--
        (start, end) = ('%d.0' % (index+1), '%d.0' % (index+2))
        self._textwidget.tag_add('highlight', start, end)

    def markonly(self, item):
        """
        Remove any current highlighting, and mark the given item.
        :raise ValueError: If ``item`` is not contained in the list.
--
        self.unmark()
        self.mark(item)

    def view(self, item):
        """
        Adjust the view such that the given item is visible.  If
        the item is already visible, then do nothing.
--
    # Callbacks
    #////////////////////////////////////////////////////////////

    def add_callback(self, event, func):
        """
        Register a callback function with the list.  This function
        will be called whenever the given event occurs.
--
        else: events = [event]

        for e in events:
            self._callbacks.setdefault(e,{})[func] = 1

    def remove_callback(self, event, func=None):
        """
        Deregister a callback function.  If ``func`` is none, then
        all callbacks are removed for the given event.
--
    # Tkinter Methods
    #////////////////////////////////////////////////////////////

    def pack(self, cnf={}, **kw):
#        "@include: Tkinter.Pack.pack"
        self._itemframe.pack(cnf, **kw)

    def grid(self, cnf={}, **kw):
#        "@include: Tkinter.Grid.grid"
        self._itemframe.grid(cnf, *kw)

    def focus(self):
#        "@include: Tkinter.Widget.focus"
        self._textwidget.focus()

--
    # Internal Methods
    #////////////////////////////////////////////////////////////

    def _init_itemframe(self, options):
        self._itemframe = Frame(self._parent)

        # Create the basic Text widget & scrollbar.
        options.setdefault('background', '#e0e0e0')
        self._textwidget = Text(self._itemframe, **options)
        self._textscroll = Scrollbar(self._itemframe, takefocus=0,
                                     orient='vertical')
--
                                    border='', underline=1)
        self._textwidget.tag_lower('highlight', 'sel')

    def _fire_callback(self, event, itemnum):
        if event not in self._callbacks: return
        if 0 <= itemnum < len(self._items):
            item = self._items[itemnum]
--
        for cb_func in list(self._callbacks[event].keys()):
            cb_func(item)

    def _buttonpress(self, event):
        clickloc = '@%d,%d' % (event.x,event.y)
        insert_point = self._textwidget.index(clickloc)
        itemnum = int(insert_point.split('.')[0])-1
        self._fire_callback('click%d' % event.num, itemnum)

    def _keypress(self, event):
        if event.keysym == 'Return' or event.keysym == 'space':
            insert_point = self._textwidget.index('insert')
            itemnum = int(insert_point.split('.')[0])-1
--
##//////////////////////////////////////////////////////

class MutableOptionMenu(Menubutton):
    def __init__(self, master, values, **options):
        self._callback = options.get('command')
        if 'command' in options: del options['command']

--

        self["menu"] = self._menu

    def add(self, value):
        if value in self._values: return
        def set(value=value): self.set(value)
        self._menu.add_command(label=value, command=set)
        self._values.append(value)

    def set(self, value):
        self._variable.set(value)
        if self._callback:
            self._callback(value)

    def remove(self, value):
        # Might raise indexerror: pass to parent.
        i = self._values.index(value)
        del self._values[i]
        self._menu.delete(i, i)

    def __getitem__(self, name):
        if name == 'menu':
            return self.__menu
        return Widget.__getitem__(self, name)

    def destroy(self):
        """Destroy this widget and the associated menu."""
        Menubutton.destroy(self)
        self._menu = None
--
##  Test code.
##//////////////////////////////////////////////////////

def demo():
    """
    A simple demonstration showing how to use canvas widgets.
    """
    def fill(cw):
        from random import randint
        cw['fill'] = '#00%04d' % randint(0,9999)
    def color(cw):
        from random import randint
        cw['color'] = '#ff%04d' % randint(0,9999)

--
    """
    A canvas widget that displays a single segment of a hierarchical
    tree.  Each ``TreeSegmentWidget`` connects a single "node widget"
    to a sequence of zero or more "subtree widgets".  By default, the
    bottom of the node is connected to the top of each subtree by a
    single line.  However, if the ``roof`` attribute is set, then a
    single triangular "roof" will connect the node to all of its
--
      - ``roof``: What sort of connection to draw between the node and
        its subtrees.  If ``roof`` is true, draw a single triangular
        "roof" over the subtrees.  If ``roof`` is false, draw a line
        between each subtree and the node.  Default value is false.
      - ``xspace``: The amount of horizontal space to leave between
        subtrees when managing this widget.  Default value is 10.
      - ``yspace``: The amount of space to place between the node and
        its children when managing this widget.  Default value is 15.
      - ``color``: The color of the lines connecting the node to its
        subtrees; and of the outline of the triangular roof.  Default
        value is ``'#006060'``.
      - ``fill``: The fill color for the triangular roof.  Default
        value is ``''`` (no fill).
      - ``width``: The width of the lines connecting the node to its
        subtrees; and of the outline of the triangular roof.  Default
        value is 1.
      - ``orientation``: Determines whether the tree branches downwards
        or rightwards.  Possible values are ``'horizontal'`` and
        ``'vertical'``.  The default value is ``'vertical'`` (i.e.,
        branch downwards).
      - ``draggable``: whether the widget can be dragged by the user.
    """
    def __init__(self, canvas, node, subtrees, **attribs):
        """
        :type node:
        :type subtrees: list(CanvasWidgetI)
--

        CanvasWidget.__init__(self, canvas, **attribs)

    def __setitem__(self, attr, value):
        canvas = self.canvas()
        if attr == 'roof':
            self._roof = value
--
        else:
            CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr == 'roof': return self._roof
        elif attr == 'width':
            return self.canvas().itemcget(self._polygon, attr)
--
        else:
            return CanvasWidget.__getitem__(self, attr)

    def node(self):
        return self._node

    def subtrees(self):
        return self._subtrees[:]

    def set_node(self, node):
        """
        Set the node to ``node``.
        """
--
        self._node = node
        self.update(self._node)

    def replace_child(self, oldchild, newchild):
        """
        Replace the child ``oldchild`` with ``newchild``.
        """
--
        self._add_child_widget(newchild)
        self.update(newchild)

    def remove_child(self, child):
        index = self._subtrees.index(child)
        del self._subtrees[index]
        self._remove_child_widget(child)
        self.canvas().delete(self._lines.pop())
        self.update(self._node)

    def insert_child(self, index, child):
        self._subtrees.insert(index, child)
        self._add_child_widget(child)
        self._lines.append(canvas.create_line(0,0,0,0, fill='#006060'))
--

    # but.. lines???

    def _tags(self):
        if self._roof:
            return [self._polygon]
        else:
            return self._lines

    def _subtree_top(self, child):
        if isinstance(child, TreeSegmentWidget):
            bbox = child.node().bbox()
        else:
--
        else:
            return ((bbox[0]+bbox[2])/2.0, bbox[1])

    def _node_bottom(self):
        bbox = self._node.bbox()
        if self._horizontal:
            return (bbox[2], (bbox[1]+bbox[3])/2.0)
        else:
            return ((bbox[0]+bbox[2])/2.0, bbox[3])

    def _update(self, child):
        if len(self._subtrees) == 0: return
        if self._node.bbox() is None: return # [XX] ???

--
            (subtreex, subtreey) = self._subtree_top(subtree)
            self.canvas().coords(line, nodex, nodey, subtreex, subtreey)

    def _maintain_order(self, child):
        if self._horizontal:
            return self._maintain_order_horizontal(child)
        else:
            return self._maintain_order_vertical(child)

    def _maintain_order_vertical(self, child):
        (left, top, right, bot) = child.bbox()

        if child is self._node:
--
        # Return a list of the nodes we moved
        return moved

    def _maintain_order_horizontal(self, child):
        (left, top, right, bot) = child.bbox()

        if child is self._node:
--
        # Return a list of the nodes we moved
        return moved

    def _manage_horizontal(self):
        (nodex, nodey) = self._node_bottom()

        # Put the subtrees in a line.
--
        for subtree in self._subtrees:
            subtree.move(0, nodey-center)

    def _manage_vertical(self):
        (nodex, nodey) = self._node_bottom()

        # Put the subtrees in a line.
--
        for subtree in self._subtrees:
            subtree.move(nodex-center, 0)

    def _manage(self):
        self._managing = True
        (nodex, nodey) = self._node_bottom()
        if len(self._subtrees) == 0: return
--

        self._managing = False

    def __repr__(self):
        return '[TreeSeg %s: %s]' % (self._node, self._subtrees)

def _tree_to_treeseg(canvas, t, make_node, make_leaf,
                     tree_attribs, node_attribs,
                     leaf_attribs, loc_attribs):
    if isinstance(t, Tree):
--
    else:
        return make_leaf(canvas, t, **leaf_attribs)

def tree_to_treesegment(canvas, t, make_node=TextWidget,
                        make_leaf=TextWidget, **attribs):
    """
    Convert a Tree into a ``TreeSegmentWidget``.
--
        a Tree).  Note that a location widget is a ``TextWidget``.

      - ``xspace``: The amount of horizontal space to leave between
        subtrees when managing this widget.  Default value is 10.
      - ``yspace``: The amount of space to place between the node and
        its children when managing this widget.  Default value is 15.

      - ``line_color``: The color of the lines connecting each expanded
        node to its subtrees.
--

      - ``orientation``: Determines whether the tree branches downwards
        or rightwards.  Possible values are ``'horizontal'`` and
        ``'vertical'``.  The default value is ``'vertical'`` (i.e.,
        branch downwards).

      - ``shapeable``: whether the subtrees can be independently
--
        segments.
      - ``draggable``: whether the widget can be dragged by the user.
    """
    def __init__(self, canvas, t, make_node=TextWidget,
                 make_leaf=TextWidget, **attribs):
        # Node & leaf canvas widget constructors
        self._make_node = make_node
--

        CanvasWidget.__init__(self, canvas, **attribs)

    def expanded_tree(self, *path_to_tree):
        """
        Return the ``TreeSegmentWidget`` for the specified subtree.

--
        """
        return self._expanded_trees[path_to_tree]

    def collapsed_tree(self, *path_to_tree):
        """
        Return the ``TreeSegmentWidget`` for the specified subtree.

--
        """
        return self._collapsed_trees[path_to_tree]

    def bind_click_trees(self, callback, button=1):
        """
        Add a binding to all tree segments.
        """
--
        for tseg in list(self._collapsed_trees.values()):
            tseg.bind_click(callback, button)

    def bind_drag_trees(self, callback, button=1):
        """
        Add a binding to all tree segments.
        """
--
        for tseg in list(self._collapsed_trees.values()):
            tseg.bind_drag(callback, button)

    def bind_click_leaves(self, callback, button=1):
        """
        Add a binding to all leaves.
        """
        for leaf in self._leaves: leaf.bind_click(callback, button)
        for leaf in self._leaves: leaf.bind_click(callback, button)

    def bind_drag_leaves(self, callback, button=1):
        """
        Add a binding to all leaves.
        """
        for leaf in self._leaves: leaf.bind_drag(callback, button)
        for leaf in self._leaves: leaf.bind_drag(callback, button)

    def bind_click_nodes(self, callback, button=1):
        """
        Add a binding to all nodes.
        """
        for node in self._nodes: node.bind_click(callback, button)
        for node in self._nodes: node.bind_click(callback, button)

    def bind_drag_nodes(self, callback, button=1):
        """
        Add a binding to all nodes.
        """
        for node in self._nodes: node.bind_drag(callback, button)
        for node in self._nodes: node.bind_drag(callback, button)

    def _make_collapsed_trees(self, canvas, t, key):
        if not isinstance(t, Tree): return
        make_node = self._make_node
        make_leaf = self._make_leaf
--
            child = t[i]
            self._make_collapsed_trees(canvas, child, key + (i,))

    def _make_expanded_tree(self, canvas, t, key):
        make_node = self._make_node
        make_leaf = self._make_leaf

--
            self._leaves.append(leaf)
            return leaf

    def __setitem__(self, attr, value):
        if attr[:5] == 'node_':
            for node in self._nodes: node[attr[5:]] = value
        elif attr[:5] == 'leaf_':
--
                tseg['ordered'] = value
        else: CanvasWidget.__setitem__(self, attr, value)

    def __getitem__(self, attr):
        if attr[:5] == 'node_':
            return self._nodeattribs.get(attr[5:], None)
        elif attr[:5] == 'leaf_':
--
        elif attr == 'orientation': return self._orientation
        else: return CanvasWidget.__getitem__(self, attr)

    def _tags(self): return []

    def _manage(self):
        segs = list(self._expanded_trees.values()) + list(self._collapsed_trees.values())
        for tseg in segs:
            if tseg.hidden():
--
                tseg.manage()
                tseg.hide()

    def toggle_collapsed(self, treeseg):
        """
        Collapse/expand a tree.
        """
--
##//////////////////////////////////////////////////////

class TreeView(object):
    def __init__(self, *trees):
        from math import sqrt, ceil

        self._trees = trees
--
        self._cframe.pack(expand=1, fill='both')
        self._init_menubar()

    def _layout(self):
        i = x = y = ymax = 0
        width = self._width
        for i in range(len(self._widgets)):
--
            x = widget.bbox()[2] + 10
            ymax = max(ymax, widget.bbox()[3] + 10)

    def _init_menubar(self):
        menubar = Menu(self._top)

        filemenu = Menu(menubar, tearoff=0)
--

        self._top.config(menu=menubar)

    def resize(self, *e):
        bold = ('helvetica', -self._size.get(), 'bold')
        helv = ('helvetica', -self._size.get())
        xspace = self._size.get()
--
            else: widget['line_width'] = 3
        self._layout()

    def destroy(self, *e):
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
        if in_idle(): return
        self._top.mainloop(*args, **kwargs)

def draw_trees(*trees):
    """
    Open a new window containing a graphical diagram of the given
    trees.
--
##  Demo Code
##//////////////////////////////////////////////////////

def demo():
    import random
    def fill(cw):
        cw['fill'] = '#%06d' % random.randint(0,999999)

    cf = CanvasFrame(width=550, height=450, closeenough=2)
--
                    leaf_color='green4', node_color='blue2')
    cf.add_widget(tc,10,10)

    def boxit(canvas, text):
        big = ('helvetica', -16, 'bold')
        return BoxWidget(canvas, TextWidget(canvas, text,
                                            font=big), fill='green')
    def ovalit(canvas, text):
        return OvalWidget(canvas, TextWidget(canvas, text),
                          fill='cyan')

    treetok = Tree.parse('(S (NP this tree) (VP (V is) (AdjP shapeable)))')
    tc2 = TreeWidget(cf.canvas(), treetok, boxit, ovalit, shapeable=1)

    def color(node):
        node['color'] = '#%04d00' % random.randint(0,9999)
    def color2(treeseg):
        treeseg.node()['fill'] = '#%06d' % random.randint(0,9999)
        treeseg.node().child()['color'] = 'white'

--
    tc3['draggable'] = 1
    cf.add_widget(tc3, 10, tc.bbox()[3]+10)

    def orientswitch(treewidget):
        if treewidget['orientation'] == 'horizontal':
            treewidget.expanded_tree(1,1).subtrees()[0].set_text('vertical')
            treewidget.collapsed_tree(1,1).subtrees()[0].set_text('vertical')
--
different elements of each of the trees.
The top-left tree is a TreeWidget built from
a Tree.  The top-right is a TreeWidget built
from a Tree, using non-default widget
constructors for the nodes & leaves (BoxWidget
and OvalWidget).  The bottom-left tree is
built from tree_to_treesegment."""
--
class ProductionList(ColorizedList):
    ARROW = SymbolWidget.SYMBOLS['rightarrow']

    def _init_colortags(self, textwidget, options):
        textwidget.tag_config('terminal', foreground='#006000')
        textwidget.tag_config('arrow', font='symbol', underline='0')
        textwidget.tag_config('nonterminal', foreground='blue',
                              font=('helvetica', -12, 'bold'))

    def _item_repr(self, item):
        contents = []
        contents.append(('%s\t' % item.lhs(), 'nonterminal'))
        contents.append((self.ARROW, 'arrow'))
--
    _TOKEN_RE = re.compile("\\w+|->|'[\\w ]+'|\"[\\w ]+\"|("+ARROW+")")
    _BOLD = ('helvetica', -12, 'bold')

    def __init__(self, parent, cfg=None, set_cfg_callback=None):
        self._parent = parent
        if cfg is not None: self._cfg = cfg
        else: self._cfg = ContextFreeGrammar(Nonterminal('S'), [])
--

        self._textwidget.focus()

    def _init_startframe(self):
        frame = self._startframe = Frame(self._top)
        self._start = Entry(frame)
        self._start.pack(side='right')
--
        Label(frame, text='Productions:').pack(side='left')
        self._start.insert(0, self._cfg.start().symbol())

    def _init_buttons(self):
        frame = self._buttonframe = Frame(self._top)
        Button(frame, text='Ok', command=self._ok,
               underline=0, takefocus=0).pack(side='left')
--
        Button(frame, text='Help', command=self._help,
               underline=0, takefocus=0).pack(side='right')

    def _init_bindings(self):
        self._top.title('CFG Editor')
        self._top.bind('<Control-q>', self._cancel)
        self._top.bind('<Alt-q>', self._cancel)
--
        self._top.bind('<Alt-h>', self._help)
        self._top.bind('<F1>', self._help)

    def _init_prodframe(self):
        self._prodframe = Frame(self._top)

        # Create the basic Text widget & scrollbar.
--
        self._top.bind('<ButtonPress>', self._check_analyze)

        # Tab cycles focus. (why doesn't this work??)
        def cycle(e, textwidget=self._textwidget):
            textwidget.tk_focusNext().focus()
        self._textwidget.bind('<Tab>', cycle)

--
#         prod_by_lhs = {}
#         for prod in self._cfg.productions():
#             if len(prod.rhs()) > 0:
#                 prod_by_lhs.setdefault(prod.lhs(),[]).append(prod)
#         for (lhs, prods) in prod_by_lhs.items():
#             self._textwidget.insert('end', '%s ->' % lhs)
#             self._textwidget.insert('end', self._rhs(prods[0]))
--
#                 self._textwidget.insert('end', '%s' % prod)
#         self._analyze()

#     def _rhs(self, prod):
#         s = ''
#         for elt in prod.rhs():
#             if isinstance(elt, Nonterminal): s += ' %s' % elt.symbol()
#             else: s += ' %r' % elt
#         return s

    def _clear_tags(self, linenum):
        """
        Remove all tags (except ``arrow`` and ``sel``) from the given
        line of the text widget used for editing the productions.
--
            if tag not in ('arrow', 'sel'):
                self._textwidget.tag_remove(tag, start, end)

    def _check_analyze(self, *e):
        """
        Check if we've moved to a new line.  If we have, then remove
        all colorization from the line we moved to, and re-colorize
--
            self._analyze_line(self._linenum)
            self._linenum = linenum

    def _replace_arrows(self, *e):
        """
        Replace any ``'->'`` text strings with arrows (char \\256, in
        symbol font).  This searches the whole buffer, but is fast
--
            if arrow == '': break
            self._textwidget.tag_add('arrow', arrow, arrow+'+1char')

    def _analyze_token(self, match, linenum):
        """
        Given a line number and a regexp match for a token on that
        line, colorize the token.  Note that the regexp match gives us
--
        end = '%d.%d' % (linenum, match.end())
        self._textwidget.tag_add(tag, start, end)

    def _init_nonterminal_tag(self, tag, foreground='blue'):
        self._textwidget.tag_config(tag, foreground=foreground,
                                    font=CFGEditor._BOLD)
        if not self._highlight_matching_nonterminals:
            return
        def enter(e, textwidget=self._textwidget, tag=tag):
            textwidget.tag_config(tag, background='#80ff80')
        def leave(e, textwidget=self._textwidget, tag=tag):
            textwidget.tag_config(tag, background='')
        self._textwidget.tag_bind(tag, '<Enter>', enter)
        self._textwidget.tag_bind(tag, '<Leave>', leave)

    def _analyze_line(self, linenum):
        """
        Colorize a given line.
        """
--
        if CFGEditor._PRODUCTION_RE.match(line):
            # It's valid; Use _TOKEN_RE to tokenize the production,
            # and call analyze_token on each token.
            def analyze_token(match, self=self, linenum=linenum):
                self._analyze_token(match, linenum)
                return ''
            CFGEditor._TOKEN_RE.sub(analyze_token, line)
--
            # It's invalid; show the user where the error is.
            self._mark_error(linenum, line)

    def _mark_error(self, linenum, line):
        """
        Mark the location of an error in a line.
        """
--
            end = '%d.end' % linenum
        self._textwidget.tag_add('error', start, end)

    def _analyze(self, *e):
        """
        Replace ``->`` with arrows, and colorize the entire buffer.
        """
--
        for linenum in range(1, numlines+1):  # line numbers start at 1.
            self._analyze_line(linenum)

    def _parse_productions(self):
        """
        Parse the current contents of the textwidget buffer, to create
        a list of productions.
--
            #(lhs_str, rhs_str) = line.split('->')
            #lhs = Nonterminal(lhs_str.strip())
            #rhs = []
            #def parse_token(match, rhs=rhs):
            #    token = match.group()
            #    if token[0] in "'\"": rhs.append(token[1:-1])
            #    else: rhs.append(Nonterminal(token))
--

        return productions

    def _destroy(self, *e):
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def _ok(self, *e):
        self._apply()
        self._destroy()

    def _apply(self, *e):
        productions = self._parse_productions()
        start = Nonterminal(self._start.get())
        cfg = ContextFreeGrammar(start, productions)
        if self._set_cfg_callback is not None:
            self._set_cfg_callback(cfg)

    def _reset(self, *e):
        self._textwidget.delete('1.0', 'end')
        for production in self._cfg.productions():
            self._textwidget.insert('end', '%s\n' % production)
--
        if self._set_cfg_callback is not None:
            self._set_cfg_callback(self._cfg)

    def _cancel(self, *e):
        try: self._reset()
        except: pass
        self._destroy()

    def _help(self, *e):
        # The default font's not very legible; try using 'fixed' instead.
        try:
            ShowText(self._parent, 'Help: Chart Parser Demo',
                     (_CFGEditor_HELP).strip(), width=75, font='fixed')
--
######################################################################

class CFGDemo(object):
    def __init__(self, grammar, text):
        self._grammar = grammar
        self._text = text

--
    # Initialization
    #//////////////////////////////////////////////////

    def _init_bindings(self, top):
        top.bind('<Control-q>', self.destroy)

    def _init_menubar(self, parent): pass

    def _init_buttons(self, parent): pass

    def _init_grammar(self, parent):
        self._prodlist = ProductionList(parent, self._grammar, width=20)
        self._prodlist.pack(side='top', fill='both', expand=1)
        self._prodlist.focus()
        self._prodlist.add_callback('select', self._selectprod_cb)
        self._prodlist.add_callback('move', self._selectprod_cb)

    def _init_treelet(self, parent):
        self._treelet_canvas = Canvas(parent, background='white')
        self._treelet_canvas.pack(side='bottom', fill='x')
        self._treelet = None

    def _init_workspace(self, parent):
        self._workspace = CanvasFrame(parent, background='white')
        self._workspace.pack(side='right', fill='both', expand=1)
        self._tree = None
--
    # Workspace
    #//////////////////////////////////////////////////

    def reset_workspace(self):
        c = self._workspace.canvas()
        fontsize = int(self._size.get())
        node_font = ('helvetica', -(fontsize+4), 'bold')
--
        #self._nodes = {start:1}
        #self._leaves = dict([(l,1) for l in leaves])

    def workspace_markprod(self, production):
        pass

    def _markproduction(self, prod, tree=None):
        if tree is None: tree = self._tree
        for i in range(len(tree.subtrees())-len(prod.rhs())):
            if tree['color', i] == 'white':
--
    # Grammar
    #//////////////////////////////////////////////////

    def _selectprod_cb(self, production):
        canvas = self._treelet_canvas

        self._prodlist.highlight(production)
--
        # Mark the places where we can add it to the workspace.
        self._markproduction(production)

    def destroy(self, *args):
        self._top.destroy()

    def mainloop(self, *args, **kwargs):
        self._top.mainloop(*args, **kwargs)

def demo2():
    from nltk import Nonterminal, Production, ContextFreeGrammar
    nonterminals = 'S VP NP PP P N Name V Det'
    (S, VP, NP, PP, P, N, Name, V, Det) = [Nonterminal(s)
--
# Old Demo
######################################################################

def demo():
    from nltk import Nonterminal, parse_cfg
    nonterminals = 'S VP NP PP P N Name V Det'
    (S, VP, NP, PP, P, N, Name, V, Det) = [Nonterminal(s)
--
    P -> 'with'
    """)

    def cb(grammar): print(grammar)
    top = Tk()
    editor = CFGEditor(top, grammar, cb)
    Label(top, text='\nTesting CFG Editor\n').pack()
    Button(top, text='Quit', command=top.destroy).pack()
    top.mainloop()

def demo3():
    from nltk import Production
    (S, VP, NP, PP, P, N, Name, V, Det) = \
        nonterminals('S, VP, NP, PP, P, N, Name, V, Det')
--
        )

    t = Tk()
    def destroy(e, t=t): t.destroy()
    t.bind('q', destroy)
    p = ProductionList(t, productions)
    p.pack(expand=1, fill='both')
--
    'VBZ'), ("n't", 'RB'), ('all', 'DT'), ('that', 'DT'), ('bad', 'JJ'),
    ('.', '.')]

This package defines several taggers, which take a token list (typically a
sentence), assign a tag to each token, and return the resulting list of
tagged tokens.  Most of the taggers are built automatically based on a
training corpus.  For example, the unigram tagger tags each word *w*
--
from nltk.tag.api        import TaggerI
from nltk.tag.util       import str2tuple, tuple2str, untag
from nltk.tag.sequential import (SequentialBackoffTagger, ContextTagger,
                                 DefaultTagger, NgramTagger, UnigramTagger,
                                 BigramTagger, TrigramTagger, AffixTagger,
                                 RegexpTagger, ClassifierBasedTagger,
                                 ClassifierBasedPOSTagger)
--

# Standard treebank POS tagger
_POS_TAGGER = 'taggers/maxent_treebank_pos_tagger/english.pickle'
def pos_tag(tokens):
    """
    Use NLTK's currently recommended part of speech tagger to
    tag the given list of tokens.
--
    tagger = load(_POS_TAGGER)
    return tagger.tag(tokens)

def batch_pos_tag(sentences):
    """
    Use NLTK's currently recommended part of speech tagger to tag the
    given list of sentences, each consisting of a list of tokens.
--
_TEXT = 0  # index of text in a tuple
_TAG = 1   # index of tag in a tuple

def _identity(labeled_symbols):
    return labeled_symbols


--
class HiddenMarkovModelTagger(TaggerI):
    """
    Hidden Markov model class, a generative model for labelling sequence data.
    These models define the joint probability of a sequence of symbols and
    their labels (state transitions) as the product of the starting state
    probability, the probability of each state transition, and the probability
    of each observation being generated from each state. This is described in
--
        of starting in state i
    :type priors: ProbDistI
    :param transform: an optional function for transforming training
        instances, defaults to the identity function.
    :type transform: callable
    """
    def __init__(self, symbols, states, transitions, outputs, priors,
                 transform=_identity, **kwargs):
        self._symbols = list(set(symbols))
        self._states = list(set(states))
--
        self._transform = transform

    @classmethod
    def _train(cls, labeled_sequence, test_sequence=None,
                    unlabeled_sequence=None, transform=_identity,
                    estimator=None, **kwargs):

        if estimator is None:
            def estimator(fd, bins):
                return LidstoneProbDist(fd, 0.1, bins)

        labeled_sequence = LazyMap(transform, labeled_sequence)
--
        return hmm

    @classmethod
    def train(cls, labeled_sequence, test_sequence=None,
                   unlabeled_sequence=None, **kwargs):
        """
        Train a new HiddenMarkovModelTagger using the given labeled and
--
            i.e. a list of sentences represented as words
        :type unlabeled_sequence: list(list)
        :param transform: an optional function for transforming training
            instances, defaults to the identity function, see ``transform()``
        :type transform: function
        :param estimator: an optional function or class that maps a
            condition's frequency distribution to its probability
            distribution, defaults to a Lidstone distribution with gamma = 0.1
        :type estimator: class or function
        :param verbose: boolean flag indicating whether training should be
            verbose or include printed output
--
        return cls._train(labeled_sequence, test_sequence,
                          unlabeled_sequence, **kwargs)

    def probability(self, sequence):
        """
        Returns the probability of the given symbol sequence. If the sequence
        is labelled, then returns the joint probability of the symbol, state
--
        """
        return 2**(self.log_probability(self._transform(sequence)))

    def log_probability(self, sequence):
        """
        Returns the log-probability of the given symbol sequence. If the
        sequence is labelled, then returns the joint log-probability of the
--
            p = logsumexp2(alpha[T-1])
            return p

    def tag(self, unlabeled_sequence):
        """
        Tags the sequence with the highest probability state sequence. This
        uses the best_path method to find the Viterbi path.
--
        unlabeled_sequence = self._transform(unlabeled_sequence)
        return self._tag(unlabeled_sequence)

    def _tag(self, unlabeled_sequence):
        path = self._best_path(unlabeled_sequence)
        return list(izip(unlabeled_sequence, path))

    def _output_logprob(self, state, symbol):
        """
        :return: the log probability of the symbol being observed in the given
            state
--
        """
        return self._outputs[state].logprob(symbol)

    def _create_cache(self):
        """
        The cache is a tuple (P, O, X, S) where:

--
                S[self._symbols[k]] = k
            self._cache = (P, O, X, S)

    def _update_cache(self, symbols):
        # add new symbols to the symbol table and repopulate the output
        # probabilities and symbol table mapping
        if symbols:
--
                    S[self._symbols[k]] = k
                self._cache = (P, O, X, S)

    def reset_cache(self):
        self._cache = None

    def best_path(self, unlabeled_sequence):
        """
        Returns the state sequence of the optimal (most probable) path through
        the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
--
        unlabeled_sequence = self._transform(unlabeled_sequence)
        return self._best_path(unlabeled_sequence)

    def _best_path(self, unlabeled_sequence):
        T = len(unlabeled_sequence)
        N = len(self._states)
        self._create_cache()
--
        sequence.reverse()
        return list(map(self._states.__getitem__, sequence))

    def best_path_simple(self, unlabeled_sequence):
        """
        Returns the state sequence of the optimal (most probable) path through
        the HMM. Uses the Viterbi algorithm to calculate this part by dynamic
--
        unlabeled_sequence = self._transform(unlabeled_sequence)
        return self._best_path_simple(unlabeled_sequence)

    def _best_path_simple(self, unlabeled_sequence):
        T = len(unlabeled_sequence)
        N = len(self._states)
        V = np.zeros((T, N), np.float64)
--
        sequence.reverse()
        return sequence

    def random_sample(self, rng, length):
        """
        Randomly sample the HMM to generate a sentence of a given length. This
        samples the prior distribution then the observation distribution and
--

        return tokens

    def _sample_probdist(self, probdist, p, samples):
        cum_p = 0
        for sample in samples:
            add_p = probdist.prob(sample)
--
        raise Exception('Invalid probability distribution - '
                        'does not sum to one')

    def entropy(self, unlabeled_sequence):
        """
        Returns the entropy over labellings of the given sequence. This is
        given by::
--

        return entropy

    def point_entropy(self, unlabeled_sequence):
        """
        Returns the pointwise entropy over the possible states at each
        position in the chain, given the observation sequence.
--

        return entropies

    def _exhaustive_entropy(self, unlabeled_sequence):
        unlabeled_sequence = self._transform(unlabeled_sequence)

        T = len(unlabeled_sequence)
--

        return entropy

    def _exhaustive_point_entropy(self, unlabeled_sequence):
        unlabeled_sequence = self._transform(unlabeled_sequence)

        T = len(unlabeled_sequence)
--

        return entropies

    def _transitions_matrix(self):
        """ Return a matrix of transition log probabilities. """
        trans_iter = (self._transitions[sj].logprob(si)
                      for sj in self._states
--
        N = len(self._states)
        return transitions_logprob.reshape((N, N)).T

    def _outputs_vector(self, symbol):
        """
        Return a vector with log probabilities of emitting a symbol
        when entering states.
--
        out_iter = (self._output_logprob(sj, symbol) for sj in self._states)
        return np.fromiter(out_iter, dtype=np.float64)

    def _forward_probability(self, unlabeled_sequence):
        """
        Return the forward probability matrix, a T by N array of
        log-probabilities, where T is the length of the sequence and N is the
--

        return alpha

    def _backward_probability(self, unlabeled_sequence):
        """
        Return the backward probability matrix, a T by N array of
        log-probabilities, where T is the length of the sequence and N is the
--

        return beta

    def test(self, test_sequence, verbose=False, **kwargs):
        """
        Tests the HiddenMarkovModelTagger instance.

--
        :type verbose: bool
        """

        def words(sent):
            return [word for (word, tag) in sent]

        def tags(sent):
            return [tag for (word, tag) in sent]

        def flatten(seq):
            return list(itertools.chain(*seq))

        test_sequence = self._transform(test_sequence)
--
        count = sum(len(sent) for sent in test_sequence)
        print('accuracy over %d tokens: %.2f' % (count, acc * 100))

    def __repr__(self):
        return ('<HiddenMarkovModelTagger %d states and %d output symbols>'
                % (len(self._states), len(self._symbols)))

--
    :param symbols: the set of observation symbols
    :type symbols:  sequence of any
    """
    def __init__(self, states=None, symbols=None):
        self._states = (states if states else [])
        self._symbols = (symbols if symbols else [])

    def train(self, labeled_sequences=None, unlabeled_sequences=None,
              **kwargs):
        """
        Trains the HMM using both (or either of) supervised and unsupervised
--
        return model


    def _baum_welch_step(self, sequence, model, symbol_to_number):

        N = len(model._states)
        M = len(model._symbols)
--

        return lpk, A_numer, A_denom, B_numer, B_denom

    def train_unsupervised(self, unlabeled_sequences, update_outputs=True,
                           **kwargs):
        """
        Trains the HMM using the Baum-Welch algorithm to maximise the
--

        return model

    def train_supervised(self, labelled_sequences, **kwargs):
        """
        Supervised training maximising the joint probability of the symbol and
        state sequences. This is done via collecting frequencies of
--
            otherwise a MLE estimate is used
        """

        # default to the MLE estimate
        estimator = kwargs.get('estimator')
        if estimator is None:
            estimator = lambda fdist, bins: MLEProbDist(fdist)
--
        return HiddenMarkovModelTagger(self._symbols, self._states, A, B, pi)


def _ninf_array(shape):
    res = np.empty(shape, np.float64)
    res.fill(-np.inf)
    return res


def logsumexp2(arr):
    max_ = arr.max()
    return np.log2(np.sum(2**(arr - max_))) + max_


def _log_add(*values):
    """
    Adds the logged values, returning the logarithm of the addition.
    """
--
        return x


def _create_hmm_tagger(states, symbols, A, B, pi):
    def pd(values, samples):
        d = dict(zip(samples, values))
        return DictionaryProbDist(d)

    def cpd(array, conditions, samples):
        d = {}
        for values, condition in zip(array, conditions):
            d[condition] = pd(values, samples)
--
                                   transitions=A, outputs=B, priors=pi)


def _market_hmm_example():
    """
    Return an example HMM (described at page 381, Huang et al)
    """
--
    return model, states, symbols


def demo():
    # demonstrates HMM probability calculation

    print()
--
        print('H_exh(point)=', model._exhaustive_point_entropy(sequence))
        print()

def load_pos(num_sents):
    from nltk.corpus import brown

    sentences = brown.tagged_sents(categories='news')[:num_sents]
--

    return cleaned_sentences, list(tag_set), list(symbols)

def demo_pos():
    # demonstrates POS tagging using supervised training

    print()
--
    print('Testing...')
    hmm.test(labelled_sequences[:10], verbose=True)

def _untag(sentences):
    unlabeled = []
    for sentence in sentences:
        unlabeled.append([(token[_TEXT], None) for token in sentence])
    return unlabeled

def demo_pos_bw(test=10, supervised=20, unsupervised=10, verbose=True,
                max_iterations=5):
    # demonstrates the Baum-Welch algorithm in POS tagging

--
    hmm.test(sentences[:test], verbose=verbose)

    print('Training (unsupervised, %d sentences)...' % unsupervised)
    # it's rather slow - so only use 10 samples by default
    unlabeled = _untag(sentences[test+supervised:])
    hmm = trainer.train_unsupervised(unlabeled, model=hmm,
                                     max_iterations=max_iterations)
    hmm.test(sentences[:test], verbose=verbose)

def demo_bw():
    # demo Baum Welch by generating some sequences and then performing
    # unsupervised training on them

--
    extracting the named entities.

    SENNA pipeline has a fixed maximum size of the sentences that it can read.
    By default it is 1024 token/sentence. If you have larger sentences, changing
    the MAX_SENTENCE_SIZE value in SENNA_main.c should be considered and your
    system specific binary should be rebuilt. Otherwise this could introduce
    misalignment errors.
--
    The input is:
    - path to the directory that contains SENNA executables.
    - List of the operations needed to be performed.
    - (optionally) the encoding of the input data (default:utf-8)

    Example:

--

    SUPPORTED_OPERATIONS = ['pos', 'chk', 'ner']

    def __init__(self, senna_path, operations, encoding='utf-8'):
        self._encoding = encoding
        self._path = path.normpath(senna_path) + sep
        self.operations = operations

    @property
    def executable(self):
        """
        A property that determines the system specific binary that should be
        used in the pipeline. In case, the system is not known the senna binary will
--
            return path.join(self._path, 'senna-osx')
        return path.join(self._path, 'senna')

    def _map(self):
        """
        A method that calculates the order of the columns that SENNA pipeline
        will output the tags into. This depends on the operations being ordered.
--
                i+= 1
        return _map

    def tag(self, tokens):
        """
        Applies the specified operation(s) on a list of tokens.
        """
        return self.batch_tag([tokens])[0]

    def batch_tag(self, sentences):
        """
        Applies the tag method over a list of sentences. This method will return a
        list of dictionaries. Every dictionary will contain a word with its
--

    The input is:
    - path to the directory that contains SENNA executables.
    - (optionally) the encoding of the input data (default:utf-8)

    Example:

--
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'),
        ('of', 'IN'), ('an', 'DT'), ('unladen', 'JJ'), ('swallow', 'VB'), ('?', '.')]
    """
    def __init__(self, path, encoding='utf-8'):
        super(POSTagger, self).__init__(path, ['pos'], encoding)

    def batch_tag(self, sentences):
        """
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
--

    The input is:
    - path to the directory that contains SENNA executables.
    - (optionally) the encoding of the input data (default:utf-8)

    Example:

--
        [('UN', u'B-ORG'), ('headquarters', u'O'), ('are', u'O'), ('in', u'O'),
        ('NY', u'B-LOC'), (',', u'O'), ('USA', u'B-LOC'), ('.', u'O')]
    """
    def __init__(self, path, encoding='utf-8'):
        super(NERTagger, self).__init__(path, ['ner'], encoding)

    def batch_tag(self, sentences):
        """
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
--

    The input is:
    - path to the directory that contains SENNA executables.
    - (optionally) the encoding of the input data (default:utf-8)

    Example:

--
        ('of', u'B-PP'), ('an', u'B-NP'), ('unladen', u'I-NP'), ('swallow',u'I-NP'),
        ('?', u'O')]
    """
    def __init__(self, path, encoding='utf-8'):
        super(CHKTagger, self).__init__(path, ['chk'], encoding)

    def batch_tag(self, sentences):
        """
        Applies the tag method over a list of sentences. This method will return
        for each sentence a list of tuples of (word, tag).
--
        return tagged_sents

# skip doctests if Senna is not installed
def setup_module(module):
    from nose import SkipTest
    tagger = POSTagger('/usr/share/senna-v2.0')
    if not path.isfile(tagger.executable):
--
        feature detector was a lambda function).
    """

    def __init__(self, filename, feature_detector=None):
        # Read the CRFInfo from the model file.
        zf = zipfile.ZipFile(filename)
        crf_info = CRFInfo.fromstring(zf.read('crf-info.xml'))
--
    #/////////////////////////////////////////////////////////////////

    @property
    def filename(self):
        """
        The filename of the crf model file that backs this
        MalletCRF.  The crf model file is actually a zip file, and
--
        return self.crf_info.model_filename

    @property
    def feature_detector(self):
        """
        The feature detector function that is used to convert tokens
        to featuresets.  This function has the signature::
--
    #: The name of the java script used to run MalletCRFs.
    _RUN_CRF = "org.nltk.mallet.RunCRF"

    def batch_tag(self, sentences):
        # Write the test corpus to a temporary file
        (fd, test_file) = mkstemp('.txt', 'test')
        self.write_test_corpus(sentences, os.fdopen(fd, 'w'))
--
    _TRAIN_CRF = "org.nltk.mallet.TrainCRF"

    @classmethod
    def train(cls, feature_detector, corpus, filename=None,
              weight_groups=None, gaussian_variance=1, default_label='O',
              transduction_type='VITERBI', max_iterations=500,
              add_start_state=True, add_end_state=True, trace=1):
        """
--
        :type gaussian_variance: float
        :param gaussian_variance: The gaussian variance of the prior
            that should be used to train the new CRF.
        :type default_label: str
        :param default_label: The "label for initial context and
            uninteresting tokens" (from Mallet's SimpleTagger.java.)
            It's unclear whether this currently has any effect.
        :type transduction_type: str
--

        # Create crf-info object describing the new CRF.
        crf_info = MalletCRF._build_crf_info(
            corpus, gaussian_variance, default_label, max_iterations,
            transduction_type, weight_groups, add_start_state,
            add_end_state, filename, feature_detector)

--
        return crf

    @staticmethod
    def _build_crf_info(corpus, gaussian_variance, default_label,
                        max_iterations, transduction_type, weight_groups,
                        add_start_state, add_end_state,
                        model_filename, feature_detector):
--
            labels.add('__end__')
        transitions = set() # not necessary to find this?
        for sent in corpus:
            prevtag = default_label
            for (tok,tag) in sent:
                labels.add(tag)
                transitions.add( (prevtag, tag) )
--
                transitions.add( (sent[-1][1], '__end__') )
        labels = sorted(labels)

        # 0th order default:
        if weight_groups is None:
            weight_groups = [CRFInfo.WeightGroup(name=l, src='.*',
                                                 dst=re.escape(l))
--
            state_info_list.append(state_info)

        return CRFInfo(state_info_list, gaussian_variance,
                       default_label, max_iterations,
                       transduction_type, weight_groups,
                       add_start_state, add_end_state,
                       model_filename, feature_detector)

    #: A table used to filter the output that mallet generates during
    #: training.  By default, mallet generates very verbose output.
    #: This table is used to select which lines of output are actually
    #: worth displaying to the user, based on the level of the *trace*
    #: parameter.  Each entry of this table is a tuple
--
        ]

    @staticmethod
    def _filter_training_output(p, trace):
        """
        Filter the (very verbose) output that is generated by mallet,
        and only display the interesting lines.  The lines that are
--
    # Communication w/ mallet
    #/////////////////////////////////////////////////////////////////

    def write_training_corpus(self, corpus, stream, close_stream=True):
        """
        Write a given training corpus to a given stream, in a format that
        can be read by the java script org.nltk.mallet.TrainCRF.
--
            stream.write('\n')
        if close_stream: stream.close()

    def write_test_corpus(self, corpus, stream, close_stream=True):
        """
        Write a given test corpus to a given stream, in a format that
        can be read by the java script org.nltk.mallet.TestCRF.
--
            stream.write('\n')
        if close_stream: stream.close()

    def parse_mallet_output(self, s):
        """
        Parse the output that is generated by the java script
        org.nltk.mallet.TestCRF, and convert it to a labeled
--

    _ESCAPE_RE = re.compile('[^a-zA-Z0-9]')
    @staticmethod
    def _escape_sub(m):
        return '%' + ('%02x' % ord(m.group()))

    @staticmethod
    def _format_feature(fname, fval):
        """
        Return a string name for a given feature (name, value) pair,
        appropriate for consumption by mallet.  We escape every
--
    # String Representation
    #/////////////////////////////////////////////////////////////////

    def __repr__(self):
        return 'MalletCRF(%r)' % self.crf_info.model_filename

###########################################################################
--
    ``MalletCRF.train()`` function.  See CRFInfo.WeightGroup for
    more information.
    """
    def __init__(self, states, gaussian_variance, default_label,
                 max_iterations, transduction_type, weight_groups,
                 add_start_state, add_end_state, model_filename,
                 feature_detector):
        self.gaussian_variance = float(gaussian_variance)
        self.default_label = default_label
        self.states = states
        self.max_iterations = max_iterations
        self.transduction_type = transduction_type
--
        '<crf>\n'
        '  <modelFile>%(model_filename)s</modelFile>\n'
        '  <gaussianVariance>%(gaussian_variance)d</gaussianVariance>\n'
        '  <defaultLabel>%(default_label)s</defaultLabel>\n'
        '  <maxIterations>%(max_iterations)s</maxIterations>\n'
        '  <transductionType>%(transduction_type)s</transductionType>\n'
        '  <featureDetector name="%(feature_detector_name)s">\n'
--
        '  </weightGroups>\n'
        '</crf>\n')

    def toxml(self):
        info = self.__dict__.copy()
        info['states'] = '\n'.join(state.toxml() for state in self.states)
        info['w_groups'] = '\n'.join(wg.toxml() for wg in self.weight_groups)
--
        return self._XML_TEMPLATE % info

    @staticmethod
    def fromstring(s):
        return CRFInfo._read(ElementTree.fromstring(s))

    @staticmethod
    def _read(etree):
        states = [CRFInfo.State._read(et) for et in
                  etree.findall('states/state')]
        weight_groups = [CRFInfo.WeightGroup._read(et) for et in
--

        return CRFInfo(states,
                       float(etree.find('gaussianVariance').text),
                       etree.find('defaultLabel').text,
                       int(etree.find('maxIterations').text),
                       etree.find('transductionType').text,
                       weight_groups,
--
                       etree.find('modelFile').text,
                       feature_detector)

    def write(self, filename, encoding='utf8'):
        with codecs.open(filename, 'w', encoding) as out:
            out.write(self.toxml())
            out.write('\n')
--
        """
        A description of a single CRF state.
        """
        def __init__(self, name, initial_cost, final_cost, transitions):
            if initial_cost != '+inf': initial_cost = float(initial_cost)
            if final_cost != '+inf': final_cost = float(final_cost)
            self.name = name
--
            '%(transitions)s\n'
            '      </transitions>\n'
            '    </state>\n')
        def toxml(self):
            info = self.__dict__.copy()
            info['transitions'] = '\n'.join(transition.toxml()
                                            for transition in self.transitions)
            return self._XML_TEMPLATE % info

        @staticmethod
        def _read(etree):
            transitions = [CRFInfo.Transition._read(et)
                           for et in etree.findall('transitions/transition')]
            return CRFInfo.State(etree.get('name'),
--
        """
        A description of a single CRF transition.
        """
        def __init__(self, destination, label, weightgroups):
            """
            :param destination: The name of the state that this transition
                connects to.
--
        _XML_TEMPLATE = ('        <transition label="%(label)s" '
                         'destination="%(destination)s" '
                         'weightGroups="%(w_groups)s"/>')
        def toxml(self):
            info = self.__dict__
            info['w_groups'] = ' '.join(wg for wg in self.weightgroups)
            return self._XML_TEMPLATE % info

        @staticmethod
        def _read(etree):
            return CRFInfo.Transition(etree.get('destination'),
                                      etree.get('label'),
                                      etree.get('weightGroups').split())
--
        should be paired with all transitions from a given set of source
        tags to a given set of destination tags.
        """
        def __init__(self, name, src, dst, features='.*'):
            """
            :param name: A unique name for this weight group.
            :param src: The set of source tags that should be used for
--

        _XML_TEMPLATE = ('    <weightGroup name="%(name)s" src="%(src)s" '
                         'dst="%(dst)s" features="%(features)s" />')
        def toxml(self):
            return self._XML_TEMPLATE % self.__dict__

        @staticmethod
        def _read(etree):
            return CRFInfo.WeightGroup(etree.get('name'),
                                       etree.get('src'),
                                       etree.get('dst'),
                                       etree.get('features'))

        # [xx] feature name????
        def match(self, src, dst):
            # Check if the source matches
            src_match = self._src_match_cache.get(src)
            if src_match is None:
--
## Demonstration code
###########################################################################

def demo(train_size=100, test_size=100, java_home=None, mallet_home=None):
    from nltk.corpus import brown
    import textwrap

    # Define a very simple feature detector
    def fd(sentence, index):
        word = sentence[index]
        return dict(word=word, suffix=word[-2:], len=len(word))

--

    # Get the training & test corpus.  We simplify the tagset a little:
    # just the first 2 chars.
    def strip(corpus): return [[(w, t[:2]) for (w,t) in sent]
                               for sent in corpus]
    brown_train = strip(brown.tagged_sents(categories='news')[:train_size])
    brown_test = strip(brown.tagged_sents(categories='editorial')[:test_size])
--
    gain in the accuracy of the results.
    '''

    def __init__(self, unk=None, Trained=False, N=1000, C=False):
        '''
        Construct a TnT statistical tagger. Tagger must be trained
        before being used to tag input.
--
        self.unknown = 0
        self.known = 0

    def train(self, data):
        '''
        Uses a set of tagged data to train the tagger.
        If an unknown word tagger is specified,
--
        #print i, self._l1, i, self._l2, i, self._l3


    def _compute_lambda(self):
        '''
        creates lambda values based upon training data

--



    def _safe_div(self, v1, v2):
        '''
        Safe floating point division function, does not allow division by 0
        returns -1 if the denominator is 0
--
        else:
            return float(v1) / float(v2)

    def tagdata(self, data):
        '''
        Tags each sentence in a list of sentences

--
        return res


    def tag(self, data):
        '''
        Tags a single sentence

--
        return res


    def _tagword(self, sent, current_states):
        '''
        :param sent : List of words remaining in the sentence
        :type sent  : [word,]
--
# helper function -- basic sentence tokenizer
########################################

def basic_sent_chop(data, raw=True):
    '''
    Basic method for tokenizing input into sentences
    for this tagger:
--



def demo():
    from nltk.corpus import brown
    sents = list(brown.tagged_sents())
    test = list(brown.sents())
--
        print()


def demo2():
    from nltk.corpus import treebank

    d = list(treebank.tagged_sents())
--
        print('Percentage unknown:', sp_un)
        print('Accuracy over known words:', (sacc / sp_kn))

def demo3():
    from nltk.corpus import treebank, brown

    d = list(treebank.tagged_sents())
--
"""

from __future__ import print_function, unicode_literals, division
from collections import defaultdict
from os.path import join

from nltk.data import load
--
_UNIVERSAL_DATA = "taggers/universal_tagset"
_UNIVERSAL_TAGS = ('VERB','NOUN','PRON','ADJ','ADV','ADP','CONJ','DET','NUM','PRT','X','.')

_MAPPINGS = defaultdict(lambda: defaultdict(dict))


def _load_universal_map(fileid):
    mapping = {}
    contents = load(join(_UNIVERSAL_DATA, fileid+'.map'), format="text")
    for line in contents.splitlines():
--
        _MAPPINGS[fileid]['universal'][fine] = coarse

            
def tagset_mapping(source, target):
    """
    Retrieve the mapping dictionary between tagsets.
    
--
            _load_universal_map(source)
    return _MAPPINGS[source][target]

def map_tag(source, target, source_tag):
    """
    Maps the tag from the source tagset to the target tagset.
    
--
# URL: <http://www.nltk.org/>
# For license information, see LICENSE.TXT

def str2tuple(s, sep='/'):
    """
    Given the string representation of a tagged token, return the
    corresponding tuple representation.  The rightmost occurrence of
--
    else:
        return (s, None)

def tuple2str(tagged_token, sep='/'):
    """
    Given the tuple representation of a tagged token, return the
    corresponding string representation.  This representation is
--
        assert sep not in tag, 'tag may not contain sep!'
        return '%s%s%s' % (word, sep, tag)

def untag(tagged_sentence):
    """
    Given a tagged sentence, return an untagged version of that
    sentence.  I.e., return a list containing the first element
--
import random        # for shuffling WSJ files
import yaml          # to save and load taggers in files
import textwrap
from collections import defaultdict

from nltk.tag.util import untag
from nltk.tag.api import TaggerI
--
class BrillTagger(TaggerI, yaml.YAMLObject):
    """
    Brill's transformational rule-based tagger.  Brill taggers use an
    initial tagger (such as ``tag.DefaultTagger``) to assign an initial
    tag sequence to a text; and then apply an ordered list of
    transformational rules to correct the tags of individual tokens.
    These transformation rules are specified by the ``BrillRule``
--
    """

    yaml_tag = '!nltk.BrillTagger'
    def __init__(self, initial_tagger, rules):
        """
        :param initial_tagger: The initial tagger
        :type initial_tagger: TaggerI
--
        self._initial_tagger = initial_tagger
        self._rules = tuple(rules)

    def rules(self):
        return self._rules

    def tag(self, tokens):
        # Inherit documentation from TaggerI

        # Run the initial tagger.
--

        # Create a dictionary that maps each tag to a list of the
        # indices of tokens that have that tag.
        tag_to_positions = defaultdict(set)
        for i, (token, tag) in enumerate(tagged_tokens):
            tag_to_positions[tag].add(i)

--

    Brill rules must be comparable and hashable.
    """
    def __init__(self, original_tag, replacement_tag):
        assert self.__class__ != BrillRule, \
               "BrillRule is an abstract base class"

--
        self.replacement_tag = replacement_tag
        """The tag with which this BrillRule may replace another tag."""

    def apply(self, tokens, positions=None):
        """
        Apply this rule at every position in positions where it
        applies to the given sentence.  I.e., for each position p
--

        return change

    def applies(self, tokens, index):
        """
        :return: True if the rule would change the tag of
            ``tokens[index]``, False otherwise
--
        :param index: The index to check
        :type index: int
        """
        assert False, "Brill rules must define applies()"

    # Rules must be comparable and hashable for the algorithm to work
    def __eq__(self):
        assert False, "Brill rules must be comparable"
    def __ne__(self):
        assert False, "Brill rules must be comparable"
    def __hash__(self):
        assert False, "Brill rules must be hashable"


--
    the presence of tokens with given properties at given ranges of
    positions, relative to the token.

    Each subclass of proximate tokens brill rule defines a method
    ``extract_property()``, which extracts a specific property from the
    the token, such as its text or tag.  Each instance is
    parameterized by a set of tuples, specifying ranges of positions
--
    :raise ValueError: If start>end for any condition.
    """

    def __init__(self, original_tag, replacement_tag, *conditions):
        assert self.__class__ != ProximateTokensRule, \
               "ProximateTokensRule is an abstract base class"
        BrillRule.__init__(self, original_tag, replacement_tag)
--

    # Make Brill rules look nice in YAML.
    @classmethod
    def to_yaml(cls, dumper, data):
        node = dumper.represent_mapping(cls.yaml_tag, dict(
            description="%s" % data,
            conditions=list(list(x) for x in data._conditions),
--
            replacement=data.replacement_tag))
        return node
    @classmethod
    def from_yaml(cls, loader, node):
        map = loader.construct_mapping(node, deep=True)
        return cls(map['original'], map['replacement'],
        *(tuple(x) for x in map['conditions']))

    @staticmethod
    def extract_property(token):
        """
        Returns some property characterizing this token, such as its
        base lexical item or its tag.
--
        :return: The property
        :rtype: any
        """
        assert False, "ProximateTokenRules must define extract_property()"

    def applies(self, tokens, index):
        # Inherit docs from BrillRule

        # Does the given token have this rule's "original tag"?
--
        # Every condition checked out, so the rule is applicable.
        return True

    def __eq__(self, other):
        return (self is other or
                (other is not None and
                 other.__class__ == self.__class__ and
--
                 self.replacement_tag == other.replacement_tag and
                 self._conditions == other._conditions))

    def __ne__(self, other):
        return not (self==other)

    def __hash__(self):
        # Cache our hash value (justified by profiling.)
        try:
            return self.__hash
--
                                 self._conditions, self.__class__.__name__) )
            return self.__hash

    def __repr__(self):
        # Cache our repr (justified by profiling -- this is used as
        # a sort key when deterministic=True.)
        try:
--
            return self.__repr


    def __str__(self):
        replacement = '%s -> %s' % (self.original_tag,
                                    self.replacement_tag)
        if len(self._conditions) == 0:
--
                                               for c in self._conditions)
        return replacement+conditions

    def _condition_to_str(self, condition):
        """
        Return a string representation of the given condition.
        This helper method is used by __str__.
--
        return ("the %s of %s is '%s'" %
                (self.PROPERTY_NAME, self._range_to_str(start, end), value))

    def _range_to_str(self, start, end):
        """
        Return a string representation for the given range.  This
        helper method is used by __str__.
--
    PROPERTY_NAME = 'tag' # for printing.
    yaml_tag = '!ProximateTagsRule'
    @staticmethod
    def extract_property(token):
        """:return: The given token's tag."""
        return token[1]

--
    PROPERTY_NAME = 'text' # for printing.
    yaml_tag = '!ProximateWordsRule'
    @staticmethod
    def extract_property(token):
        """:return: The given token's text."""
        return token[0]

--
    apply at given sentence positions.  ``BrillTemplateI`` is used by
    ``Brill`` training algorithms to generate candidate rules.
    """
    def __init__(self):
        raise NotImplementedError()

    def applicable_rules(self, tokens, i, correctTag):
        """
        Return a list of the transformational rules that would correct
        the *i*th subtoken's tag in the given token.  In particular,
--
        """
        raise NotImplementedError()

    def get_neighborhood(self, token, index):
        """
        Returns the set of indices *i* such that
        ``applicable_rules(token, i, ...)`` depends on the value of
--
    :raise ValueError: If start>end for any boundary.
    """

    def __init__(self, rule_class, *boundaries):
        self._rule_class = rule_class
        self._boundaries = boundaries
        for (s,e) in boundaries:
--
                raise ValueError('Boundary %s has an invalid range' %
                                 ((s,e),))

    def applicable_rules(self, tokens, index, correct_tag):
        if tokens[index][1] == correct_tag:
            return []

--
        return [self._rule_class(tokens[index][1], correct_tag, *conds)
                for conds in condition_combos]

    def _applicable_conditions(self, tokens, index, start, end):
        """
        :return: A set of all conditions for proximate token rules
        that are applicable to *tokens[index]*, given boundaries of
--
            conditions.append( (start, end, value) )
        return conditions

    def get_neighborhood(self, tokens, index):
        # inherit docs from BrillTemplateI

        # applicable_rules(tokens, index, ...) depends on index.
--
    :raise ValueError: If start>end for any boundary.
    """

    def __init__(self, rule_class, *boundaries):
        self._ptt1 = ProximateTokensTemplate(rule_class, *boundaries)
        reversed = [(-e,-s) for (s,e) in boundaries]
        self._ptt2 = ProximateTokensTemplate(rule_class, *reversed)

    # Generates lists of a subtype of ProximateTokensRule.
    def applicable_rules(self, tokens, index, correctTag):
        """
        See ``BrillTemplateI`` for full specifications.

--
        return (self._ptt1.applicable_rules(tokens, index, correctTag) +
                self._ptt2.applicable_rules(tokens, index, correctTag))

    def get_neighborhood(self, tokens, index):
        # inherit docs from BrillTemplateI
        n1 = self._ptt1.get_neighborhood(tokens, index)
        n2 = self._ptt2.get_neighborhood(tokens, index)
--
        not specified, treat as true iff trace > 0.
    """

    def __init__(self, initial_tagger, templates, trace=0,
                 deterministic=None):
        if deterministic is None: deterministic = (trace > 0)
        self._initial_tagger = initial_tagger
--
    # Training
    #////////////////////////////////////////////////////////////

    def train(self, train_sents, max_rules=200, min_score=2):
        """
        Trains the Brill tagger on the corpus *train_sents*,
        producing at most *max_rules* transformations, each of which
--

    # Finds the rule that makes the biggest net improvement in the corpus.
    # Returns a (rule, score) pair.
    def _best_rule(self, test_sents, train_sents):
        # Create a dictionary mapping from each tag to a list of the
        # indices that have that tag in both test_sents and
        # train_sents (i.e., where it is correctly tagged).
        correct_indices = defaultdict(list)
        for sentnum, sent in enumerate(test_sents):
            for wordnum, tagged_word in enumerate(sent):
                if tagged_word[1] == train_sents[sentnum][wordnum][1]:
--
        # Return the best rule, and its score.
        return best_rule, best_score, best_fixscore

    def _find_rules(self, test_sents, train_sents):
        """
        Find all rules that correct at least one token's tag in *test_sents*.

--

        # Create a dictionary mapping from rules to their positive-only
        # scores.
        rule_score_dict = defaultdict(int)
        for (sentnum, wordnum) in error_indices:
            test_sent = test_sents[sentnum]
            train_sent = train_sents[sentnum]
--
        return sorted(rule_score_dict.items(),
                      key=lambda rule_score: -rule_score[1])

    def _find_rules_at(self, test_sent, train_sent, i):
        """
        :rtype: set
        :return: the set of all rules (based on the templates) that
--
    # Tracing
    #////////////////////////////////////////////////////////////

    def _trace_header(self):
        print("""
           B      |
   S   F   r   O  |        Score = Fixed - Broken
--
------------------+-------------------------------------------------------
        """.rstrip())

    def _trace_rule(self, rule, score, fixscore, numchanges):
        if self._trace > 2:
            print(('%4d%4d%4d%4d ' % (score, fixscore, fixscore-score,
                                      numchanges-fixscore*2+score)), '|', end=' ')
--
    """
    A faster trainer for brill taggers.
    """
    def __init__(self, initial_tagger, templates, trace=0,
                 deterministic=False):
        if not deterministic:
            deterministic = (trace > 0)
--
    # Training
    #////////////////////////////////////////////////////////////

    def train(self, train_sents, max_rules=200, min_score=2):
        # Basic idea: Keep track of the rules that apply at each position.
        # And keep track of the positions to which each rule applies.

--
        # Create and return a tagger from the rules we found.
        return BrillTagger(self._initial_tagger, rules)

    def _init_mappings(self, test_sents, train_sents):
        """
        Initialize the tag position mapping & the rule related
        mappings.  For each error in test_sents, find new rules that
        would correct them, and add them to the rule mappings.
        """
        self._tag_positions = defaultdict(list)
        self._rules_by_position = defaultdict(set)
        self._positions_by_rule = defaultdict(dict)
        self._rules_by_score = defaultdict(set)
        self._rule_scores = defaultdict(int)
        self._first_unknown_position = defaultdict(int)

        # Scan through the corpus, initializing the tag_positions
        # mapping and all the rule-related mappings.
--
                        self._update_rule_applies(rule, sentnum, wordnum,
                                                  train_sents)

    def _clean(self):
        self._tag_positions = None
        self._rules_by_position = None
        self._positions_by_rule = None
--
        self._rule_scores = None
        self._first_unknown_position = None

    def _find_rules(self, sent, wordnum, new_tag):
        """
        Use the templates to find rules that apply at index *wordnum*
        in the sentence *sent* and generate the tag *new_tag*.
--
            for rule in template.applicable_rules(sent, wordnum, new_tag):
                yield rule

    def _update_rule_applies(self, rule, sentnum, wordnum, train_sents):
        """
        Update the rule data tables to reflect the fact that
        *rule* applies at the position *(sentnum, wordnum)*.
--
        self._rules_by_score[old_score].discard(rule)
        self._rules_by_score[self._rule_scores[rule]].add(rule)

    def _update_rule_not_applies(self, rule, sentnum, wordnum):
        """
        Update the rule data tables to reflect the fact that *rule*
        does not apply at the position *(sentnum, wordnum)*.
--
        # Optional addition: if the rule now applies nowhere, delete
        # all its dictionary entries.

    def _best_rule(self, train_sents, test_sents, min_score):
        """
        Find the next best rule.  This is done by repeatedly taking a
        rule with the highest score and stepping through the corpus to
--
        # We reached the min-score threshold.
        return None

    def _apply_rule(self, rule, test_sents):
        """
        Update *test_sents* by applying *rule* everywhere where its
        conditions are met.
--
            text = test_sents[sentnum][wordnum][0]
            test_sents[sentnum][wordnum] = (text, new_tag)

    def _update_tag_positions(self, rule):
        """
        Update _tag_positions to reflect the changes to tags that are
        made by *rule*.
--
            new_tag_positions = self._tag_positions[rule.replacement_tag]
            bisect.insort_left(new_tag_positions, pos)

    def _update_rules(self, rule, train_sents, test_sents):
        """
        Check if we should add or remove any rules from consideration,
        given the changes made by *rule*.
--
    # Tracing
    #////////////////////////////////////////////////////////////

    def _trace_header(self):
        print("""
           B      |
   S   F   r   O  |        Score = Fixed - Broken
--
------------------+-------------------------------------------------------
        """.rstrip())

    def _trace_rule(self, rule):
        assert self._rule_scores[rule] == \
               sum(self._positions_by_rule[rule].values())

--
        else:
            print(rule)

    def _trace_apply(self, num_updates):
        prefix = ' '*18+'|'
        print(prefix)
        print(prefix, 'Applying rule to %d positions.' % num_updates)

    def _trace_update_rules(self, num_obsolete, num_new, num_unseen):
        prefix = ' '*18+'|'
        print(prefix, 'Updated rule tables:')
        print(prefix, ('  - %d rule applications removed' % num_obsolete))
--
######################################################################

# returns a list of errors in string format
def error_list (train_sents, test_sents, radius=2):
    """
    Returns a list of human-readable strings indicating the errors in the
    given tagging of the corpus.
--
# Demonstration
######################################################################

def demo(num_sents=2000, max_rules=200, min_score=3,
         error_output="errors.out", rule_output="rules.yaml",
         randomize=False, train=.8, trace=3):
    """
--
Classes for tagging sentences sequentially, left to right.  The
abstract base class SequentialBackoffTagger serves as the base
class for all the taggers in this module.  Tagging of individual words
is performed by the method ``choose_tag()``, which is defined by
subclasses of SequentialBackoffTagger.  If a tagger is unable to
determine a tag for the specified token, then its backoff tagger is
consulted instead.  Any SequentialBackoffTagger may serve as a
--
    """
    An abstract base class for taggers that tags words sequentially,
    left to right.  Tagging of individual words is performed by the
    ``choose_tag()`` method, which should be defined by subclasses.  If
    a tagger is unable to determine a tag for the specified token,
    then its backoff tagger is consulted.

    :ivar _taggers: A list of all the taggers that should be tried to
        tag a token (i.e., self and its backoff taggers).
    """
    def __init__(self, backoff=None):
        if backoff is None:
            self._taggers = [self]
        else:
            self._taggers = [self] + backoff._taggers

    @property
    def backoff(self):
        """The backoff tagger for this tagger."""
        return (self._taggers[1] if len(self._taggers) > 1 else None)

    def tag(self, tokens):
        # docs inherited from TaggerI
        tags = []
        for i in range(len(tokens)):
            tags.append(self.tag_one(tokens, i, tags))
        return list(zip(tokens, tags))

    def tag_one(self, tokens, index, history):
        """
        Determine an appropriate tag for the specified token, and
        return that tag.  If this tagger is unable to determine a tag
--
            if tag is not None:  break
        return tag

    def choose_tag(self, tokens, index, history):
        """
        Decide which tag should be used for the specified token, and
        return that tag.  If this tagger is unable to determine a tag
--
    """
    An abstract base class for sequential backoff taggers that choose
    a tag for a token based on the value of its "context".  Different
    subclasses are used to define different contexts.

    A ContextTagger chooses the tag for a token by calculating the
    token's context, and looking up the corresponding tag in a table.
--

    :ivar _context_to_tag: Dictionary mapping contexts to tags.
    """
    def __init__(self, context_to_tag, backoff=None):
        """
        :param context_to_tag: A dictionary mapping contexts to tags.
        :param backoff: The backoff tagger that should be used for this tagger.
--
        SequentialBackoffTagger.__init__(self, backoff)
        self._context_to_tag = (context_to_tag if context_to_tag else {})

    def context(self, tokens, index, history):
        """
        :return: the context that should be used to look up the tag
            for the specified token; or None if the specified token
--
        """
        raise NotImplementedError()

    def choose_tag(self, tokens, index, history):
        context = self.context(tokens, index, history)
        return self._context_to_tag.get(context)

    def size(self):
        """
        :return: The number of entries in the table used by this
            tagger to map from contexts to tags.
        """
        return len(self._context_to_tag)

    def __repr__(self):
        return '<%s: size=%d>' % (self.__class__.__name__, self.size())

    def _train(self, tagged_corpus, cutoff=0, verbose=False):
        """
        Initialize this ContextTagger's ``_context_to_tag`` table
        based on the given training data.  In particular, for each
--
######################################################################

@python_2_unicode_compatible
class DefaultTagger(SequentialBackoffTagger, yaml.YAMLObject):
    """
    A tagger that assigns the same tag to every token.

        >>> from nltk.tag.sequential import DefaultTagger
        >>> default_tagger = DefaultTagger('NN')
        >>> list(default_tagger.tag('This is a test'.split()))
        [('This', 'NN'), ('is', 'NN'), ('a', 'NN'), ('test', 'NN')]

    This tagger is recommended as a backoff tagger, in cases where
--
    :param tag: The tag to assign to each token
    :type tag: str
    """
    yaml_tag = '!nltk.DefaultTagger'

    def __init__(self, tag):
        self._tag = tag
        SequentialBackoffTagger.__init__(self, None)

    def choose_tag(self, tokens, index, history):
        return self._tag  # ignore token and history

    def __repr__(self):
        return '<DefaultTagger: tag=%s>' % self._tag


class NgramTagger(ContextTagger, yaml.YAMLObject):
--
    """
    yaml_tag = '!nltk.NgramTagger'

    def __init__(self, n, train=None, model=None,
                 backoff=None, cutoff=0, verbose=False):
        self._n = n
        self._check_params(train, model)
--
        if train:
            self._train(train, cutoff, verbose)

    def context(self, tokens, index, history):
        tag_context = tuple(history[max(0,index-self._n+1):index])
        return (tag_context, tokens[index])

--

    yaml_tag = '!nltk.UnigramTagger'

    def __init__(self, train=None, model=None,
                 backoff=None, cutoff=0, verbose=False):
        NgramTagger.__init__(self, 1, train, model,
                             backoff, cutoff, verbose)

    def context(self, tokens, index, history):
        return tokens[index]


--
    """
    yaml_tag = '!nltk.BigramTagger'

    def __init__(self, train=None, model=None,
                 backoff=None, cutoff=0, verbose=False):
        NgramTagger.__init__(self, 2, train, model,
                             backoff, cutoff, verbose)
--
    """
    yaml_tag = '!nltk.TrigramTagger'

    def __init__(self, train=None, model=None,
                 backoff=None, cutoff=0, verbose=False):
        NgramTagger.__init__(self, 3, train, model,
                             backoff, cutoff, verbose)
--

    yaml_tag = '!nltk.AffixTagger'

    def __init__(self, train=None, model=None, affix_length=-3,
                 min_stem_length=2, backoff=None, cutoff=0, verbose=False):

        self._check_params(train, model)
--
        if train:
            self._train(train, cutoff, verbose)

    def context(self, tokens, index, history):
        token = tokens[index]
        if len(token) < self._min_word_length:
            return None
--
        ...      (r'.*s$', 'NNS'),                  # plural nouns
        ...      (r'.*ing$', 'VBG'),                # gerunds
        ...      (r'.*ed$', 'VBD'),                 # past tense verbs
        ...      (r'.*', 'NN')                      # nouns (default)
        ... ])
        >>> regexp_tagger
        <Regexp Tagger: size=9>
--

    yaml_tag = '!nltk.RegexpTagger'

    def __init__(self, regexps, backoff=None):
        """
        """
        SequentialBackoffTagger.__init__(self, backoff)
--
        self._regexs = re.compile('|'.join('(?P<%s>%s)' % (label, regex) for regex,label in regexps_labels))
        self._size=len(regexps)

    def choose_tag(self, tokens, index, history):
        m = self._regexs.match(tokens[index])
        if m:
          return self._map[m.lastgroup]
        return None

    def __repr__(self):
        return '<Regexp Tagger: size=%d>' % self._size


--
        back on its backoff tagger if the probability of the most
        likely tag is less than *cutoff_prob*.
    """
    def __init__(self, feature_detector=None, train=None,
                 classifier_builder=NaiveBayesClassifier.train,
                 classifier=None, backoff=None,
                 cutoff_prob=None, verbose=False):
--
        if train:
            self._train(train, classifier_builder, verbose)

    def choose_tag(self, tokens, index, history):
        # Use our feature detector to get the featureset.
        featureset = self.feature_detector(tokens, index, history)

--
        tag = pdist.max()
        return (tag if pdist.prob(tag) >= self._cutoff_prob else None)

    def _train(self, tagged_corpus, classifier_builder, verbose):
        """
        Build a new classifier, based on the given training data
        *tagged_corpus*.
--
            print('Training classifier (%d instances)' % len(classifier_corpus))
        self._classifier = classifier_builder(classifier_corpus)

    def __repr__(self):
        return '<ClassifierBasedTagger: %r>' % self._classifier

    def feature_detector(self, tokens, index, history):
        """
        Return the feature detector that this tagger uses to generate
        featuresets for its classifier.  The feature detector is a
--
        """
        return self._feature_detector(tokens, index, history)

    def classifier(self):
        """
        Return the classifier that this tagger uses to choose a tag
        for each word in a sentence.  The input for this classifier is
--
    """
    A classifier based part of speech tagger.
    """
    def feature_detector(self, tokens, index, history):
        word = tokens[index]
        if index == 0:
            prevword = prevprevword = None
--

class StanfordTagger(TaggerI):
    """
    An interface to Stanford taggers. Subclasses must define:

    - ``_cmd`` property: A property that returns the command that will be
      executed.
--
    _SEPARATOR = ''
    _JAR = ''

    def __init__(self, path_to_model, path_to_jar=None, encoding='ascii', verbose=False, java_options='-mx1000m'):

        if not self._JAR:
            warnings.warn('The StanfordTagger class is not meant to be '
--
        self.java_options = java_options

    @property
    def _cmd(self):
      raise NotImplementedError

    def tag(self, tokens):
        return self.batch_tag([tokens])[0]

    def batch_tag(self, sentences):
        encoding = self._encoding
        default_options = ' '.join(_java_options)
        config_java(options=self.java_options, verbose=False)

        # Create a temporary input file
--
        # Delete the temporary file
        os.unlink(self._input_file_path)

        # Return java configurations to their default values
        config_java(options=default_options, verbose=False)

        return self.parse_output(stanpos_output)

    def parse_output(self, text):
        # Output the tagged sentences
        tagged_sentences = []
        for tagged_sentence in text.strip().split("\n"):
--
     - a model trained on training data
     - (optionally) the path to the stanford tagger jar file. If not specified here,
       then this jar file must be specified in the CLASSPATH envinroment variable.
     - (optionally) the encoding of the training data (default: ASCII)

    Example:

--
    _SEPARATOR = '_'
    _JAR = 'stanford-postagger.jar'

    def __init__(self, *args, **kwargs):
        super(POSTagger, self).__init__(*args, **kwargs)

    @property
    def _cmd(self):
        return ['edu.stanford.nlp.tagger.maxent.MaxentTagger', \
                '-model', self._stanford_model, '-textFile', \
                self._input_file_path, '-tokenize', 'false']
--
    - a model trained on training data
    - (optionally) the path to the stanford tagger jar file. If not specified here,
      then this jar file must be specified in the CLASSPATH envinroment variable.
    - (optionally) the encoding of the training data (default: ASCII)

    Example:

--
    _JAR = 'stanford-ner.jar'
    _FORMAT = 'slashTags'

    def __init__(self, *args, **kwargs):
        super(NERTagger, self).__init__(*args, **kwargs)

    @property
    def _cmd(self):
        return ['edu.stanford.nlp.ie.crf.CRFClassifier', \
                '-loadClassifier', self._stanford_model, '-textFile', \
                self._input_file_path, '-outputFormat', self._FORMAT]

    def parse_output(self, text):
      if self._FORMAT == 'slashTags':
        return super(NERTagger, self).parse_output(text)
      raise NotImplementedError
--
_hunpos_url = 'http://code.google.com/p/hunpos/'

_hunpos_charset = 'ISO-8859-1'
"""The default encoding used by hunpos: ISO-8859-1."""

class HunposTagger(TaggerI):
    """
    A class for pos tagging with HunPos. The input is the paths to:
     - a model trained on training data
     - (optionally) the path to the hunpos-tag binary
     - (optionally) the encoding of the training data (default: ISO-8859-1)

    Example:

--
        [('What', 'WP'), ('is', 'VBZ'), ('the', 'DT'), ('airspeed', 'NN'), ('of', 'IN'), ('an', 'DT'), ('unladen', 'NN'), ('swallow', 'VB'), ('?', '.')]
    """

    def __init__(self, path_to_model, path_to_bin=None,
                 encoding=_hunpos_charset, verbose=False):
        """
        Starts the hunpos-tag executable and establishes a connection with it.
--
        :param encoding: The encoding used by the model. Unicode tokens
            passed to the tag() and batch_tag() methods are converted to
            this charset when they are sent to hunpos-tag.
            The default is ISO-8859-1 (Latin-1).

            This parameter is ignored for str tokens, which are sent as-is.
            The caller must ensure that tokens are encoded in the right charset.
--
                             shell=False, stdin=PIPE, stdout=PIPE, stderr=PIPE)
        self._closed = False

    def __del__(self):
        self.close()

    def close(self):
        """Closes the pipe to the hunpos executable."""
        if not self._closed:
            self._hunpos.communicate()
            self._closed = True

    def __enter__(self):
        return self
    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def tag(self, tokens):
        """Tags a single sentence: a list of words.
        The tokens should not contain any newline characters.
        """
--
        return tagged_tokens

# skip doctests if Hunpos tagger is not installed
def setup_module(module):
    from nose import SkipTest
    try:
        HunposTagger('english.model')
--
    For example, featureset taggers, which are subclassed from
    ``FeaturesetTagger``, require that each token be a ``featureset``.

    Subclasses must define:
      - either ``tag()`` or ``batch_tag()`` (or both)
    """
    def tag(self, tokens):
        """
        Determine the most appropriate tag sequence for the given
        token sequence, and return a corresponding list of tagged
--
        else:
            raise NotImplementedError()

    def batch_tag(self, sentences):
        """
        Apply ``self.tag()`` to each element of *sentences*.  I.e.:

--
        """
        return [self.tag(sent) for sent in sentences]

    def evaluate(self, gold):
        """
        Score the accuracy of the tagger against the gold standard.
        Strip the tags from the gold standard text, retag it using
--
        test_tokens = sum(tagged_sents, [])
        return accuracy(gold_tokens, test_tokens)

    def _check_params(self, train, model):
        if (train and model) or (not train and not model):
            raise ValueError('Must specify either training data or trained model.')

--

Corpus Reader Functions
=======================
Each corpus module defines one or more "corpus reader functions",
which can be used to read documents from that corpus.  These functions
take an argument, ``item``, which is used to indicate which document
should be read from the corpus:
--
    'words', WordListCorpusReader, r'(?!README|\.).*', encoding='ascii')
ycoe = LazyCorpusLoader(
    'ycoe', YCOECorpusReader)
# defined after treebank
propbank = LazyCorpusLoader(
    'propbank', PropbankCorpusReader,
    'prop.txt', 'frames/.*\.xml', 'verbs.txt',
    lambda filename: re.sub(r'^wsj/\d\d/', '', filename),
    treebank) # Must be defined *after* treebank corpus.
nombank = LazyCorpusLoader(
    'nombank.1.0', NombankCorpusReader,
    'nombank.1.0', 'frames/.*\.xml', 'nombank.1.0.words',
    lambda filename: re.sub(r'^wsj/\d\d/', '', filename),
    treebank) # Must be defined *after* treebank corpus.
propbank_ptb = LazyCorpusLoader(
    'propbank', PropbankCorpusReader,
    'prop.txt', 'frames/.*\.xml', 'verbs.txt',
    lambda filename: filename.upper(),
    ptb) # Must be defined *after* ptb corpus.
nombank_ptb = LazyCorpusLoader(
    'nombank.1.0', NombankCorpusReader,
    'nombank.1.0', 'frames/.*\.xml', 'nombank.1.0.words',
    lambda filename: filename.upper(),
    ptb) # Must be defined *after* ptb corpus.

def demo():
    # This is out-of-date:
    abc.demo()
    brown.demo()
--

# ** this is for nose **
# unload all corpus after tests
def teardown_module(module=None):
    import nltk.corpus
    for name in dir(nltk.corpus):
        obj = getattr(nltk.corpus, name, None)
--


class TEICorpusView(StreamBackedCorpusView):
	def __init__(self, corpus_file,
					tagged, group_by_sent, group_by_para,
					tagset=None, headLen=0, textids=None):
		self._tagged = tagged
--

	_pagesize = 4096

	def read_block(self, stream):
		block = stream.readlines(self._pagesize)
		block = concat(block)
		while (block.count('<text id') > block.count('</text>')) \
--
				output.extend(para)
		return output

	def _parse_tag(self, tag_word_tuple):
		(tag, word) = tag_word_tuple
		if tag.startswith('w'):
			tag = ANA.search(tag).group(1)
--

	headLen = 2770

	def __init__(self, *args, **kwargs):
		if 'textid_file' in kwargs: self._textids = kwargs['textid_file']
		else: self._textids = None

--

		self._init_textids()

	def _init_textids(self):
		self._f2t = defaultdict(list)
		self._t2f = defaultdict(list)
		if self._textids is not None:
			for line in self.open(self._textids).readlines():
				line = line.strip()
--
				for text_id in text_ids.split(self._delimiter):
					self._add_textids(file_id, text_id)

	def _add_textids(self, file_id, text_id):
		self._f2t[file_id].append(text_id)
		self._t2f[text_id].append(file_id)

	def _resolve(self, fileids, categories, textids=None):
		tmp = None
		if fileids is not None:
			if not tmp:
--
				raise ValueError('Specify only fileids, categories or textids')
		return None, None

	def decode_tag(self, tag):
		# to be implemented
		return tag

	def textids(self, fileids=None, categories=None):
		"""
		In the pl196x corpus each category is stored in single
		file and thus both methods provide identical functionality. In order
--
			fileids = [fileids]
		return sorted(sum((self._f2t[d] for d in fileids), []))

	def words(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def sents(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def paras(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def tagged_words(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def tagged_sents(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def tagged_paras(self, fileids=None, categories=None, textids=None):
		fileids, textids = self._resolve(fileids, categories, textids)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--
										headLen=self.headLen)
								for fileid in fileids])

	def xml(self, fileids=None, categories=None):
		fileids, _ = self._resolve(fileids, categories)
		if len(fileids) == 1: return XMLCorpusReader.xml(self, fileids[0])
		else: raise TypeError('Expected a single file')

	def raw(self, fileids=None, categories=None):
		fileids, _ = self._resolve(fileids, categories)
		if fileids is None: fileids = self._fileids
		elif isinstance(fileids, compat.string_types): fileids = [fileids]
--

@compat.python_2_unicode_compatible
class IEERDocument(object):
    def __init__(self, text, docno=None, doctype=None,
                 date_time=None, headline=''):
        self.text = text
        self.docno = docno
--
        self.date_time = date_time
        self.headline = headline

    def __repr__(self):
        if self.headline:
            headline = ' '.join(self.headline.leaves())
        else:
--
class IEERCorpusReader(CorpusReader):
    """
    """
    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def docs(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid, self._read_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def parsed_docs(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid,
                                              self._read_parsed_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def _read_parsed_block(self,stream):
        # TODO: figure out while empty documents are being returned
        return [self._parse(doc) for doc in self._read_block(stream)
                if self._parse(doc).docno is not None]

    def _parse(self, doc):
        val = nltk.chunk.ieerstr2tree(doc, top_node="DOCUMENT")
        if isinstance(val, dict):
            return IEERDocument(**val)
        else:
            return IEERDocument(val)

    def _read_block(self, stream):
        out = []
        # Skip any preamble.
        while True:
--
from __future__ import print_function

import re
from collections import defaultdict
from functools import reduce

from nltk.corpus.reader import CorpusReader
--
    _key_re = re.compile(r'\("?([^"]+)"? \(desc [0-9.]+\).+')

    @staticmethod
    def __defaultdict_factory():
        ''' Factory for creating defaultdict of defaultdict(dict)s '''
        return defaultdict(dict)

    def __init__(self, root, badscore=0.0):
        '''
        Initialize the thesaurus.

--
        '''

        super(LinThesaurusCorpusReader, self).__init__(root, r'sim[A-Z]\.lsp')
        self._thesaurus = defaultdict(LinThesaurusCorpusReader.__defaultdict_factory)
        self._badscore = badscore
        for path, encoding, fileid in self.abspaths(include_encoding=True, include_fileid=True):
            with open(path) as lin_file:
--
                            ngram, score = split_line
                            self._thesaurus[fileid][key][ngram.strip('"')] = float(score)

    def similarity(self, ngram1, ngram2, fileid=None):
        '''
        Returns the similarity score for two ngrams.

--
                return [(fid, (self._thesaurus[fid][ngram1][ngram2] if ngram2 in self._thesaurus[fid][ngram1]
                                  else self._badscore)) for fid in self._fileids]

    def scored_synonyms(self, ngram, fileid=None):
        '''
        Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram

--
        else:
            return [(fileid, self._thesaurus[fileid][ngram].items()) for fileid in self._fileids]

    def synonyms(self, ngram, fileid=None):
        '''
        Returns a list of synonyms for the current ngram.

--
        else:
            return [(fileid, self._thesaurus[fileid][ngram].keys()) for fileid in self._fileids]

    def __contains__(self, ngram):
        '''
        Determines whether or not the given ngram is in the thesaurus.

--
# Demo
######################################################################

def demo():
    from nltk.corpus import lin_thesaurus as thes

    word1 = "business"
--

class NPSChatCorpusReader(XMLCorpusReader):

    def __init__(self, root, fileids, wrap_etree=False, tagset=None):
        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)
        self._tagset = tagset

    def xml_posts(self, fileids=None):
        if self._wrap_etree:
            return concat([XMLCorpusView(fileid, 'Session/Posts/Post',
                                         self._wrap_elt)
--
            return concat([XMLCorpusView(fileid, 'Session/Posts/Post')
                           for fileid in self.abspaths(fileids)])

    def posts(self, fileids=None):
        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',
                                     self._elt_to_words)
                       for fileid in self.abspaths(fileids)])

    def tagged_posts(self, fileids=None, tagset=None):
        def reader(elt, handler):
            return self._elt_to_tagged_words(elt, handler, tagset)
        return concat([XMLCorpusView(fileid, 'Session/Posts/Post/terminals',
                                     reader)
                       for fileid in self.abspaths(fileids)])

    def words(self, fileids=None):
        return LazyConcatenation(self.posts(fileids))

    def tagged_words(self, fileids=None, tagset=None):
        return LazyConcatenation(self.tagged_posts(fileids, tagset))

    def _wrap_elt(self, elt, handler):
        return ElementWrapper(elt)

    def _elt_to_words(self, elt, handler):
        return [self._simplify_username(t.attrib['word'])
                for t in elt.findall('t')]

    def _elt_to_tagged_words(self, elt, handler, tagset=None):
        tagged_post = [(self._simplify_username(t.attrib['word']),
                        t.attrib['pos']) for t in elt.findall('t')]
        if tagset and tagset != self._tagset:
--
        return tagged_post

    @staticmethod
    def _simplify_username(word):
        if 'User' in word:
            word = 'U' + word.split('User', 1)[1]
        elif isinstance(word, bytes):
--
    """
    Reader for corpora that consist of plaintext documents.  Paragraphs
    are assumed to be split using blank lines.  Sentences and words can
    be tokenized using the default tokenizers, or by custom tokenizers
    specificed as parameters to the constructor.

    This corpus reader can be customized (e.g., to skip preface
--
       ``PlaintextCorpusReader`` may specify alternative corpus view
       classes (e.g., to skip the preface sections of documents.)"""

    def __init__(self, root, fileids,
                 word_tokenizer=WordPunctTokenizer(),
                 sent_tokenizer=nltk.data.LazyLoader(
                     'tokenizers/punkt/english.pickle'),
--
        self._sent_tokenizer = sent_tokenizer
        self._para_block_reader = para_block_reader

    def raw(self, fileids=None):
        """
        :return: the given file(s) as a single string.
        :rtype: str
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        """
        :return: the given file(s) as a list of words
            and punctuation symbols.
--
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])

    def sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
--
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])

    def paras(self, fileids=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
                       for (path, enc, fileid)
                       in self.abspaths(fileids, True, True)])

    def _read_word_block(self, stream):
        words = []
        for i in range(20): # Read 20 lines at a time.
            words.extend(self._word_tokenizer.tokenize(stream.readline()))
        return words

    def _read_sent_block(self, stream):
        sents = []
        for para in self._para_block_reader(stream):
            sents.extend([self._word_tokenizer.tokenize(sent)
                          for sent in self._sent_tokenizer.tokenize(para)])
        return sents

    def _read_para_block(self, stream):
        paras = []
        for para in self._para_block_reader(stream):
            paras.append([self._word_tokenizer.tokenize(sent)
--
    A reader for plaintext corpora whose documents are divided into
    categories based on their file identifiers.
    """
    def __init__(self, *args, **kwargs):
        """
        Initialize the corpus reader.  Categorization arguments
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
--
        CategorizedCorpusReader.__init__(self, kwargs)
        PlaintextCorpusReader.__init__(self, *args, **kwargs)

    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
        if categories is not None:
            return self.fileids(categories)
        else:
            return fileids
    def raw(self, fileids=None, categories=None):
        return PlaintextCorpusReader.raw(
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
        return PlaintextCorpusReader.words(
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
        return PlaintextCorpusReader.sents(
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
        return PlaintextCorpusReader.paras(
            self, self._resolve(fileids, categories))

# is there a better way?
class PortugueseCategorizedPlaintextCorpusReader(CategorizedPlaintextCorpusReader):
    def __init__(self, *args, **kwargs):
        CategorizedCorpusReader.__init__(self, kwargs)
        kwargs['sent_tokenizer'] = nltk.data.LazyLoader('tokenizers/punkt/portuguese.pickle')
        PlaintextCorpusReader.__init__(self, *args, **kwargs)
--
        and paragraphs for Europarl.
    """

    def _read_word_block(self, stream):
        words = []
        for i in range(20): # Read 20 lines at a time.
            words.extend(stream.readline().split())
        return words

    def _read_sent_block(self, stream):
        sents = []
        for para in self._para_block_reader(stream):
            sents.extend([sent.split() for sent in para.splitlines()])
        return sents

    def _read_para_block(self, stream):
        paras = []
        for para in self._para_block_reader(stream):
            paras.append([sent.split() for sent in para.splitlines()])
        return paras

    def chapters(self, fileids=None):
        """
        :return: the given file(s) as a list of
            chapters, each encoded as a list of sentences, which are
--
                                       encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def paras(self, fileids=None):
        raise NotImplementedError('The Europarl corpus reader does not support paragraphs. Please use chapters() instead.')

--

Corpus Reader Functions
=======================
Each corpus module defines one or more "corpus reader functions",
which can be used to read documents from that corpus.  These functions
take an argument, ``item``, which is used to indicate which document
should be read from the corpus:
--
from nltk.corpus.reader.udhr import *

# Make sure that nltk.corpus.reader.bracket_parse gives the module, not
# the function bracket_parse() defined in nltk.tree:
from . import bracket_parse

__all__ = [
--
    are split using a block reader.  They are then tokenized into
    sentences using a sentence tokenizer.  Finally, these sentences
    are parsed into chunk trees using a string-to-chunktree conversion
    function.  Each of these steps can be performed using a default
    function or a custom function.  By default, paragraphs are split
    on blank lines; sentences are listed one per line; and sentences
    are parsed into chunk trees using ``nltk.chunk.tagstr2tree``.
    """
    def __init__(self, root, fileids, extension='',
                 str2chunktree=tagstr2tree,
                 sent_tokenizer=RegexpTokenizer('\n', gaps=True),
                 para_block_reader=read_blankline_block,
--
        """Arguments for corpus views generated by this corpus: a tuple
        (str2chunktree, sent_tokenizer, para_block_tokenizer)"""

    def raw(self, fileids=None):
        """
        :return: the given file(s) as a single string.
        :rtype: str
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        """
        :return: the given file(s) as a list of words
            and punctuation symbols.
--
        return concat([ChunkedCorpusView(f, enc, 0, 0, 0, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
--
        return concat([ChunkedCorpusView(f, enc, 0, 1, 0, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def paras(self, fileids=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
        return concat([ChunkedCorpusView(f, enc, 0, 1, 1, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def tagged_words(self, fileids=None):
        """
        :return: the given file(s) as a list of tagged
            words and punctuation symbols, encoded as tuples
--
        return concat([ChunkedCorpusView(f, enc, 1, 0, 0, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def tagged_sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
--
        return concat([ChunkedCorpusView(f, enc, 1, 1, 0, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def tagged_paras(self, fileids=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
        return concat([ChunkedCorpusView(f, enc, 1, 1, 1, 0, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def chunked_words(self, fileids=None):
        """
        :return: the given file(s) as a list of tagged
            words and chunks.  Words are encoded as ``(word, tag)``
--
        return concat([ChunkedCorpusView(f, enc, 1, 0, 0, 1, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def chunked_sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences, each encoded as a shallow Tree.  The leaves
--
        return concat([ChunkedCorpusView(f, enc, 1, 1, 0, 1, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def chunked_paras(self, fileids=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
        return concat([ChunkedCorpusView(f, enc, 1, 1, 1, 1, *self._cv_args)
                       for (f, enc) in self.abspaths(fileids, True)])

    def _read_block(self, stream):
        return [tagstr2tree(t) for t in read_blankline_block(stream)]

class ChunkedCorpusView(StreamBackedCorpusView):
    def __init__(self, fileid, encoding, tagged, group_by_sent,
                 group_by_para, chunked, str2chunktree, sent_tokenizer,
                 para_block_reader):
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)
--
        self._sent_tokenizer = sent_tokenizer
        self._para_block_reader = para_block_reader

    def read_block(self, stream):
        block = []
        for para_str in self._para_block_reader(stream):
            para = []
--
        # Return the block
        return block

    def _untag(self, tree):
        for i, child in enumerate(tree):
            if isinstance(child, Tree):
                self._untag(child)
--
    English Prose (YCOE), a 1.5 million word syntactically-annotated
    corpus of Old English prose texts.
    """
    def __init__(self, root, encoding='utf8'):
        CorpusReader.__init__(self, root, [], encoding)

        self._psd_reader = YCOEParseCorpusReader(
--
        CorpusReader.__init__(self, root, fileids, encoding)
        self._documents = sorted(documents)

    def documents(self, fileids=None):
        """
        Return a list of document identifiers for all documents in
        this corpus, or for the documents with the given file(s) if
--
        # Strip off the '.pos' and '.psd' extensions.
        return sorted(set(f[:-4] for f in fileids))

    def fileids(self, documents=None):
        """
        Return a list of file identifiers for the files that make up
        this corpus, or that store the given document(s) if specified.
--
        return sorted(set(['%s.pos' % doc for doc in documents] +
                          ['%s.psd' % doc for doc in documents]))

    def _getfileids(self, documents, subcorpus):
        """
        Helper that selects the appropriate fileids for a given set of
        documents from a given subcorpus (pos or psd).
--
        return ['%s.%s' % (d, subcorpus) for d in documents]

    # Delegate to one of our two sub-readers:
    def words(self, documents=None):
        return self._pos_reader.words(self._getfileids(documents, 'pos'))
    def sents(self, documents=None):
        return self._pos_reader.sents(self._getfileids(documents, 'pos'))
    def paras(self, documents=None):
        return self._pos_reader.paras(self._getfileids(documents, 'pos'))
    def tagged_words(self, documents=None):
        return self._pos_reader.tagged_words(self._getfileids(documents, 'pos'))
    def tagged_sents(self, documents=None):
        return self._pos_reader.tagged_sents(self._getfileids(documents, 'pos'))
    def tagged_paras(self, documents=None):
        return self._pos_reader.tagged_paras(self._getfileids(documents, 'pos'))
    def parsed_sents(self, documents=None):
        return self._psd_reader.parsed_sents(self._getfileids(documents, 'psd'))


class YCOEParseCorpusReader(BracketParseCorpusReader):
    """Specialized version of the standard bracket parse corpus reader
    that strips out (CODE ...) and (ID ...) nodes."""
    def _parse(self, t):
        t = re.sub(r'(?u)\((CODE|ID)[^\)]*\)', '', t)
        if re.match(r'\s*\(\s*\)\s*$', t): return None
        return BracketParseCorpusReader._parse(self, t)

class YCOETaggedCorpusReader(TaggedCorpusReader):
    def __init__(self, root, items, encoding='utf8'):
        gaps_re = r'(?u)(?<=/\.)\s+|\s*\S*_CODE\s*|\s*\S*_ID\s*'
        sent_tokenizer = RegexpTokenizer(gaps_re, gaps=True)
        TaggedCorpusReader.__init__(self, root, items, sep='_',
--
    method.  For access to simple word lists and tagged word lists, use
    ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.
    """
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
        self._lazy = lazy

    def words(self, fileids=None, strip_space=True, stem=False):
        """
        :return: the given file(s) as a list of words
            and punctuation symbols.
--
                                       strip_space, stem)
                           for fileid in self.abspaths(fileids)])

    def tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False):
        """
        :return: the given file(s) as a list of tagged
            words and punctuation symbols, encoded as tuples
--
            return concat([self._words(fileid, False, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])

    def sents(self, fileids=None, strip_space=True, stem=False):
        """
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
--
            return concat([self._words(fileid, True, None, strip_space, stem)
                           for fileid in self.abspaths(fileids)])

    def tagged_sents(self, fileids=None, c5=False, strip_space=True,
                     stem=False):
        """
        :return: the given file(s) as a list of
--
            return concat([self._words(fileid, True, tag, strip_space, stem)
                           for fileid in self.abspaths(fileids)])

    def _words(self, fileid, bracket_sent, tag, strip_space, stem):
        """
        Helper used to implement the view methods -- returns a list of
        words or a list of sentences, optionally tagged.
--
        assert None not in result
        return result

def _all_xmlwords_in(elt, result=None):
    if result is None: result = []
    for child in elt:
        if child.tag in ('c', 'w'): result.append(child)
--
    A list of words, augmented by an attribute ``num`` used to record
    the sentence identifier (the ``n`` attribute from the XML).
    """
    def __init__(self, num, items):
        self.num = num
        list.__init__(self, items)

--
    """
    A stream backed corpus view specialized for use with the BNC corpus.
    """
    def __init__(self, fileid, sent, tag, strip_space, stem):
        """
        :param fileid: The name of the underlying file.
        :param sent: If true, include sentence bracketing.
--
    editor = None #: Editor
    resps = None #: Statement of responsibility

    def handle_header(self, elt, context):
        # Set up some metadata!
        titles = elt.findall('titleStmt/title')
        if titles: self.title = '\n'.join(
--
            '\n'.join(resp_elt.text.strip() for resp_elt in resp)
            for resp in resps)

    def handle_elt(self, elt, context):
        if self._sent: return self.handle_sent(elt)
        else: return self.handle_word(elt)

    def handle_word(self, elt):
        word = elt.text
        if not word:
            word = "" # fixes issue 337?
--
            word = (word, elt.get('pos', elt.get('c5')))
        return word

    def handle_sent(self, elt):
        sent = []
        for child in elt:
            if child.tag == 'mw':
--
    Reader for corpora that consist of parenthesis-delineated parse
    trees.
    """
    def __init__(self, root, fileids, comment_char=None,
                 detect_blocks='unindented_paren', encoding='utf8',
                 tagset=None):
        """
--
        self._detect_blocks = detect_blocks
        self._tagset = tagset

    def _read_block(self, stream):
        if self._detect_blocks == 'sexpr':
            return read_sexpr_block(stream, comment_char=self._comment_char)
        elif self._detect_blocks == 'blankline':
--
        else:
            assert 0, 'bad block type'

    def _normalize(self, t):
        # If there's an empty set of brackets surrounding the actual
        # parse, then strip them off.
        if EMPTY_BRACKETS.match(t):
--
        t = re.sub(r"\(([^\s()]+) ([^\s()]+) [^\s()]+\)", r"(\1 \2)", t)
        return t

    def _parse(self, t):
        try:
            return Tree.parse(self._normalize(t))

--
            #sys.stderr.write(' '.join(t.split())+'\n')
            return Tree('S', self._tag(t))

    def _tag(self, t, tagset=None):
        tagged_sent = [(w,t) for (t,w) in TAGWORD.findall(self._normalize(t))]
        if tagset and tagset != self._tagset:
            tagged_sent = [(w, map_tag(self._tagset, tagset, t)) for (w,t) in tagged_sent]
        return tagged_sent

    def _word(self, t):
        return WORD.findall(self._normalize(t))

class CategorizedBracketParseCorpusReader(CategorizedCorpusReader,
--
    divided into categories based on their file identifiers.
    @author: Nathan Schneider <nschneid@cs.cmu.edu>
    """
    def __init__(self, *args, **kwargs):
        """
        Initialize the corpus reader.  Categorization arguments
        (C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to
--
        CategorizedCorpusReader.__init__(self, kwargs)
        BracketParseCorpusReader.__init__(self, *args, **kwargs)

    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
        if categories is not None:
            return self.fileids(categories)
        else:
            return fileids
    def raw(self, fileids=None, categories=None):
        return BracketParseCorpusReader.raw(
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
        return BracketParseCorpusReader.words(
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
        return BracketParseCorpusReader.sents(
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
        return BracketParseCorpusReader.paras(
            self, self._resolve(fileids, categories))
    def tagged_words(self, fileids=None, categories=None, tagset=None):
        return BracketParseCorpusReader.tagged_words(
            self, self._resolve(fileids, categories), tagset)
    def tagged_sents(self, fileids=None, categories=None, tagset=None):
        return BracketParseCorpusReader.tagged_sents(
            self, self._resolve(fileids, categories), tagset)
    def tagged_paras(self, fileids=None, categories=None, tagset=None):
        return BracketParseCorpusReader.tagged_paras(
            self, self._resolve(fileids, categories), tagset)
    def parsed_words(self, fileids=None, categories=None):
        return BracketParseCorpusReader.parsed_words(
            self, self._resolve(fileids, categories))
    def parsed_sents(self, fileids=None, categories=None):
        return BracketParseCorpusReader.parsed_sents(
            self, self._resolve(fileids, categories))
    def parsed_paras(self, fileids=None, categories=None):
        return BracketParseCorpusReader.parsed_paras(
            self, self._resolve(fileids, categories))

--
    """
    Reader for the Alpino Dutch Treebank.
    """
    def __init__(self, root, encoding='ISO-8859-1', tagset=None):
        BracketParseCorpusReader.__init__(self, root, 'alpino\.xml',
                                 detect_blocks='blankline',
                                 encoding=encoding,
                                 tagset=tagset)

    def _normalize(self, t):
        if t[:10] != "<alpino_ds":
            return ""
        # convert XML to sexpr notation
--
    Treebank with information about the predicate argument structure
    of every noun instance.  The corpus consists of two parts: the
    predicate-argument annotations themselves, and a set of "frameset
    files" which define the argument labels used by the annotations,
    on a per-noun basis.  Each "frameset file" contains one or more
    predicates, such as ``'turn'`` or ``'turn_on'``, each of which is
    divided into coarse-grained word senses called "rolesets".  For
    each "roleset", the frameset file provides descriptions of the
    argument roles, along with examples.
    """
    def __init__(self, root, nomfile, framefiles='',
                 nounsfile=None, parse_fileid_xform=None,
                 parse_corpus=None, encoding='utf8'):
        """
--
        self._parse_fileid_xform = parse_fileid_xform
        self._parse_corpus = parse_corpus

    def raw(self, fileids=None):
        """
        :return: the text contents of the given fileids, as a single string.
        """
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def instances(self, baseform=None):
        """
        :return: a corpus view that acts as a list of
        ``NombankInstance`` objects, one for each noun in the corpus.
--
                                      lambda stream: self._read_instance_block(stream, **kwargs),
                                      encoding=self.encoding(self._nomfile))

    def lines(self):
        """
        :return: a corpus view that acts as a list of strings, one for
        each line in the predicate-argument annotation file.
--
                                      read_line_block,
                                      encoding=self.encoding(self._nomfile))

    def roleset(self, roleset_id):
        """
        :return: the xml description for the given roleset.
        """
--
            raise ValueError('Roleset %s not found in %s' %
                             (roleset_id, framefile))

    def rolesets(self, baseform=None):
        """
        :return: list of xml descriptions for rolesets.
        """
--
            rsets.append(etree.findall('predicate/roleset'))
        return LazyConcatenation(rsets)

    def nouns(self):
        """
        :return: a corpus view that acts as a list of all noun lemmas
        in this corpus (from the nombank.1.0.words file).
--
                                      read_line_block,
                                      encoding=self.encoding(self._nounsfile))

    def _read_instance_block(self, stream, instance_filter=lambda inst: True):
        block = []

        # Read 100 at a time.
--
@python_2_unicode_compatible
class NombankInstance(object):

    def __init__(self, fileid, sentnum, wordnum, baseform, sensenumber,
                 predicate, predid, arguments, parse_corpus=None):

        self.fileid = fileid
--
        instances in this nombank corpus."""

    @property
    def roleset(self):
        """The name of the roleset used by this instance's predicate.
        Use ``nombank.roleset() <NombankCorpusReader.roleset>`` to
        look up information about the roleset."""
--
        r = r.replace('1/10', '1-slash-10').replace('1-slash-10', 'oneslashonezero')
        return '%s.%s'%(r, self.sensenumber)

    def __repr__(self):
        return ('<NombankInstance: %s, sent %s, word %s>' %
                (self.fileid, self.sentnum, self.wordnum))

    def __str__(self):
        s = '%s %s %s %s %s' % (self.fileid, self.sentnum, self.wordnum,
                                self.baseform, self.sensenumber)
        items = self.arguments + ((self.predicate, 'rel'),)
--
            s += ' %s-%s' % (argloc, argid)
        return s

    def _get_tree(self):
        if self.parse_corpus is None: return None
        if self.fileid not in self.parse_corpus.fileids(): return None
        return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
--
        the corresponding tree is not available.""")

    @staticmethod
    def parse(s, parse_fileid_xform=None, parse_corpus=None):
        pieces = s.split()
        if len(pieces) < 6:
            raise ValueError('Badly formatted nombank line: %r' % s)
--
      chains in a tree.  It consists of a sequence of pieces, which
      can be ``NombankTreePointer`` or ``NombankSplitTreePointer`` pointers.
    """
    def __init__(self):
        if self.__class__ == NombankPointer:
            raise NotImplementedError()

@python_2_unicode_compatible
class NombankChainTreePointer(NombankPointer):
    def __init__(self, pieces):
        self.pieces = pieces
        """A list of the pieces that make up this chain.  Elements may
           be either ``NombankSplitTreePointer`` or
           ``NombankTreePointer`` pointers."""

    def __str__(self):
        return '*'.join('%s' % p for p in self.pieces)
    def __repr__(self):
        return '<NombankChainTreePointer: %s>' % self
    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return Tree('*CHAIN*', [p.select(tree) for p in self.pieces])

@python_2_unicode_compatible
class NombankSplitTreePointer(NombankPointer):
    def __init__(self, pieces):
        self.pieces = pieces
        """A list of the pieces that make up this chain.  Elements are
           all ``NombankTreePointer`` pointers."""

    def __str__(self):
        return ','.join('%s' % p for p in self.pieces)
    def __repr__(self):
        return '<NombankSplitTreePointer: %s>' % self
    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return Tree('*SPLIT*', [p.select(tree) for p in self.pieces])

--
    wordnum:height,

    """
    def __init__(self, wordnum, height):
        self.wordnum = wordnum
        self.height = height

    @staticmethod
    def parse(s):
        # Deal with chains (xx*yy*zz)
        pieces = s.split('*')
        if len(pieces) > 1:
--
        if len(pieces) != 2: raise ValueError('bad nombank pointer %r' % s)
        return NombankTreePointer(int(pieces[0]), int(pieces[1]))

    def __str__(self):
        return '%s:%s' % (self.wordnum, self.height)

    def __repr__(self):
        return 'NombankTreePointer(%d, %d)' % (self.wordnum, self.height)

    def __eq__(self, other):
        while isinstance(other, (NombankChainTreePointer,
                                 NombankSplitTreePointer)):
            other = other.pieces[0]
--

        return (self.wordnum == other.wordnum and self.height == other.height)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        while isinstance(other, (NombankChainTreePointer,
                                 NombankSplitTreePointer)):
            other = other.pieces[0]
--

        return (self.wordnum, -self.height) < (other.wordnum, -other.height)

    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return tree[self.treepos(tree)]

    def treepos(self, tree):
        """
        Convert this pointer to a standard 'tree position' pointer,
        given that it points to the given tree.
--
    """
    List of words, one per line.  Blank lines are ignored.
    """
    def words(self, fileids=None):
        return line_tokenize(self.raw(fileids))

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])


class SwadeshCorpusReader(WordListCorpusReader):
    def entries(self, fileids=None):
        """
        :return: a tuple of words for the specified fileids.
        """
--
from .api import *

class ToolboxCorpusReader(CorpusReader):
    def xml(self, fileids, key=None):
        return concat([ToolboxData(path, enc).parse(key=key)
                       for (path, enc) in self.abspaths(fileids, True)])

    def fields(self, fileids, strip=True, unwrap=True, encoding='utf8',
               errors='strict', unicode_fields=None):
        return concat([list(ToolboxData(fileid,enc).fields(
                             strip, unwrap, encoding, errors, unicode_fields))
--
                       in self.abspaths(fileids, include_encoding=True)])

    # should probably be done lazily:
    def entries(self, fileids, **kwargs):
        if 'key' in kwargs:
            key = kwargs['key']
            del kwargs['key']
        else:
            key = 'lx'  # the default key in MDF
        entries = []
        for marker, contents in self.fields(fileids, **kwargs):
            if marker == key:
--
                    pass
        return entries

    def words(self, fileids, key='lx'):
        return [contents for marker, contents in self.fields(fileids) if marker == key]

    def raw(self, fileids):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])


def demo():
    pass

if __name__ == '__main__':
--
from nltk.corpus.reader.util import *
from nltk.corpus.reader.api import *

# default function to convert morphlist to str for tree representation
_morphs2str_default = lambda morphs: '/'.join(m[0] for m in morphs if m[0] != 'EOS')

class KNBCorpusReader(SyntaxCorpusReader):
    """
--
      tags = (surface, reading, lemma, pos1, posid1, pos2, posid2, pos3, posid3, others ...)
    """

    def __init__(self, root, fileids, encoding='utf8', morphs2str=_morphs2str_default):
        """
        Initialize KNBCorpusReader
        morphs2str is a function to convert morphlist to str for tree representation
--
        CorpusReader.__init__(self, root, fileids, encoding)
        self.morphs2str = morphs2str

    def _read_block(self, stream):
        # blocks are split by blankline (or EOF) - default
        return read_blankline_block(stream)

    def _word(self, t):
        res = []
        for line in t.splitlines():
            # ignore the Bunsets headers
--
        return res

    # ignores tagset argument
    def _tag(self, t, tagset=None):
        res = []
        for line in t.splitlines():
            # ignore the Bunsets headers
--

        return res

    def _parse(self, t):
        dg = DependencyGraph()
        i = 0
        for line in t.splitlines():
--
# Demo
######################################################################

def demo():

    import nltk
    from nltk.corpus.util import LazyCorpusLoader
--
    fileids = [f for f in find_corpus_fileids(FileSystemPathPointer(root), ".*")
               if re.search(r"\d\-\d\-[\d]+\-[\d]+", f)]

    def _knbc_fileids_sort(x):
        cells = x.split('-')
        return (cells[0], int(cells[1]), int(cells[2]), int(cells[3]))

--
    print('\n'.join( ' '.join("%s/%s"%(w[0], w[1].split(' ')[2]) for w in sent)
                     for sent in knbc.tagged_sents()[0:2] ))

def test():

    from nltk.corpus.util import LazyCorpusLoader

--
    """
    Reader for the sinica treebank.
    """
    def _read_block(self, stream):
        sent = stream.readline()
        sent = IDENTIFIER.sub('', sent)
        sent = APPENDIX.sub('', sent)
        return [sent]

    def _parse(self, sent):
        return sinica_parse(sent)

    def _tag(self, sent, tagset=None):
        tagged_sent = [(w,t) for (t,w) in TAGWORD.findall(sent)]
        if tagset and tagset != self._tagset:
            tagged_sent = [(w, map_tag(self._tagset, tagset, t)) for (w,t) in tagged_sent]
        return tagged_sent

    def _word(self, sent):
        return WORD.findall(sent)
--

@compat.python_2_unicode_compatible
class SensevalInstance(object):
    def __init__(self, word, position, context, senses):
        self.word = word
        self.senses = tuple(senses)
        self.position = position
        self.context = context

    def __repr__(self):
        return ('SensevalInstance(word=%r, position=%r, '
                'context=%r, senses=%r)' %
                (self.word, self.position, self.context, self.senses))


class SensevalCorpusReader(CorpusReader):
    def instances(self, fileids=None):
        return concat([SensevalCorpusView(fileid, enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def raw(self, fileids=None):
        """
        :return: the text contents of the given fileids, as a single string.
        """
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def _entry(self, tree):
        elts = []
        for lexelt in tree.findall('lexelt'):
            for inst in lexelt.findall('instance'):
--


class SensevalCorpusView(StreamBackedCorpusView):
    def __init__(self, fileid, encoding):
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)

        self._word_tokenizer = WhitespaceTokenizer()
        self._lexelt_starts = [0] # list of streampos
        self._lexelts = [None] # list of lexelt names

    def read_block(self, stream):
        # Decide which lexical element we're in.
        lexelt_num = bisect.bisect_right(self._lexelt_starts, stream.tell())-1
        lexelt = self._lexelts[lexelt_num]
--
                inst = ElementTree.fromstring(xml_block)
                return [self._parse_instance(inst, lexelt)]

    def _parse_instance(self, instance, lexelt):
        senses = []
        context = []
        position = None
--
                assert False, 'unexpected tag %s' % child.tag
        return SensevalInstance(lexelt, position, context, senses)

def _fixXML(text):
    """
    Fix the various issues with Senseval pseudo-XML.
    """
--
    zero or more tokens from a stream, and returns them as a list.  A
    very simple example of a block reader is:

        >>> def simple_block_reader(stream):
        ...     return stream.readline().split()

    This simple block reader reads a single line at a time, and
    returns a single token (consisting of a string) for each
    whitespace-separated substring on the line.

    When deciding how to define the block reader for a given
    corpus, careful consideration should be given to the size of
    blocks handled by the block reader.  Smaller block sizes will
    increase the memory requirements of the corpus view's internal
--

    :warning: If the contents of the file are modified during the
        lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior
        is undefined.

    :warning: If a unicode encoding is specified when constructing a
        ``CorpusView``, then the block reader may only call
--
       end_toknum is the token index of the first token not in the
       block; and tokens is a list of the tokens in the block.
    """
    def __init__(self, fileid, block_reader=None, startpos=0,
                 encoding='utf8'):
        """
        Create a new corpus view, based on the file ``fileid``, and
--

        :type: str or PathPointer""")

    def read_block(self, stream):
        """
        Read a block from the input stream.

--
        """
        raise NotImplementedError('Abstract Method')

    def _open(self):
        """
        Open the file stream associated with this corpus view.  This
        will be called performed if any value is read from the view
--
        else:
            self._stream = open(self._fileid, 'rb')

    def close(self):
        """
        Close the file stream associated with this corpus view.  This
        can be useful if you are worried about running out of file
--
            self._stream.close()
        self._stream = None

    def __len__(self):
        if self._len is None:
            # iterate_from() sets self._len when it reaches the end
            # of the file:
            for tok in self.iterate_from(self._toknum[-1]): pass
        return self._len

    def __getitem__(self, i):
        if isinstance(i, slice):
            start, stop = slice_bounds(self, i)
            # Check if it's in the cache.
--

    # If we wanted to be thread-safe, then this method would need to
    # do some locking.
    def iterate_from(self, start_tok):
        # Start by feeding from the cache, if possible.
        if self._cache[0] <= start_tok < self._cache[1]:
            for tok in self._cache[2][start_tok-self._cache[0]:]:
--

    # Use concat for these, so we can use a ConcatenatedCorpusView
    # when possible.
    def __add__(self, other):
        return concat([self, other])
    def __radd__(self, other):
        return concat([other, self])
    def __mul__(self, count):
        return concat([self] * count)
    def __rmul__(self, count):
        return concat([self] * count)

class ConcatenatedCorpusView(AbstractLazySequence):
--
    ``StreamBackedCorpusViews<StreamBackedCorpusView>``.  At most
    one file handle is left open at any time.
    """
    def __init__(self, corpus_views):
        self._pieces = corpus_views
        """A list of the corpus subviews that make up this
        concatenation."""
--
        """The most recently accessed corpus subview (or None).
        Before a new subview is accessed, this subview will be closed."""

    def __len__(self):
        if len(self._offsets) <= len(self._pieces):
            # Iterate to the end of the corpus.
            for tok in self.iterate_from(self._offsets[-1]): pass

        return self._offsets[-1]

    def close(self):
        for piece in self._pieces:
            piece.close()

    def iterate_from(self, start_tok):
        piecenum = bisect.bisect_right(self._offsets, start_tok)-1

        while piecenum < len(self._pieces):
--
            # Move on to the next piece.
            piecenum += 1

def concat(docs):
    """
    Concatenate together the contents of multiple documents from a
    single corpus, using an appropriate concatenation function.  This
--
    BLOCK_SIZE = 100
    PROTOCOL = -1

    def __init__(self, fileid, delete_on_gc=False):
        """
        Create a new corpus view that reads the pickle corpus
        ``fileid``.
--
        self._delete_on_gc = delete_on_gc
        StreamBackedCorpusView.__init__(self, fileid)

    def read_block(self, stream):
        result = []
        for i in range(self.BLOCK_SIZE):
            try: result.append(pickle.load(stream))
            except EOFError: break
        return result

    def __del__(self):
        """
        If ``delete_on_gc`` was set to true when this
        ``PickleCorpusView`` was created, then delete the corpus view's
--
        self.__dict__.clear() # make the garbage collector's job easier

    @classmethod
    def write(cls, sequence, output_file):
        if isinstance(output_file, compat.string_types):
            output_file = open(output_file, 'wb')
        for item in sequence:
            pickle.dump(item, output_file, cls.PROTOCOL)

    @classmethod
    def cache_to_tempfile(cls, sequence, delete_on_gc=True):
        """
        Write the given sequence to a temporary file as a pickle
        corpus; and then return a ``PickleCorpusView`` view for that
--
#{ Block Readers
######################################################################

def read_whitespace_block(stream):
    toks = []
    for i in range(20): # Read 20 lines at a time.
        toks.extend(stream.readline().split())
    return toks

def read_wordpunct_block(stream):
    toks = []
    for i in range(20): # Read 20 lines at a time.
        toks.extend(wordpunct_tokenize(stream.readline()))
    return toks

def read_line_block(stream):
    toks = []
    for i in range(20):
        line = stream.readline()
--
        toks.append(line.rstrip('\n'))
    return toks

def read_blankline_block(stream):
    s = ''
    while True:
        line = stream.readline()
--
        else:
            s += line

def read_alignedsent_block(stream):
    s = ''
    while True:
        line = stream.readline()
--
            if re.match('^\d+-\d+', line) is not None:
                return [s]

def read_regexp_block(stream, start_re, end_re=None):
    """
    Read a sequence of tokens from a stream, where tokens begin with
    lines that match ``start_re``.  If ``end_re`` is specified, then
--
        # Anything else is part of the token.
        lines.append(line)

def read_sexpr_block(stream, block_size=16384, comment_char=None):
    """
    Read a sequence of s-expressions from the stream, and leave the
    stream's file position at the end the last complete s-expression
--
    incomplete s-expression is returned when the end of the file is
    reached.

    :param block_size: The default block size for reading.  If an
        s-expression is longer than one block, then more than one
        block will be read.
    :param comment_char: A character that marks comments.  Any lines
--
                    return [block.strip()]
            else: raise

def _sub_space(m):
    """Helper function: given a regexp match, return a string of
    spaces that's the same length as the matched string."""
    return ' '*(m.end()-m.start())

def _parse_sexpr_block(block):
    tokens = []
    start = end = 0

--
#{ Finding Corpus Items
######################################################################

def find_corpus_fileids(root, regexp):
    if not isinstance(root, PathPointer):
        raise TypeError('find_corpus_fileids: expected a PathPointer')
    regexp += '$'
--
    else:
        raise AssertionError("Don't know how to handle %r" % root)

def _path_from(parent, child):
    if os.path.split(parent)[1] == '':
        parent = os.path.split(parent)[0]
    path = []
--
#{ Paragraph structure in Treebank files
######################################################################

def tagged_treebank_para_block_reader(stream):
    # Read the next paragraph.
    para = ''
    while True:
--
    ``encoding`` argument, because the unicode encoding is specified by
    the XML files themselves.  See the XML specs for more info.
    """
    def __init__(self, root, fileids, wrap_etree=False):
        self._wrap_etree = wrap_etree
        CorpusReader.__init__(self, root, fileids)

    def xml(self, fileid=None):
        # Make sure we have exactly one file -- no concatenating XML.
        if fileid is None and len(self._fileids) == 1:
            fileid = self._fileids[0]
--
        # Return the ElementTree element.
        return elt

    def words(self, fileid=None):
        """
        Returns all of the words and punctuation symbols in the specified file
        that were in text nodes -- ie, tags are ignored. Like the xml() method,
--
                out.extend(toks)
        return out

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])
--
        appearing anywhere in the xml tree.

    The view items are generated from the selected XML elements via
    the method ``handle_elt()``.  By default, this method returns the
    element as-is (i.e., as an ElementTree object); but it can be
    overridden, either via subclassing or via the ``elt_handler``
    constructor parameter.
--
    #: The number of characters read at a time by this corpus reader.
    _BLOCK_SIZE = 1024

    def __init__(self, fileid, tagspec, elt_handler=None):
        """
        Create a new corpus view based on a specified XML file.

--
        encoding = self._detect_encoding(fileid)
        StreamBackedCorpusView.__init__(self, fileid, encoding=encoding)

    def _detect_encoding(self, fileid):
        if isinstance(fileid, PathPointer):
            s = fileid.open().readline()
        else:
--
        m = re.match(br"\s*<\?xml\b.*\bencoding='([^']+)'", s)
        if m:
            return m.group(1).decode()
        # No encoding found -- what should the default be?
        return 'utf-8'

    def handle_elt(self, elt, context):
        """
        Convert an element into an appropriate value for inclusion in
        the view.  Unless overridden by a subclass or by the
--
        (?P<END_TAG>        <\s*/[^>/\?!\s][^>]*>               )""",
        re.DOTALL|re.VERBOSE)

    def _read_xml_fragment(self, stream):
        """
        Read a string from the given stream that does not contain any
        un-closed tags.  In particular, this function first reads a
--
            # Otherwise, read another block. (i.e., return to the
            # top of the loop.)

    def read_block(self, stream, tagspec=None, elt_handler=None):
        """
        Read from ``stream`` until we find at least one element that
        matches ``tagspec``, and return the result of applying
--
    ])


    def __init__(self, root='udhr'):
        fileids = find_corpus_fileids(root, r'(?!README|\.).*')
        super(UdhrCorpusReader, self).__init__(
            root,
--
from .util import StreamBackedCorpusView, concat
from .api import CorpusReader

def _parse_args(fun):
    @functools.wraps(fun)
    def decorator(self, fileids=None, **kwargs):
        kwargs.pop('tags', None)
        if not fileids:
            fileids = self.fileids()
--
    option, specify parameter "append_space=True", e.g. ``words(append_space=True)``.
    As a result either ' ' or (' ', 'space') will be inserted between tokens.

    By default, xml entities like &quot; and &amp; are replaced by corresponding
    characters. You can turn off this feature, specifying parameter
    "replace_xmlentities=False", e.g. ``words(replace_xmlentities=False)``.
    """

    def __init__(self, root, fileids):
        CorpusReader.__init__(self, root, fileids, None, None)

    def raw(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return ''.join([open(fileid, 'r').read()
            for fileid in self._list_morph_files(fileids)])

    def channels(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return self._parse_header(fileids, 'channel')

    def domains(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return self._parse_header(fileids, 'domain')

    def categories(self, fileids=None):
        if not fileids:
            fileids = self.fileids()
        return [self._map_category(cat)
                for cat in self._parse_header(fileids, 'keyTerm')]

    def fileids(self, channels=None, domains=None, categories=None):
        if channels is not None and domains is not None and \
                categories is not None:
            raise ValueError('You can specify only one of channels, domains '
--
                    map=self._map_category)

    @_parse_args
    def sents(self, fileids=None, **kwargs):
        return concat([self._view(fileid,
            mode=IPIPANCorpusView.SENTS_MODE, tags=False, **kwargs)
            for fileid in self._list_morph_files(fileids)])

    @_parse_args
    def paras(self, fileids=None, **kwargs):
        return concat([self._view(fileid,
            mode=IPIPANCorpusView.PARAS_MODE, tags=False, **kwargs)
            for fileid in self._list_morph_files(fileids)])

    @_parse_args
    def words(self, fileids=None, **kwargs):
        return concat([self._view(fileid, tags=False, **kwargs)
            for fileid in self._list_morph_files(fileids)])

    @_parse_args
    def tagged_sents(self, fileids=None, **kwargs):
        return concat([self._view(fileid, mode=IPIPANCorpusView.SENTS_MODE,
            **kwargs)
            for fileid in self._list_morph_files(fileids)])

    @_parse_args
    def tagged_paras(self, fileids=None, **kwargs):
        return concat([self._view(fileid, mode=IPIPANCorpusView.PARAS_MODE,
            **kwargs)
            for fileid in self._list_morph_files(fileids)])

    @_parse_args
    def tagged_words(self, fileids=None, **kwargs):
        return concat([self._view(fileid, **kwargs)
            for fileid in self._list_morph_files(fileids)])

    def _list_morph_files(self, fileids):
        return [f for f in self.abspaths(fileids)]

    def _list_header_files(self, fileids):
        return [f.replace('morph.xml', 'header.xml')
                for f in self._list_morph_files(fileids)]

    def _parse_header(self, fileids, tag):
        values = set()
        for f in self._list_header_files(fileids):
            values_list = self._get_tag(f, tag)
--
                values.add(v)
        return list(values)

    def _list_morph_files_by(self, tag, values, map=None):
        fileids = self.fileids()
        ret_fileids = set()
        for f in fileids:
--
                    ret_fileids.add(f)
        return list(ret_fileids)

    def _get_tag(self, f, tag):
        tags = []
        header = open(f, 'r').read()
        tag_end = 0
--
            tag_end = header.find('</'+tag+'>', tag_pos)
            tags.append(header[tag_pos+len(tag)+2:tag_end])

    def _map_category(self, cat):
        pos = cat.find('>')
        if pos == -1:
            return cat
        else:
            return cat[pos+1:]

    def _view(self, filename, **kwargs):
        tags = kwargs.pop('tags', True)
        mode = kwargs.pop('mode', 0)
        simplify_tags = kwargs.pop('simplify_tags', False)
--
    SENTS_MODE = 1
    PARAS_MODE = 2

    def __init__(self, filename, startpos=0, **kwargs):
        StreamBackedCorpusView.__init__(self, filename, None, startpos, None)
        self.in_sentence = False
        self.position = 0
--
        self.append_space = kwargs.pop('append_space', False)
        self.replace_xmlentities = kwargs.pop('replace_xmlentities', True)

    def read_block(self, stream):
        sentence = []
        sentences = []
        space = False
--
            elif line.startswith('</cesAna'):
                pass

    def _read_data(self, stream):
        self.position = stream.tell()
        buff = stream.read(4096)
        lines = buff.split('\n')
        lines.reverse()
        return lines

    def _seek(self, stream):
        stream.seek(self.position)

    def _append_space(self, sentence):
        if self.show_tags:
            sentence.append((' ', 'space'))
        else:
--
   Each record is a dictionary from field names to values, and the fields are
   as follows::

     id         speaker ID as defined in the original TIMIT speaker info table
     sex        speaker gender (M:male, F:female)
     dr         speaker dialect region (1:new england, 2:northern,
                3:north midland, 4:south midland, 5:southern, 6:new york city,
--
    """A regexp matching fileids that are used by this corpus reader."""
    _UTTERANCE_RE = r'\w+-\w+/\w+\.txt'

    def __init__(self, root, encoding='utf8'):
        """
        Construct a new TIMIT corpus reader in the given directory.
        :param root: The root directory for this corpus.
--
        self._root = root
        self.speakers = sorted(set(u.split('/')[0] for u in self._utterances))

    def fileids(self, filetype=None):
        """
        Return a list of file identifiers for the files that make up
        this corpus.
--
        else:
            raise ValueError('Bad value for filetype: %r' % filetype)

    def utteranceids(self, dialect=None, sex=None, spkrid=None,
                   sent_type=None, sentid=None):
        """
        :return: A list of the utterance identifiers for all
--
            utterances = [u for u in utterances if u[10:] in spkrid]
        return utterances

    def transcription_dict(self):
        """
        :return: A dictionary giving the 'standard' transcription for
        each word.
--
            _transcriptions[m.group(1)] = m.group(2).split()
        return _transcriptions

    def spkrid(self, utterance):
        return utterance.split('/')[0]

    def sentid(self, utterance):
        return utterance.split('/')[1]

    def utterance(self, spkrid, sentid):
        return '%s/%s' % (spkrid, sentid)

    def spkrutteranceids(self, speaker):
        """
        :return: A list of all utterances associated with a given
        speaker.
--
        return [utterance for utterance in self._utterances
                if utterance.startswith(speaker+'/')]

    def spkrinfo(self, speaker):
        """
        :return: A dictionary mapping .. something.
        """
--

        return self._speakerinfo[speaker]

    def phones(self, utterances=None):
        return [line.split()[-1]
                for fileid in self._utterance_fileids(utterances, '.phn')
                for line in self.open(fileid) if line.strip()]

    def phone_times(self, utterances=None):
        """
        offset is represented as a number of 16kHz samples!
        """
--
                for fileid in self._utterance_fileids(utterances, '.phn')
                for line in self.open(fileid) if line.strip()]

    def words(self, utterances=None):
        return [line.split()[-1]
                for fileid in self._utterance_fileids(utterances, '.wrd')
                for line in self.open(fileid) if line.strip()]

    def word_times(self, utterances=None):
        return [(line.split()[2], int(line.split()[0]), int(line.split()[1]))
                for fileid in self._utterance_fileids(utterances, '.wrd')
                for line in self.open(fileid) if line.strip()]

    def sents(self, utterances=None):
        return [[line.split()[-1]
                 for line in self.open(fileid) if line.strip()]
                for fileid in self._utterance_fileids(utterances, '.wrd')]

    def sent_times(self, utterances=None):
        return [(line.split(None,2)[-1].strip(),
                 int(line.split()[0]), int(line.split()[1]))
                for fileid in self._utterance_fileids(utterances, '.txt')
                for line in self.open(fileid) if line.strip()]

    def phone_trees(self, utterances=None):
        if utterances is None: utterances = self._utterances
        if isinstance(utterances, compat.string_types): utterances = [utterances]

--
    # [xx] NOTE: This is currently broken -- we're assuming that the
    # fileids are WAV fileids (aka RIFF), but they're actually NIST SPHERE
    # fileids.
    def wav(self, utterance, start=0, end=None):
        # nltk.chunk conflicts with the stdlib module 'chunk'
        wave = import_from_stdlib('wave')

--
        tf.seek(0)
        return tf.read()

    def audiodata(self, utterance, start=0, end=None):
        assert(end is None or end > start)
        headersize = 44
        if end is None:
--
            data = self.open(utterance+'.wav').read(headersize+end*2)
        return data[headersize+start*2:]

    def _utterance_fileids(self, utterances, extension):
        if utterances is None: utterances = self._utterances
        if isinstance(utterances, compat.string_types): utterances = [utterances]
        return ['%s%s' % (u, extension) for u in utterances]

    def play(self, utterance, start=0, end=None):
        """
        Play the given audio sample.

--

@compat.python_2_unicode_compatible
class SpeakerInfo(object):
    def __init__(self, id, sex, dr, use, recdate, birthdate,
                 ht, race, edu, comments=None):
        self.id = id
        self.sex = sex
--
        self.edu = edu
        self.comments = comments

    def __repr__(self):
        attribs = 'id sex dr use recdate birthdate ht race edu comments'
        args = ['%s=%r' % (attr, getattr(self, attr))
                for attr in attribs.split()]
        return 'SpeakerInfo(%s)' % (', '.join(args))


def read_timit_block(stream):
    """
    Block reader for timit tagged sentences, which are preceded by a sentence
    number that will be ignored.
--
    spearker identifier and utterance id.  Note that utterance ids
    are only unique within a given discourse.
    """
    def __init__(self, words, speaker, id):
        list.__init__(self, words)
        self.speaker = speaker
        self.id = int(id)

    def __repr__(self):
        if len(self) == 0:
            text = ''
        elif isinstance(self[0], tuple):
--
    # Use the "tagged" file even for non-tagged data methods, since
    # it's tokenized.

    def __init__(self, root, tagset=None):
        CorpusReader.__init__(self, root, self._FILES)
        self._tagset = tagset

    def words(self):
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      self._words_block_reader)

    def tagged_words(self, tagset=None):
        def tagged_words_block_reader(stream):
            return self._tagged_words_block_reader(stream, tagset)
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      tagged_words_block_reader)

    def turns(self):
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      self._turns_block_reader)

    def tagged_turns(self, tagset=None):
        def tagged_turns_block_reader(stream):
            return self._tagged_turns_block_reader(stream, tagset)
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      tagged_turns_block_reader)

    def discourses(self):
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      self._discourses_block_reader)

    def tagged_discourses(self, tagset=False):
        def tagged_discourses_block_reader(stream):
            return self._tagged_discourses_block_reader(stream, tagset)
        return StreamBackedCorpusView(self.abspath('tagged'),
                                      tagged_discourses_block_reader)

    def _discourses_block_reader(self, stream):
        # returns at most 1 discourse.  (The other methods depend on this.)
        return [[self._parse_utterance(u, include_tag=False)
                 for b in read_blankline_block(stream)
                 for u in b.split('\n') if u.strip()]]

    def _tagged_discourses_block_reader(self, stream, tagset=None):
        # returns at most 1 discourse.  (The other methods depend on this.)
        return [[self._parse_utterance(u, include_tag=True,
                                       tagset=tagset)
                 for b in read_blankline_block(stream)
                 for u in b.split('\n') if u.strip()]]

    def _turns_block_reader(self, stream):
        return self._discourses_block_reader(stream)[0]

    def _tagged_turns_block_reader(self, stream, tagset=None):
        return self._tagged_discourses_block_reader(stream, tagset)[0]

    def _words_block_reader(self, stream):
        return sum(self._discourses_block_reader(stream)[0], [])

    def _tagged_words_block_reader(self, stream, tagset=None):
        return sum(self._tagged_discourses_block_reader(stream,
                                                        tagset)[0], [])

    _UTTERANCE_RE = re.compile('(\w+)\.(\d+)\:\s*(.*)')
    _SEP = '/'
    def _parse_utterance(self, utterance, include_tag, tagset=None):
        m = self._UTTERANCE_RE.match(utterance)
        if m is None:
            raise ValueError('Bad utterance %r' % utterance)
--

class ChasenCorpusReader(CorpusReader):

    def __init__(self, root, fileids, encoding='utf8', sent_splitter=None):
        self._sent_splitter = sent_splitter
        CorpusReader.__init__(self, root, fileids, encoding)

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        False, False, False, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_words(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        True, False, False, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        False, True, False, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_sents(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        True, True, False, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])

    def paras(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        False, True, True, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_paras(self, fileids=None):
        return concat([ChasenCorpusView(fileid, enc,
                                        True, True, True, self._sent_splitter)
            for (fileid, enc) in self.abspaths(fileids, True)])
--
    but this'll use fixed sets of word and sentence tokenizer.
    """

    def __init__(self, corpus_file, encoding,
                 tagged, group_by_sent, group_by_para, sent_splitter=None):
        self._tagged = tagged
        self._group_by_sent = group_by_sent
--
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)


    def read_block(self, stream):
        """Reads one paragraph at a time."""
        block = []
        for para_str in read_regexp_block(stream, r".", r"^EOS\n"):
--

        return block

def demo():

    import nltk
    from nltk.corpus.util import LazyCorpusLoader
--
    print('\nEOS\n'.join('\n'.join("%s/%s" % (w[0],w[1].split('\t')[2]) for w in sent)
                          for sent in jeita.tagged_sents()[2170:2173]))

def test():

    from nltk.corpus.util import LazyCorpusLoader

--
import re
from itertools import islice, chain
from operator import itemgetter, attrgetter
from collections import defaultdict

from nltk.corpus.reader import CorpusReader
from nltk.util import binary_search_file as _binary_search_file
--
class _WordNetObject(object):
    """A common base class for lemmas and synsets."""

    def hypernyms(self):
        return self._related('@')

    def instance_hypernyms(self):
        return self._related('@i')

    def hyponyms(self):
        return self._related('~')

    def instance_hyponyms(self):
        return self._related('~i')

    def member_holonyms(self):
        return self._related('#m')

    def substance_holonyms(self):
        return self._related('#s')

    def part_holonyms(self):
        return self._related('#p')

    def member_meronyms(self):
        return self._related('%m')

    def substance_meronyms(self):
        return self._related('%s')

    def part_meronyms(self):
        return self._related('%p')

    def topic_domains(self):
        return self._related(';c')

    def region_domains(self):
        return self._related(';r')

    def usage_domains(self):
        return self._related(';u')

    def attributes(self):
        return self._related('=')

    def entailments(self):
        return self._related('*')

    def causes(self):
        return self._related('>')

    def also_sees(self):
        return self._related('^')

    def verb_groups(self):
        return self._related('$')

    def similar_tos(self):
        return self._related('&')

    def __hash__(self):
        return hash(self.name)

    def __eq__(self, other):
        return self.name == other.name

    def __ne__(self, other):
        return self.name != other.name

    def __lt__(self, other):
        return self.name < other.name


--
    Lemma methods:

    Lemmas have the following methods for retrieving related Lemmas. They
    correspond to the names for the pointer symbols defined here:
    http://wordnet.princeton.edu/man/wninput.5WN.html#sect3
    These methods all return lists of Lemmas:

--
                 '_lexname_index', '_lex_id', 'key']

    # formerly _from_synset_info
    def __init__(self, wordnet_corpus_reader, synset, name,
                 lexname_index, lex_id, syntactic_marker):
        self._wordnet_corpus_reader = wordnet_corpus_reader
        self.name = name
--

        self.key = None # gets set later.

    def __repr__(self):
        tup = type(self).__name__, self.synset.name, self.name
        return "%s('%s.%s')" % tup

    def _related(self, relation_symbol):
        get_synset = self._wordnet_corpus_reader._synset_from_pos_and_offset
        return sorted([get_synset(pos, offset).lemmas[lemma_index]
                for pos, offset, lemma_index
                in self.synset._lemma_pointers[self.name, relation_symbol]])

    def count(self):
        """Return the frequency count for this Lemma"""
        return self._wordnet_corpus_reader.lemma_count(self)

    def antonyms(self):
        return self._related('!')

    def derivationally_related_forms(self):
        return self._related('+')

    def pertainyms(self):
        return self._related('\\')


--
    - pos: The synset's part of speech, matching one of the module level
      attributes ADJ, ADJ_SAT, ADV, NOUN or VERB.
    - lemmas: A list of the Lemma objects for this synset.
    - definition: The definition for this synset.
    - examples: A list of example strings for this synset.
    - offset: The offset in the WordNet dict file of this synset.
    - #lexname: The name of the lexicographer file containing this synset.
--
    Synset methods:

    Synsets have the following methods for retrieving related Synsets.
    They correspond to the names for the pointer symbols defined here:
    http://wordnet.princeton.edu/man/wninput.5WN.html#sect3
    These methods all return lists of Synsets.

--
    - lowest_common_hypernyms

    Note that Synsets do not support the following relations because
    these are defined by WordNet as lexical relations:

    - antonyms
    - derivationally_related_forms
--

    __slots__ = ['pos', 'offset', 'name', 'frame_ids',
                 'lemmas', 'lemma_names',
                 'definition', 'examples', 'lexname',
                 '_pointers', '_lemma_pointers', '_max_depth',
                 '_min_depth', ]

    def __init__(self, wordnet_corpus_reader):
        self._wordnet_corpus_reader = wordnet_corpus_reader
        # All of these attributes get initialized by
        # WordNetCorpusReader._synset_from_pos_and_line()
--
        self.frame_ids = []
        self.lemmas = []
        self.lemma_names = []
        self.definition = None
        self.examples = []
        self.lexname = None # lexicographer name

        self._pointers = defaultdict(set)
        self._lemma_pointers = defaultdict(set)

    def _needs_root(self):
        if self.pos == NOUN:
            if self._wordnet_corpus_reader.get_version() == '1.6':
                return True
--
        elif self.pos == VERB:
            return True

    def root_hypernyms(self):
        """Get the topmost hypernyms of this synset in WordNet."""

        result = []
--
#        else:
#            return list(set(root for h in self.hypernyms()
#                            for root in h.root_hypernyms()))
    def max_depth(self):
        """
        :return: The length of the longest hypernym path from this
        synset to the root.
--
                self._max_depth = 1 + max(h.max_depth() for h in hypernyms)
        return self._max_depth

    def min_depth(self):
        """
        :return: The length of the shortest hypernym path from this
        synset to the root.
--
                self._min_depth = 1 + min(h.min_depth() for h in hypernyms)
        return self._min_depth

    def closure(self, rel, depth=-1):
        """Return the transitive closure of source under the rel
        relationship, breadth-first

--
                    synset_offsets.append(synset.offset)
                    yield synset

    def hypernym_paths(self):
        """
        Get the path(s) from this synset to the root, where each path is a
        list of the synset nodes traversed on the way to the root.
--
                paths.append(ancestor_list)
        return paths

    def common_hypernyms(self, other):
        """
        Find all synsets that are hypernyms of this synset and the
        other synset.
--
                           for other_synset in other_synsets)
        return list(self_synsets.intersection(other_synsets))

    def lowest_common_hypernyms(self, other, simulate_root=False, use_min_depth=False):
        """
        Get a list of lowest synset(s) that both synsets have as a hypernym.
        When `use_min_depth == False` this means that the synset which appears as a
--
        :type simulate_root: bool
        :param simulate_root: The various verb taxonomies do not
            share a single root which disallows this metric from working for
            synsets that are not connected. This flag (False by default)
            creates a fake root that connects all the taxonomies. Set it
            to True to enable this behavior. For the noun taxonomy,
            there is usually a default root except for WordNet version 1.6.
            If you are using wordnet 1.6, a fake root will need to be added
            for nouns as well.
        :type use_min_depth: bool
--
        except ValueError:
            return []

    def hypernym_distances(self, distance=0, simulate_root=False):
        """
        Get the path(s) from this synset to the root, counting the distance
        of each node from the initial node on the way. A set of
--
            distances.add((fake_synset, fake_synset_distance+1))
        return distances

    def shortest_path_distance(self, other, simulate_root=False):
        """
        Returns the distance of the shortest path linking the two synsets (if
        one exists). For each synset, all the ancestor nodes and their
--

        return path_distance

    def tree(self, rel, depth=-1, cut_mark=None):
        """
        >>> from nltk.corpus import wordnet as wn
        >>> dog = wn.synset('dog.n.01')
--
        return tree

    # interface to similarity methods
    def path_similarity(self, other, verbose=False, simulate_root=True):
        """
        Path Distance Similarity:
        Return a score denoting how similar two word senses are, based on the
--
        :type simulate_root: bool
        :param simulate_root: The various verb taxonomies do not
            share a single root which disallows this metric from working for
            synsets that are not connected. This flag (True by default)
            creates a fake root that connects all the taxonomies. Set it
            to false to disable this behavior. For the noun taxonomy,
            there is usually a default root except for WordNet version 1.6.
            If you are using wordnet 1.6, a fake root will be added for nouns
            as well.
        :return: A score denoting the similarity of the two ``Synset`` objects,
--
            return None
        return 1.0 / (distance + 1)

    def lch_similarity(self, other, verbose=False, simulate_root=True):
        """
        Leacock Chodorow Similarity:
        Return a score denoting how similar two word senses are, based on the
--
        :type simulate_root: bool
        :param simulate_root: The various verb taxonomies do not
            share a single root which disallows this metric from working for
            synsets that are not connected. This flag (True by default)
            creates a fake root that connects all the taxonomies. Set it
            to false to disable this behavior. For the noun taxonomy,
            there is usually a default root except for WordNet version 1.6.
            If you are using wordnet 1.6, a fake root will be added for nouns
            as well.
        :return: A score denoting the similarity of the two ``Synset`` objects,
--
            return None
        return -math.log((distance + 1) / (2.0 * depth))

    def wup_similarity(self, other, verbose=False, simulate_root=True):
        """
        Wu-Palmer Similarity:
        Return a score denoting how similar two word senses are, based on the
--
        verbs now almost always agree but not always for nouns.

        The LCS does not necessarily feature in the shortest path connecting
        the two senses, as it is by definition the common ancestor deepest in
        the taxonomy, not closest to the two senses. Typically, however, it
        will so feature. Where multiple candidates for the LCS exist, that
        whose shortest path to the root node is the longest will be selected.
--
        :type simulate_root: bool
        :param simulate_root: The various verb taxonomies do not
            share a single root which disallows this metric from working for
            synsets that are not connected. This flag (True by default)
            creates a fake root that connects all the taxonomies. Set it
            to false to disable this behavior. For the noun taxonomy,
            there is usually a default root except for WordNet version 1.6.
            If you are using wordnet 1.6, a fake root will be added for nouns
            as well.
        :return: A float score denoting the similarity of the two ``Synset`` objects,
--
        len2 += depth
        return (2.0 * depth) / (len1 + len2)

    def res_similarity(self, other, ic, verbose=False):
        """
        Resnik Similarity:
        Return a score denoting how similar two word senses are, based on the
--
        ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)
        return lcs_ic

    def jcn_similarity(self, other, ic, verbose=False):
        """
        Jiang-Conrath Similarity:
        Return a score denoting how similar two word senses are, based on the
--

        return 1 / ic_difference

    def lin_similarity(self, other, ic, verbose=False):
        """
        Lin Similarity:
        Return a score denoting how similar two word senses are, based on the
--
        ic1, ic2, lcs_ic = _lcs_ic(self, other, ic)
        return (2.0 * lcs_ic) / (ic1 + ic2)

    def _iter_hypernym_lists(self):
        """
        :return: An iterator over ``Synset`` objects that are either proper
        hypernyms or instance of hypernyms of the synset.
--
                        synset.instance_hypernyms())
                    if hypernym not in seen]

    def __repr__(self):
        return "%s('%s')" % (type(self).__name__, self.name)

    def _related(self, relation_symbol):
        get_synset = self._wordnet_corpus_reader._synset_from_pos_and_offset
        pointer_tuples = self._pointers[relation_symbol]
        return sorted([get_synset(pos, offset) for pos, offset in pointer_tuples])
--
              'data.adj', 'data.adv', 'data.noun', 'data.verb',
              'adj.exc', 'adv.exc', 'noun.exc', 'verb.exc', )

    def __init__(self, root):
        """
        Construct a new wordnet corpus reader, with the given root
        directory.
--
        super(WordNetCorpusReader, self).__init__(root, self._FILES,
                                                  encoding=self._ENCODING)

        self._lemma_pos_offset_map = defaultdict(dict)
        """A index that provides the file offset

        Map from lemma -> pos -> synset_index -> offset"""

        self._synset_offset_cache = defaultdict(dict)
        """A cache so we don't have to reconstuct synsets

        Map from pos -> offset -> synset"""

        self._max_depth = defaultdict(dict)
        """A lookup for the maximum depth of each part of speech.  Useful for
        the lch similarity metric.
        """
--
        self._load_exception_map()


    def _load_lemma_pos_offset_map(self):
        for suffix in self._FILEMAP.values():

            # parse each line of the file (ignoring comment lines)
--
                if pos == ADJ:
                    self._lemma_pos_offset_map[lemma][ADJ_SAT] = synset_offsets

    def _load_exception_map(self):
        # load the exception file data into memory
        for pos, suffix in self._FILEMAP.items():
            self._exception_map[pos] = {}
--
                self._exception_map[pos][terms[0]] = terms[1:]
        self._exception_map[ADJ_SAT] = self._exception_map[ADJ]

    def _compute_max_depth(self, pos, simulate_root):
        """
        Compute the max depth for the given part of speech.  This is
        used by the lch similarity metric.
--
            depth += 1
        self._max_depth[pos] = depth

    def get_version(self):
        fh = self._data_file(ADJ)
        for line in fh:
            match = re.search(r'WordNet (\d+\.\d+) Copyright', line)
--
    #////////////////////////////////////////////////////////////
    # Loading Lemmas
    #////////////////////////////////////////////////////////////
    def lemma(self, name):
        # e.g.: '.45_caliber.a.01..45_caliber'
        separator = SENSENUM_RE.search(name).start()
        synset_name, lemma_name = name[:separator+3], name[separator+4:]
--
                return lemma
        raise WordNetError('no lemma %r in %r' % (lemma_name, synset_name))

    def lemma_from_key(self, key):
        # Keys are case sensitive and always lower-case
        key = key.lower()

--
    #////////////////////////////////////////////////////////////
    # Loading Synsets
    #////////////////////////////////////////////////////////////
    def synset(self, name):
        # split name into lemma, part of speech and synset number
        lemma, pos, synset_index_str = name.lower().rsplit('.', 2)
        synset_index = int(synset_index_str) - 1
--
        # Return the synset object.
        return synset

    def _data_file(self, pos):
        """
        Return an open file pointer for the data file for the given
        part of speech.
--
            self._data_file_map[pos] = self.open(fileid)
        return self._data_file_map[pos]

    def _synset_from_pos_and_offset(self, pos, offset):
        # Check to see if the synset is in the cache
        if offset in self._synset_offset_cache[pos]:
            return self._synset_offset_cache[pos][offset]
--
        self._synset_offset_cache[pos][offset] = synset
        return synset

    def _synset_from_pos_and_line(self, pos, data_file_line):
        # Construct a new (empty) synset.
        synset = Synset(self)

        # parse the entry for this synset
        try:

            # parse out the definitions and examples from the gloss
            columns_str, gloss = data_file_line.split('|')
            gloss = gloss.strip()
            definitions = []
            for gloss_part in gloss.split(';'):
                gloss_part = gloss_part.strip()
                if gloss_part.startswith('"'):
                    synset.examples.append(gloss_part.strip('"'))
                else:
                    definitions.append(gloss_part)
            synset.definition = '; '.join(definitions)

            # split the other info into fields
            _iter = iter(columns_str.split())
--
    #////////////////////////////////////////////////////////////
    # Retrieve synsets and lemmas.
    #////////////////////////////////////////////////////////////
    def synsets(self, lemma, pos=None):
        """Load all synsets with a given lemma and part of speech tag.
        If no pos is specified, all synsets for all parts of speech
        will be loaded.
--
                for form in self._morphy(lemma, p)
                for offset in index[form].get(p, [])]

    def lemmas(self, lemma, pos=None):
        """Return all Lemma objects with a name matching the specified lemma
        name and part of speech tag. Matches any part of speech tag if none is
        specified."""
--
                for lemma_obj in synset.lemmas
                if lemma_obj.name.lower() == lemma]

    def all_lemma_names(self, pos=None):
        """Return all lemma names for all synsets for the given
        part of speech tag. If pos is not specified, all synsets
        for all parts of speech will be used.
--
                for lemma in self._lemma_pos_offset_map
                if pos in self._lemma_pos_offset_map[lemma])

    def all_synsets(self, pos=None):
        """Iterate over all synsets with a given part of speech tag.
        If no pos is specified, all synsets for all parts of speech
        will be loaded.
--
        for pos_tag in pos_tags:
            # Open the file for reading.  Note that we can not re-use
            # the file poitners from self._data_file_map here, because
            # we're defining an iterator, and those file pointers might
            # be moved while we're not looking.
            if pos_tag == ADJ_SAT:
                pos_tag = ADJ
--
    #////////////////////////////////////////////////////////////
    # Misc
    #////////////////////////////////////////////////////////////
    def lemma_count(self, lemma):
        """Return the frequency count for this Lemma"""
        # open the count file if we haven't already
        if self._key_count_file is None:
--
        else:
            return 0

    def path_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
        return synset1.path_similarity(synset2, verbose, simulate_root)
    path_similarity.__doc__ = Synset.path_similarity.__doc__

    def lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
        return synset1.lch_similarity(synset2, verbose, simulate_root)
    lch_similarity.__doc__ = Synset.lch_similarity.__doc__

    def wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True):
        return synset1.wup_similarity(synset2, verbose, simulate_root)
    wup_similarity.__doc__ = Synset.wup_similarity.__doc__

    def res_similarity(self, synset1, synset2, ic, verbose=False):
        return synset1.res_similarity(synset2, ic, verbose)
    res_similarity.__doc__ = Synset.res_similarity.__doc__

    def jcn_similarity(self, synset1, synset2, ic, verbose=False):
        return synset1.jcn_similarity(synset2, ic, verbose)
    jcn_similarity.__doc__ = Synset.jcn_similarity.__doc__

    def lin_similarity(self, synset1, synset2, ic, verbose=False):
        return synset1.lin_similarity(synset2, ic, verbose)
    lin_similarity.__doc__ = Synset.lin_similarity.__doc__

--
    # Morphy
    #////////////////////////////////////////////////////////////
    # Morphy, adapted from Oliver Steele's pywordnet
    def morphy(self, form, pos=None):
        """
        Find a possible base form for the given form, with the given
        part of speech, by checking WordNet's list of exceptional
--
        ADJ: [('er', ''), ('est', ''), ('er', 'e'), ('est', 'e')],
        ADV: []}

    def _morphy(self, form, pos):
        # from jordanbg:
        # Given an original string x
        # 1. Apply rules once to the input to get y1, y2, y3, etc.
--
        exceptions = self._exception_map[pos]
        substitutions = self.MORPHOLOGICAL_SUBSTITUTIONS[pos]

        def apply_rules(forms):
            return [form[:-len(old)] + new
                    for form in forms
                    for old, new in substitutions
                    if form.endswith(old)]

        def filter_forms(forms):
            result = []
            seen = set()
            for form in forms:
--
    #////////////////////////////////////////////////////////////
    # Create information content from corpus
    #////////////////////////////////////////////////////////////
    def ic(self, corpus, weight_senses_equally = False, smoothing = 1.0):
        """
        Creates an information content lookup dictionary from a corpus.

--
        number of possible senses.  (If a word has 3 synses, each
        sense gets 0.3333 per appearance when this is False, 1.0 when
        it is true.)
        :param smoothing: How much do we smooth synset counts (default is 1.0)
        :type smoothing: float
        :return: An information content dictionary
        """
--

        ic = {}
        for pp in POS_LIST:
            ic[pp] = defaultdict(float)

        # Initialize the counts with the smoothing value
        if smoothing > 0.0:
--
    A corpus reader for the WordNet information content corpus.
    """

    def __init__(self, root, fileids):
        CorpusReader.__init__(self, root, fileids, encoding='utf8')

    # this load function would be more efficient if the data was pickled
    # Note that we can't use NLTK's frequency distributions because
    # synsets are overlapping (each instance of a synset also counts
    # as an instance of its hypernyms)
    def ic(self, icfile):
        """
        Load an information content file from the wordnet_ic corpus
        and return a dictionary.  This dictionary has just two keys,
--
        :return: An information content dictionary
        """
        ic = {}
        ic[NOUN] = defaultdict(float)
        ic[VERB] = defaultdict(float)
        for num, line in enumerate(self.open(icfile)):
            if num == 0: # skip the header
                continue
--
# More information about the metrics is available at
# http://marimba.d.umn.edu/similarity/measures.html

def path_similarity(synset1, synset2, verbose=False, simulate_root=True):
    return synset1.path_similarity(synset2, verbose, simulate_root)
path_similarity.__doc__ = Synset.path_similarity.__doc__


def lch_similarity(synset1, synset2, verbose=False, simulate_root=True):
    return synset1.lch_similarity(synset2, verbose, simulate_root)
lch_similarity.__doc__ = Synset.lch_similarity.__doc__


def wup_similarity(synset1, synset2, verbose=False, simulate_root=True):
    return synset1.wup_similarity(synset2, verbose, simulate_root)
wup_similarity.__doc__ = Synset.wup_similarity.__doc__


def res_similarity(synset1, synset2, ic, verbose=False):
    return synset1.res_similarity(synset2, verbose)
res_similarity.__doc__ = Synset.res_similarity.__doc__


def jcn_similarity(synset1, synset2, ic, verbose=False):
    return synset1.jcn_similarity(synset2, verbose)
jcn_similarity.__doc__ = Synset.jcn_similarity.__doc__


def lin_similarity(synset1, synset2, ic, verbose=False):
    return synset1.lin_similarity(synset2, verbose)
lin_similarity.__doc__ = Synset.lin_similarity.__doc__


def _lcs_ic(synset1, synset2, ic, verbose=False):
    """
    Get the information content of the least common subsumer that has
    the highest information content value.  If two nodes have no
--

# Utility functions

def information_content(synset, ic):
    try:
        icpos = ic[synset.pos]
    except KeyError:
--
# get the part of speech (NOUN or VERB) from the information content record
# (each identifier has a 'n' or 'v' suffix)

def _get_pos(field):
    if field[-1] == 'n':
        return NOUN
    elif field[-1] == 'v':
--


# unload corpus after tests
def teardown_module(module=None):
    from nltk.corpus import wordnet
    wordnet._unload()

--
# Demo
######################################################################

def demo():
    import nltk
    print('loading wordnet')
    wn = WordNetCorpusReader(nltk.data.find('corpora/wordnet'))
--
    move_synset = S('go.v.21')
    print(move_synset.name, move_synset.pos, move_synset.lexname)
    print(move_synset.lemma_names)
    print(move_synset.definition)
    print(move_synset.examples)

    zap_n = ['zap.n.01']
    zap_v = ['zap.v.01', 'zap.v.02', 'nuke.v.01', 'microwave.v.01']

    def _get_synsets(synset_strings):
        return [S(synset) for synset in synset_strings]

    zap_n_synsets = _get_synsets(zap_n)
--
from .api import *

class CMUDictCorpusReader(CorpusReader):
    def entries(self):
        """
        :return: the cmudict lexicon as a list of entries
        containing (word, transcriptions) tuples.
--
                                              encoding=enc)
                       for fileid, enc in self.abspaths(None, True)])

    def raw(self):
        """
        :return: the cmudict lexicon as a raw string.
        """
--
            fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self):
        """
        :return: a list of all words defined in the cmudict lexicon.
        """
        return [word.lower() for (word, _) in self.entries()]

    def dict(self):
        """
        :return: the cmudict lexicon as a dictionary, whose keys are
        lowercase words and whose values are lists of pronunciations.
        """
        return dict(Index(self.entries()))

def read_cmudict_block(stream):
    entries = []
    while len(entries) < 100: # Read 100 at a time.
        line = stream.readline()
--
        methods (eg words() and parsed_sents() could both share the
        same grid corpus view object).
    @todo: Better support for -DOCSTART-.  Currently, we just ignore
        it, but it could be used to define methods that retrieve a
        document at a time (eg parsed_documents()).
    """

--
    # Constructor
    #/////////////////////////////////////////////////////////////////

    def __init__(self, root, fileids, columntypes,
                 chunk_types=None, top_node='S', pos_in_tree=False,
                 srl_includes_roleset=True, encoding='utf8',
                 tree_class=Tree, tagset=None):
--
    # Data Access Methods
    #/////////////////////////////////////////////////////////////////

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        self._require(self.WORDS)
        return LazyConcatenation(LazyMap(self._get_words, self._grids(fileids)))

    def sents(self, fileids=None):
        self._require(self.WORDS)
        return LazyMap(self._get_words, self._grids(fileids))

    def tagged_words(self, fileids=None, tagset=None):
        self._require(self.WORDS, self.POS)
        def get_tagged_words(grid):
            return self._get_tagged_words(grid, tagset)
        return LazyConcatenation(LazyMap(get_tagged_words,
                                         self._grids(fileids)))

    def tagged_sents(self, fileids=None, tagset=None):
        self._require(self.WORDS, self.POS)
        def get_tagged_words(grid):
            return self._get_tagged_words(grid, tagset)
        return LazyMap(get_tagged_words, self._grids(fileids))

    def chunked_words(self, fileids=None, chunk_types=None,
                      tagset=None):
        self._require(self.WORDS, self.POS, self.CHUNK)
        if chunk_types is None: chunk_types = self._chunk_types
        def get_chunked_words(grid): # capture chunk_types as local var
            return self._get_chunked_words(grid, chunk_types, tagset)
        return LazyConcatenation(LazyMap(get_chunked_words,
                                         self._grids(fileids)))

    def chunked_sents(self, fileids=None, chunk_types=None,
                      tagset=None):
        self._require(self.WORDS, self.POS, self.CHUNK)
        if chunk_types is None: chunk_types = self._chunk_types
        def get_chunked_words(grid): # capture chunk_types as local var
            return self._get_chunked_words(grid, chunk_types, tagset)
        return LazyMap(get_chunked_words, self._grids(fileids))

    def parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None):
        self._require(self.WORDS, self.POS, self.TREE)
        if pos_in_tree is None: pos_in_tree = self._pos_in_tree
        def get_parsed_sent(grid): # capture pos_in_tree as local var
            return self._get_parsed_sent(grid, pos_in_tree, tagset)
        return LazyMap(get_parsed_sent, self._grids(fileids))

    def srl_spans(self, fileids=None):
        self._require(self.SRL)
        return LazyMap(self._get_srl_spans, self._grids(fileids))

    def srl_instances(self, fileids=None, pos_in_tree=None, flatten=True):
        self._require(self.WORDS, self.POS, self.TREE, self.SRL)
        if pos_in_tree is None: pos_in_tree = self._pos_in_tree
        def get_srl_instances(grid): # capture pos_in_tree as local var
            return self._get_srl_instances(grid, pos_in_tree)
        result = LazyMap(get_srl_instances, self._grids(fileids))
        if flatten: result = LazyConcatenation(result)
        return result

    def iob_words(self, fileids=None, tagset=None):
        """
        :return: a list of word/tag/IOB tuples
        :rtype: list(tuple)
--
        :type fileids: None or str or list
        """
        self._require(self.WORDS, self.POS, self.CHUNK)
        def get_iob_words(grid):
            return self._get_iob_words(grid, tagset)
        return LazyConcatenation(LazyMap(get_iob_words, self._grids(fileids)))

    def iob_sents(self, fileids=None, tagset=None):
        """
        :return: a list of lists of word/tag/IOB tuples
        :rtype: list(list)
--
        :type fileids: None or str or list
        """
        self._require(self.WORDS, self.POS, self.CHUNK)
        def get_iob_words(grid):
            return self._get_iob_words(grid, tagset)
        return LazyMap(get_iob_words, self._grids(fileids))

--
    # Grid Reading
    #/////////////////////////////////////////////////////////////////

    def _grids(self, fileids=None):
        # n.b.: we could cache the object returned here (keyed on
        # fileids), which would let us reuse the same corpus view for
        # different things (eg srl and parse trees).
--
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def _read_grid_block(self, stream):
        grids = []
        for block in read_blankline_block(stream):
            block = block.strip()
--
    # given a grid, transform it into some representation (e.g.,
    # a list of words or a parse tree).

    def _get_words(self, grid):
        return self._get_column(grid, self._colmap['words'])

    def _get_tagged_words(self, grid, tagset=None):
        pos_tags = self._get_column(grid, self._colmap['pos'])
        if tagset and tagset != self._tagset:
            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags))

    def _get_iob_words(self, grid, tagset=None):
        pos_tags = self._get_column(grid, self._colmap['pos'])
        if tagset and tagset != self._tagset:
            pos_tags = [map_tag(self._tagset, tagset, t) for t in pos_tags]
        return list(zip(self._get_column(grid, self._colmap['words']), pos_tags,
                   self._get_column(grid, self._colmap['chunk'])))

    def _get_chunked_words(self, grid, chunk_types, tagset=None):
        # n.b.: this method is very similar to conllstr2tree.
        words = self._get_column(grid, self._colmap['words'])
        pos_tags = self._get_column(grid, self._colmap['pos'])
--

        return stack[0]

    def _get_parsed_sent(self, grid, pos_in_tree, tagset=None):
        words = self._get_column(grid, self._colmap['words'])
        pos_tags = self._get_column(grid, self._colmap['pos'])
        if tagset and tagset != self._tagset:
--

        return tree

    def _get_srl_spans(self, grid):
        """
        list of list of (start, end), tag) tuples
        """
--

        return spanlists

    def _get_srl_instances(self, grid, pos_in_tree):
        tree = self._get_parsed_sent(grid, pos_in_tree)
        spanlists = self._get_srl_spans(grid)
        if self._srl_includes_roleset:
--
    # Helper Methods
    #/////////////////////////////////////////////////////////////////

    def _require(self, *columntypes):
        for columntype in columntypes:
            if columntype not in self._colmap:
                raise ValueError('This corpus does not contain a %s '
                                 'column.' % columntype)

    @staticmethod
    def _get_column(grid, column_index):
        return [grid[i][column_index] for i in range(len(grid))]


--
    """
    # [xx] add inst.core_arguments, inst.argm_arguments?

    def __init__(self, tree, verb_head, verb_stem, roleset, tagged_spans):
        self.verb = []
        """A list of the word indices of the words that compose the
           verb whose arguments are identified by this instance.
--
            else:
                self.arguments.append( ((start, end), tag) )

    def __repr__(self):
        plural = len(self.arguments)!=1 and 's' or ''
        return '<ConllSRLInstance for %r with %d argument%s>' % (
            (self.verb_stem, len(self.arguments), plural))

    def pprint(self):
        verbstr = ' '.join(self.words[i][0] for i in self.verb)
        hdr = 'SRL for %r (stem=%r):\n' % (verbstr, self.verb_stem)
        s = ''
--
    """
    Set of instances for a single sentence
    """
    def __init__(self, tree, instances=()):
        self.tree = tree
        list.__init__(self, instances)

    def __str__(self):
        return self.pprint()

    def pprint(self, include_tree=False):
        # Sanity check: trees should be the same
        for inst in self:
            if inst.tree != self.tree:
--
            s += '\n'
        return s

    def _tree2conll(self, tree, wordnum, words, pos, synt):
        assert isinstance(tree, Tree)
        if len(tree) == 1 and isinstance(tree[0], compat.string_types):
            pos[wordnum] = tree.node
--
    A ConllCorpusReader whose data file contains three columns: words,
    pos, and chunk.
    """
    def __init__(self, root, fileids, chunk_types, encoding='utf8',
                 tagset=None):
        ConllCorpusReader.__init__(
            self, root, fileids, ('words', 'pos', 'chunk'),
--
    """
    List of words, one per line.  Blank lines are ignored.
    """
    def words(self, fileids=None):
        return concat([IndianCorpusView(fileid, enc,
                                        False, False)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_words(self, fileids=None, tagset=None):
        if tagset and tagset != self._tagset:
            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)
        else:
--
                                        True, False, tag_mapping_function)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        return concat([IndianCorpusView(fileid, enc,
                                        False, True)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_sents(self, fileids=None, tagset=None):
        if tagset and tagset != self._tagset:
            tag_mapping_function = lambda t: map_tag(self._tagset, tagset, t)
        else:
--
                                        True, True, tag_mapping_function)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])


class IndianCorpusView(StreamBackedCorpusView):
    def __init__(self, corpus_file, encoding, tagged,
                 group_by_sent, tag_mapping_function=None):
        self._tagged = tagged
        self._group_by_sent = group_by_sent
        self._tag_mapping_function = tag_mapping_function
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)

    def read_block(self, stream):
        line = stream.readline()
        if line.startswith('<'):
            return []
--
from .xmldocs import *


def norm(value_string):
    """
    Normalize the string value in an RTE pair's ``value`` or ``entailment``
    attribute as an integer (1, 0).
--
    ``entailment`` in RTE2 and RTE3. These both get mapped on to the ``entailment``
    attribute of this class.
    """
    def __init__(self, pair, challenge=None, id=None, text=None, hyp=None,
             value=None, task=None, length=None):
        """
        :param challenge: version of the RTE challenge (i.e., RTE1, RTE2 or RTE3)
--
        else:
            self.length = length

    def __repr__(self):
        if self.challenge:
            return '<RTEPair: gid=%s-%s>' % (self.challenge, self.id)
        else:
--
    structure of input documents.
    """

    def _read_etree(self, doc):
        """
        Map the XML input into an RTEPair.

--
                for pair in doc.getiterator("pair")]


    def pairs(self, fileids):
        """
        Build a list of RTEPairs from a RTE corpus.

--
NUM:dist How far is it from Denver to Aspen ?
LOC:city What county is Modesto , California in ?
HUM:desc Who was Galileo ?
DESC:def What is an atom ?
NUM:date When did Hawaii become a state ?
"""

--
# in nltk, we use the form (data, tag) -- e.g., tagged words and
# labeled texts for classifiers.
class StringCategoryCorpusReader(CorpusReader):
    def __init__(self, root, fileids, delimiter=' ', encoding='utf8'):
        """
        :param root: The root directory for this corpus.
        :param fileids: A list or regexp specifying the fileids in this corpus.
--
        CorpusReader.__init__(self, root, fileids, encoding)
        self._delimiter = delimiter

    def tuples(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def raw(self, fileids=None):
        """
        :return: the text contents of the given fileids, as a single string.
        """
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def _read_tuple_block(self, stream):
        line = stream.readline().strip()
        if line:
            return [tuple(line.split(self._delimiter, 1))]
--
__docformat__ = 'epytext en'

import re
from collections import defaultdict

from nltk.util import flatten
from nltk.compat import string_types
--
    For access to the file text use the usual nltk functions,
    ``words()``, ``sents()``, ``tagged_words()`` and ``tagged_sents()``.
    """
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
        self._lazy = lazy

    def words(self, fileids=None, speaker='ALL', stem=False,
            relation=False, strip_space=True, replace=False):
        """
        :return: the given file(s) as a list of words
        :rtype: list(str)

        :param speaker: If specified, select specific speaker(s) defined
            in the corpus. Default is 'ALL' (all participants). Common choices
            are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude
            researchers)
        :param stem: If true, then use word stems instead of word strings.
--
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])

    def tagged_words(self, fileids=None, speaker='ALL', stem=False,
            relation=False, strip_space=True, replace=False):
        """
        :return: the given file(s) as a list of tagged
--
            ``(word,tag)``.
        :rtype: list(tuple(str,str))

        :param speaker: If specified, select specific speaker(s) defined
            in the corpus. Default is 'ALL' (all participants). Common choices
            are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude
            researchers)
        :param stem: If true, then use word stems instead of word strings.
--
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])

    def sents(self, fileids=None, speaker='ALL', stem=False,
            relation=None, strip_space=True, replace=False):
        """
        :return: the given file(s) as a list of sentences or utterances, each
            encoded as a list of word strings.
        :rtype: list(list(str))

        :param speaker: If specified, select specific speaker(s) defined
            in the corpus. Default is 'ALL' (all participants). Common choices
            are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude
            researchers)
        :param stem: If true, then use word stems instead of word strings.
--
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])

    def tagged_sents(self, fileids=None, speaker='ALL', stem=False,
            relation=None, strip_space=True, replace=False):
        """
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
        :rtype: list(list(tuple(str,str)))

        :param speaker: If specified, select specific speaker(s) defined
            in the corpus. Default is 'ALL' (all participants). Common choices
            are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude
            researchers)
        :param stem: If true, then use word stems instead of word strings.
--
        return concat([self._get_words(fileid, speaker, sent, stem, relation,
            pos, strip_space, replace) for fileid in self.abspaths(fileids)])

    def corpus(self, fileids=None):
        """
        :return: the given file(s) as a dict of ``(corpus_property_key, value)``
        :rtype: list(dict)
        """
        return [self._get_corpus(fileid) for fileid in self.abspaths(fileids)]

    def _get_corpus(self, fileid):
        results = dict()
        xmldoc = ElementTree.parse(fileid).getroot()
        for key, value in xmldoc.items():
            results[key] = value
        return results

    def participants(self, fileids=None):
        """
        :return: the given file(s) as a dict of
            ``(participant_property_key, value)``
--
        return [self._get_participants(fileid)
                            for fileid in self.abspaths(fileids)]

    def _get_participants(self, fileid):
        # multidimensional dicts
        def dictOfDicts():
            return defaultdict(dictOfDicts)

        xmldoc = ElementTree.parse(fileid).getroot()
        # getting participants' data
--
                pat[participant.get('id')][key] = value
        return pat

    def age(self, fileids=None, speaker='CHI', month=False):
        """
        :return: the given file(s) as string or int
        :rtype: list or int
--
        return [self._get_age(fileid, speaker, month)
                for fileid in self.abspaths(fileids)]

    def _get_age(self, fileid, speaker, month):
        xmldoc = ElementTree.parse(fileid).getroot()
        for pat in xmldoc.findall('.//{%s}Participants/{%s}participant'
                                  % (NS,NS)):
--
            except (TypeError, AttributeError) as e:
                return None

    def convert_age(self, age_year):
        "Caclculate age in months from a string in CHILDES format"
        m = re.match("P(\d+)Y(\d+)M?(\d?\d?)D?",age_year)
        age_month = int(m.group(1))*12 + int(m.group(2))
--
            pass
        return age_month

    def MLU(self, fileids=None, speaker='CHI'):
        """
        :return: the given file(s) as a floating number
        :rtype: list(float)
--
        return [self._getMLU(fileid, speaker=speaker)
                for fileid in self.abspaths(fileids)]

    def _getMLU(self, fileid, speaker):
        sents = self._get_words(fileid, speaker=speaker, sent=True, stem=True,
                    relation=False, pos=True, strip_space=True, replace=True)
        results = []
--
        # return {'mlu':mlu,'wordNum':numWords,'sentNum':numSents}
        return mlu

    def _get_words(self, fileid, speaker, sent, stem, relation, pos,
            strip_space, replace):
        if isinstance(speaker, string_types) and speaker != 'ALL':  # ensure we have a list of speakers
            speaker = [ speaker ]
--
    childes_url_base = r'http://childes.psy.cmu.edu/browser/index.php?url='


    def webview_file(self, fileid, urlbase=None):
        """Map a corpus file to its web version on the CHILDES website,
        and open it in a web browser.

--



def demo(corpus_root=None):
    """
    The CHILDES corpus should be manually downloaded and saved
    to ``[NLTK_Data_Dir]/corpora/childes/``
--

import re
import textwrap
from collections import defaultdict

from nltk import compat
from .xmldocs import XMLCorpusReader
--
class VerbnetCorpusReader(XMLCorpusReader):

    # No unicode encoding param, since the data files are all XML.
    def __init__(self, root, fileids, wrap_etree=False):
        XMLCorpusReader.__init__(self, root, fileids, wrap_etree)

        self._lemma_to_class = defaultdict(list)
        """A dictionary mapping from verb lemma strings to lists of
        verbnet class identifiers."""

        self._wordnet_to_class = defaultdict(list)
        """A dictionary mapping from wordnet identifier strings to
        lists of verbnet class identifiers."""

--
    """Regular expression used by ``_index()`` to quickly scan the corpus
       for basic information."""

    def lemmas(self, classid=None):
        """
        Return a list of all verb lemmas that appear in any class, or
        in the ``classid`` if specified.
--
            return [member.get('name') for member in
                    vnclass.findall('MEMBERS/MEMBER')]

    def wordnetids(self, classid=None):
        """
        Return a list of all wordnet identifiers that appear in any
        class, or in ``classid`` if specified.
--
            return sum([member.get('wn','').split() for member in
                        vnclass.findall('MEMBERS/MEMBER')], [])

    def classids(self, lemma=None, wordnetid=None, fileid=None, classid=None):
        """
        Return a list of the verbnet class identifiers.  If a file
        identifier is specified, then return only the verbnet class
        identifiers for classes (and subclasses) defined by that file.
        If a lemma is specified, then return only verbnet class
        identifiers for classes that contain that lemma as a member.
        If a wordnetid is specified, then return only identifiers for
--
        else:
            return sorted(self._class_to_fileid.keys())

    def vnclass(self, fileid_or_classid):
        """
        Return an ElementTree containing the xml for the specified
        verbnet class.
--
        else:
            raise ValueError('Unknown identifier %s' % fileid_or_classid)

    def fileids(self, vnclass_ids=None):
        """
        Return a list of fileids that make up this corpus.  If
        ``vnclass_ids`` is specified, then return the fileids that make
--
    #{ Index Initialization
    ######################################################################

    def _index(self):
        """
        Initialize the indexes ``_lemma_to_class``,
        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
--
        for fileid in self._fileids:
            self._index_helper(self.xml(fileid), fileid)

    def _index_helper(self, xmltree, fileid):
        """Helper for ``_index()``"""
        vnclass = xmltree.get('ID')
        self._class_to_fileid[vnclass] = fileid
--
        for subclass in xmltree.findall('SUBCLASSES/VNSUBCLASS'):
            self._index_helper(subclass, fileid)

    def _quick_index(self):
        """
        Initialize the indexes ``_lemma_to_class``,
        ``_wordnet_to_class``, and ``_class_to_fileid`` by scanning
--
    #{ Identifier conversion
    ######################################################################

    def longid(self, shortid):
        """Given a short verbnet class identifier (eg '37.10'), map it
        to a long id (eg 'confess-37.10').  If ``shortid`` is already a
        long id, then return it as-is"""
--
        except KeyError:
            raise ValueError('vnclass identifier %r not found' % shortid)

    def shortid(self, longid):
        """Given a long verbnet class identifier (eg 'confess-37.10'),
        map it to a short id (eg '37.10').  If ``longid`` is already a
        short id, then return it as-is."""
--
    #{ Pretty Printing
    ######################################################################

    def pprint(self, vnclass):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet class.
--
                       for vnframe in vnclass.findall('FRAMES/FRAME'))
        return s

    def pprint_subclasses(self, vnclass, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet class's subclasses.
--
        return textwrap.fill(s, 70, initial_indent=indent,
                             subsequent_indent=indent+'  ')

    def pprint_members(self, vnclass, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet class's member verbs.
--
        return textwrap.fill(s, 70, initial_indent=indent,
                             subsequent_indent=indent+'  ')

    def pprint_themroles(self, vnclass, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet class's thematic roles.
--

        return '\n'.join(pieces)

    def pprint_frame(self, vnframe, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet frame.
--
        s += self.pprint_semantics(vnframe, indent+'    ')
        return s

    def pprint_description(self, vnframe, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet frame description.
--
            s += ' (%s)' % descr.get('secondary')
        return s

    def pprint_syntax(self, vnframe, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet frame syntax.
--

        return indent + ' '.join(pieces)

    def pprint_semantics(self, vnframe, indent=''):
        """
        Return a string containing a pretty-printed representation of
        the given verbnet frame semantics.
--

@compat.python_2_unicode_compatible
class PPAttachment(object):
    def __init__(self, sent, verb, noun1, prep, noun2, attachment):
        self.sent = sent
        self.verb = verb
        self.noun1 = noun1
--
        self.noun2 = noun2
        self.attachment = attachment

    def __repr__(self):
        return ('PPAttachment(sent=%r, verb=%r, noun1=%r, prep=%r, '
                'noun2=%r, attachment=%r)' %
                (self.sent, self.verb, self.noun1, self.prep,
--
    """
    sentence_id verb noun1 preposition noun2 attachment
    """
    def attachments(self, fileids):
        return concat([StreamBackedCorpusView(fileid, self._read_obj_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tuples(self, fileids):
        return concat([StreamBackedCorpusView(fileid, self._read_tuple_block,
                                              encoding=enc)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def _read_tuple_block(self, stream):
        line = stream.readline()
        if line:
            return [tuple(line.split())]
        else:
            return []

    def _read_obj_block(self, stream):
        line = stream.readline()
        if line:
            return [PPAttachment(*line.split())]
--
    method.  For access to simple word lists and tagged word lists, use
    ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.
    """
    def __init__(self, root, fileids, lazy=True):
        XMLCorpusReader.__init__(self, root, fileids)
        self._lazy = lazy

    def words(self, fileids=None):
        """
        :return: the given file(s) as a list of words and punctuation symbols.
        :rtype: list(str)
        """
        return self._items(fileids, 'word', False, False, False)

    def chunks(self, fileids=None):
        """
        :return: the given file(s) as a list of chunks,
            each of which is a list of words and punctuation symbols
--
        """
        return self._items(fileids, 'chunk', False, False, False)

    def tagged_chunks(self, fileids=None, tag=('pos' or 'sem' or 'both')):
        """
        :return: the given file(s) as a list of tagged chunks, represented
            in tree form.
--
        """
        return self._items(fileids, 'chunk', False, tag!='sem', tag!='pos')

    def sents(self, fileids=None):
        """
        :return: the given file(s) as a list of sentences, each encoded
            as a list of word strings.
--
        """
        return self._items(fileids, 'word', True, False, False)

    def chunk_sents(self, fileids=None):
        """
        :return: the given file(s) as a list of sentences, each encoded
            as a list of chunks.
--
        """
        return self._items(fileids, 'chunk', True, False, False)

    def tagged_sents(self, fileids=None, tag=('pos' or 'sem' or 'both')):
        """
        :return: the given file(s) as a list of sentences. Each sentence
            is represented as a list of tagged chunks (in tree form).
--
        """
        return self._items(fileids, 'chunk', True, tag!='sem', tag!='pos')

    def _items(self, fileids, unit, bracket_sent, pos_tag, sem_tag):
        if unit=='word' and not bracket_sent:
            # the result of the SemcorWordView may be a multiword unit, so the
            # LazyConcatenation will make sure the sentence is flattened
--
        return concat([_(fileid, unit, bracket_sent, pos_tag, sem_tag)
                       for fileid in self.abspaths(fileids)])

    def _words(self, fileid, unit, bracket_sent, pos_tag, sem_tag):
        """
        Helper used to implement the view methods -- returns a list of
        tokens, (segmented) words, chunks, or sentences. The tokens
--
        return result

    @staticmethod
    def _word(xmlword, unit, pos_tag, sem_tag):
        tkn = xmlword.text
        if not tkn:
            tkn = "" # fixes issue 337?

        lemma = xmlword.get('lemma', tkn) # lemma or NE class
        redef = xmlword.get('rdf', tkn)	# redefinition--this indicates the lookup string
        # does not exactly match the enclosed string, e.g. due to typographical adjustments
        # or discontinuity of a multiword expression. If a redefinition has occurred,
        # the "rdf" attribute holds its inflected form and "lemma" holds its lemma.
        # For NEs, "rdf", "lemma", and "pn" all hold the same value (the NE class).
        sensenum = xmlword.get('wnsn')  # WordNet sense number
--
                else:
                    return bottom # chunk as a list

def _all_xmlwords_in(elt, result=None):
    if result is None: result = []
    for child in elt:
        if child.tag in ('wf', 'punc'): result.append(child)
--
    A list of words, augmented by an attribute ``num`` used to record
    the sentence identifier (the ``n`` attribute from the XML).
    """
    def __init__(self, num, items):
        self.num = num
        list.__init__(self, items)

--
    """
    A stream backed corpus view specialized for use with the BNC corpus.
    """
    def __init__(self, fileid, unit, bracket_sent, pos_tag, sem_tag):
        """
        :param fileid: The name of the underlying file.
        :param unit: One of `'token'`, `'word'`, or `'chunk'`.
--

        XMLCorpusView.__init__(self, fileid, tagspec)

    def handle_elt(self, elt, context):
        if self._sent: return self.handle_sent(elt)
        else: return self.handle_word(elt)

    def handle_word(self, elt):
        return SemcorCorpusReader._word(elt, self._unit, self._pos_tag, self._sem_tag)

    def handle_sent(self, elt):
        sent = []
        for child in elt:
            if child.tag in ('wf','punc'):
--

class DependencyCorpusReader(SyntaxCorpusReader):

    def __init__(self, root, fileids, encoding='utf8',
                 word_tokenizer=TabTokenizer(),
                 sent_tokenizer=RegexpTokenizer('\n', gaps=True),
                 para_block_reader=read_blankline_block):
--

    #########################################################

    def raw(self, fileids=None):
        """
        :return: the given file(s) as a single string.
        :rtype: str
--
                    result.append(fp.read())
        return concat(result)

    def words(self, fileids=None):
        return concat([DependencyCorpusView(fileid, False, False, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])

    def tagged_words(self, fileids=None):
        return concat([DependencyCorpusView(fileid, True, False, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])

    def sents(self, fileids=None):
        return concat([DependencyCorpusView(fileid, False, True, False, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, include_encoding=True)])

    def tagged_sents(self, fileids=None):
            return concat([DependencyCorpusView(fileid, True, True, False, encoding=enc)
                           for fileid, enc in self.abspaths(fileids, include_encoding=True)])

    def parsed_sents(self, fileids=None):
        sents=concat([DependencyCorpusView(fileid, False, True, True, encoding=enc)
                      for fileid, enc in self.abspaths(fileids, include_encoding=True)])
        return [DependencyGraph(sent) for sent in sents]


class DependencyCorpusView(StreamBackedCorpusView):
    _DOCSTART = '-DOCSTART- -DOCSTART- O\n' #dokumentu hasiera definitzen da

    def __init__(self, corpus_file, tagged, group_by_sent, dependencies,
                 chunk_types=None, encoding='utf8'):
        self._tagged = tagged
        self._dependencies = dependencies
--
        self._chunk_types = chunk_types
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)

    def read_block(self, stream):
        # Read the next sentence.
        sent = read_blankline_block(stream)[0].strip()
        # Strip off the docstart marker, if present.
--
    Treebank with information about the predicate argument structure
    of every verb instance.  The corpus consists of two parts: the
    predicate-argument annotations themselves, and a set of "frameset
    files" which define the argument labels used by the annotations,
    on a per-verb basis.  Each "frameset file" contains one or more
    predicates, such as ``'turn'`` or ``'turn_on'``, each of which is
    divided into coarse-grained word senses called "rolesets".  For
    each "roleset", the frameset file provides descriptions of the
    argument roles, along with examples.
    """
    def __init__(self, root, propfile, framefiles='',
                 verbsfile=None, parse_fileid_xform=None,
                 parse_corpus=None, encoding='utf8'):
        """
--
        self._parse_fileid_xform = parse_fileid_xform
        self._parse_corpus = parse_corpus

    def raw(self, fileids=None):
        """
        :return: the text contents of the given fileids, as a single string.
        """
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def instances(self, baseform=None):
        """
        :return: a corpus view that acts as a list of
        ``PropBankInstance`` objects, one for each noun in the corpus.
--
                                      lambda stream: self._read_instance_block(stream, **kwargs),
                                      encoding=self.encoding(self._propfile))

    def lines(self):
        """
        :return: a corpus view that acts as a list of strings, one for
        each line in the predicate-argument annotation file.
--
                                      read_line_block,
                                      encoding=self.encoding(self._propfile))

    def roleset(self, roleset_id):
        """
        :return: the xml description for the given roleset.
        """
--
            raise ValueError('Roleset %s not found in %s' %
                             (roleset_id, framefile))

    def rolesets(self, baseform=None):
        """
        :return: list of xml descriptions for rolesets.
        """
--
            rsets.append(etree.findall('predicate/roleset'))
        return LazyConcatenation(rsets)

    def verbs(self):
        """
        :return: a corpus view that acts as a list of all verb lemmas
        in this corpus (from the verbs.txt file).
--
                                      read_line_block,
                                      encoding=self.encoding(self._verbsfile))

    def _read_instance_block(self, stream, instance_filter=lambda inst: True):
        block = []

        # Read 100 at a time.
--
@compat.python_2_unicode_compatible
class PropbankInstance(object):

    def __init__(self, fileid, sentnum, wordnum, tagger, roleset,
                 inflection, predicate, arguments, parse_corpus=None):

        self.fileid = fileid
--
        instances in this propbank corpus."""

    @property
    def baseform(self):
        """The baseform of the predicate."""
        return self.roleset.split('.')[0]

    @property
    def sensenumber(self):
        """The sense number of the predicate."""
        return self.roleset.split('.')[1]

    @property
    def predid(self):
        """Identifier of the predicate."""
        return 'rel'

    def __repr__(self):
        return ('<PropbankInstance: %s, sent %s, word %s>' %
                (self.fileid, self.sentnum, self.wordnum))

    def __str__(self):
        s = '%s %s %s %s %s %s' % (self.fileid, self.sentnum, self.wordnum,
                                   self.tagger, self.roleset, self.inflection)
        items = self.arguments + ((self.predicate, 'rel'),)
--
            s += ' %s-%s' % (argloc, argid)
        return s

    def _get_tree(self):
        if self.parse_corpus is None: return None
        if self.fileid not in self.parse_corpus.fileids(): return None
        return self.parse_corpus.parsed_sents(self.fileid)[self.sentnum]
--
        the corresponding tree is not available.""")

    @staticmethod
    def parse(s, parse_fileid_xform=None, parse_corpus=None):
        pieces = s.split()
        if len(pieces) < 7:
            raise ValueError('Badly formatted propbank line: %r' % s)
--
        chains in a tree.  It consists of a sequence of pieces, which
        can be ``PropbankTreePointer`` or ``PropbankSplitTreePointer`` pointers.
    """
    def __init__(self):
        if self.__class__ == PropbankPointer:
            raise NotImplementedError()

@compat.python_2_unicode_compatible
class PropbankChainTreePointer(PropbankPointer):
    def __init__(self, pieces):
        self.pieces = pieces
        """A list of the pieces that make up this chain.  Elements may
           be either ``PropbankSplitTreePointer`` or
           ``PropbankTreePointer`` pointers."""

    def __str__(self):
        return '*'.join('%s' % p for p in self.pieces)
    def __repr__(self):
        return '<PropbankChainTreePointer: %s>' % self
    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return Tree('*CHAIN*', [p.select(tree) for p in self.pieces])


@compat.python_2_unicode_compatible
class PropbankSplitTreePointer(PropbankPointer):
    def __init__(self, pieces):
        self.pieces = pieces
        """A list of the pieces that make up this chain.  Elements are
           all ``PropbankTreePointer`` pointers."""

    def __str__(self):
        return ','.join('%s' % p for p in self.pieces)
    def __repr__(self):
        return '<PropbankSplitTreePointer: %s>' % self
    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return Tree('*SPLIT*', [p.select(tree) for p in self.pieces])

--
    wordnum:height,

    """
    def __init__(self, wordnum, height):
        self.wordnum = wordnum
        self.height = height

    @staticmethod
    def parse(s):
        # Deal with chains (xx*yy*zz)
        pieces = s.split('*')
        if len(pieces) > 1:
--
        if len(pieces) != 2: raise ValueError('bad propbank pointer %r' % s)
        return PropbankTreePointer(int(pieces[0]), int(pieces[1]))

    def __str__(self):
        return '%s:%s' % (self.wordnum, self.height)

    def __repr__(self):
        return 'PropbankTreePointer(%d, %d)' % (self.wordnum, self.height)

    def __eq__(self, other):
        while isinstance(other, (PropbankChainTreePointer,
                                 PropbankSplitTreePointer)):
            other = other.pieces[0]
--

        return (self.wordnum == other.wordnum and self.height == other.height)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        while isinstance(other, (PropbankChainTreePointer,
                                 PropbankSplitTreePointer)):
            other = other.pieces[0]
--

        return (self.wordnum, -self.height) < (other.wordnum, -other.height)

    def select(self, tree):
        if tree is None: raise ValueError('Parse tree not avaialable')
        return tree[self.treepos(tree)]

    def treepos(self, tree):
        """
        Convert this pointer to a standard 'tree position' pointer,
        given that it points to the given tree.
--
    NONE = '-'
    #}

    def __init__(self, form='-', tense='-', aspect='-', person='-', voice='-'):
        self.form = form
        self.tense = tense
        self.aspect = aspect
        self.person = person
        self.voice = voice

    def __str__(self):
        return self.form+self.tense+self.aspect+self.person+self.voice

    def __repr__(self):
        return '<PropbankInflection: %s>' % self

    _VALIDATE = re.compile(r'[igpv\-][fpn\-][pob\-][3\-][ap\-]$')

    @staticmethod
    def parse(s):
        if not isinstance(s, compat.string_types):
            raise TypeError('expected a string')
        if (len(s) != 5 or
--
import os, sys
import re
import textwrap
from collections import defaultdict
from pprint import pprint, pformat
from nltk.internals import ElementWrapper
from nltk.corpus.reader import XMLCorpusReader, XMLCorpusView
--
from nltk.util import AbstractLazySequence, LazyMap


def _pretty_longstring(defstr, prefix='', wrap_at=65):

    """
    Helper function for pretty-printing a long string.

    :param defstr: The string to be printed.
    :type defstr: str
    :return: A nicely formated string representation of the long string.
    :rtype: str
    """

    outstr = ""
    for line in textwrap.fill(defstr, wrap_at).split('\n'):
        outstr += prefix + line + '\n'
    return outstr

def _pretty_any(obj):

    """
    Helper function for pretty-printing any AttrDict object.
--

    return outstr

def _pretty_semtype(st):

    """
    Helper function for pretty-printing a semantic type.
--
    outstr += "semantic type ({0.ID}): {0.name}\n".format(st)
    if 'abbrev' in semkeys:
        outstr += "[abbrev] {0}\n".format(st.abbrev)
    if 'definition' in semkeys:
        outstr += "[definition]\n"
        outstr += _pretty_longstring(st.definition,'  ')
    outstr += "[rootType] {0}({1})\n".format(st.rootType.name, st.rootType.ID)
    if st.superType is None:
        outstr += "[superType] <None>\n"
--
    outstr += "  " + ", ".join('{0}({1})'.format(x.name, x.ID) for x in st.subTypes) + '\n'*(len(st.subTypes)>0)
    return outstr

def _pretty_frame_relation_type(freltyp):

    """
    Helper function for pretty-printing a frame relation type.
--
    outstr = "<frame relation type ({0.ID}): {0.superFrameName} -- {0.name} -> {0.subFrameName}>".format(freltyp)
    return outstr
    
def _pretty_frame_relation(frel):

    """
    Helper function for pretty-printing a frame relation.
--
    outstr = "<{0.type.superFrameName}={0.superFrameName} -- {0.type.name} -> {0.type.subFrameName}={0.subFrameName}>".format(frel)
    return outstr

def _pretty_fe_relation(ferel):

    """
    Helper function for pretty-printing an FE relation.
--
    outstr = "<{0.type.superFrameName}={0.frameRelation.superFrameName}.{0.superFEName} -- {0.type.name} -> {0.type.subFrameName}={0.frameRelation.subFrameName}.{0.subFEName}>".format(ferel)
    return outstr

def _pretty_lu(lu):

    """
    Helper function for pretty-printing a lexical unit.
--
    lukeys = lu.keys()
    outstr = ""
    outstr += "lexical unit ({0.ID}): {0.name}\n\n".format(lu)
    if 'definition' in lukeys:
        outstr += "[definition]\n"
        outstr += _pretty_longstring(lu.definition,'  ')
    if 'frame' in lukeys:
        outstr += "\n[frame] {0}({1})\n".format(lu.frame.name,lu.frame.ID)
    if 'incorporatedFE' in lukeys:
--

    return outstr

def _pretty_fe(fe):

    """
    Helper function for pretty-printing a frame element.
--
    fekeys = fe.keys()
    outstr = ""
    outstr += "frame element ({0.ID}): {0.name}\n    of {1.name}({1.ID})\n".format(fe, fe.frame)
    if 'definition' in fekeys:
        outstr += "[definition]\n"
        outstr += _pretty_longstring(fe.definition,'  ')
    if 'abbrev' in fekeys:
        outstr += "[abbrev] {0}\n".format(fe.abbrev)
    if 'coreType' in fekeys:
--

    return outstr

def _pretty_frame(frame):

    """
    Helper function for pretty-printing a frame.
--

    outstr = ""
    outstr += "frame ({0.ID}): {0.name}\n\n".format(frame)
    outstr += "[definition]\n"
    outstr += _pretty_longstring(frame.definition, '  ') + '\n'

    outstr += "[semTypes] {0} semantic types\n".format(len(frame.semTypes))
    outstr += "  "*(len(frame.semTypes)>0) + ", ".join("{0}({1})".format(x.name, x.ID) for x in frame.semTypes) + '\n'*(len(frame.semTypes)>0)
--
    {'a': 1, 'b': 2, 'c': 3, 'd': 4}
    """

    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        #self.__dict__ = self
    
    def __setattr__(self, name, value):
        self[name] = value
    def __getattr__(self, name):
        if name=='_short_repr':
            return self._short_repr
        return self[name]
    def __getitem__(self, name):
        v = super(AttrDict,self).__getitem__(name)
        if isinstance(v,Future):
            return v._data()
        return v

    def _short_repr(self):
        if '_type' in self:
            if self['_type'].endswith('relation'):
                return self.__repr__()
--
        else:
            return self.__repr__()
        
    def _str(self):
        outstr = ""

        if not '_type' in self:
--
        # assert isinstance(outstr, unicode) # not in Python 3.2
        return outstr
        
    def __str__(self):
        return self._str()
    def __repr__(self):
        return self.__str__()

class Future(object):
--
    Wraps and acts as a proxy for a value to be loaded lazily (on demand). 
    Adapted from https://gist.github.com/sergey-miryanov/2935416
    """
    def __init__(self, loader, *args, **kwargs):
        """
        :param loader: when called with no arguments, returns the value to be stored
        :type loader: callable
--
        super (Future, self).__init__(*args, **kwargs)
        self._loader = loader
        self._d = None
    def _data(self):
        if callable(self._loader):
            self._d = self._loader()
            self._loader = None # the data is now cached
        return self._d
 
    def __nonzero__(self):
        return bool(self._data())
    def __len__(self):
        return len(self._data())
 
    def __setitem__(self, key, value):
        return self._data ().__setitem__(key, value)
    def __getitem__(self, key):
        return self._data ().__getitem__(key)
    def __getattr__(self, key):
        return self._data().__getattr__(key)
 
    def __str__(self):
        return self._data().__str__()
    def __repr__(self):
        return self._data().__repr__()


--
    Inherits from AttrDict, so a callable value will 
    be lazily converted to an actual value.
    """
    def __init__(self, *args, **kwargs):
        _BREAK_LINES = kwargs.pop('breakLines', False)
        super(PrettyDict, self).__init__(*args, **kwargs)
        dict.__setattr__(self, '_BREAK_LINES', _BREAK_LINES)
    def __repr__(self):
        parts = []
        for k,v in sorted(self.items()):
            kv = repr(k)+': '
--
    Displays an abbreviated repr of only the first several elements, not the whole list.
    """
    # from nltk.util
    def __init__(self, *args, **kwargs):
        self._MAX_REPR_SIZE = kwargs.pop('maxReprSize', 60)
        self._BREAK_LINES = kwargs.pop('breakLines', False)
        super(PrettyList, self).__init__(*args, **kwargs)
    def __repr__(self):
        """
        Return a string representation for this corpus view that is
        similar to a list's representation; but if it would be more
--
    """
    # from nltk.util
    _MAX_REPR_SIZE = 60
    def __repr__(self):
        """
        Return a string representation for this corpus view that is
        similar to a list's representation; but if it would be more
--
    in the XML index.
    """

    def __init__(self, root, fileids):
        XMLCorpusReader.__init__(self, root, fileids)

        # framenet corpus sub dirs
--
        self._ferel_idx = None  # FE-to-FE relation instances
        self._frel_f_idx = None # frame-to-frame relations associated with each frame

    def _buildframeindex(self):
        # The total number of Frames in Framenet is fairly small (~1200) so
        # this index should not be very large
        if not self._frel_idx:
--
                               'frameIndex/frame', self._handle_elt):
            self._frame_idx[f['ID']] = f

    def _buildcorpusindex(self):
        # The total number of fulltext annotated documents in Framenet
        # is fairly small (~90) so this index should not be very large
        self._fulltext_idx = {}
--
            for doc in doclist:
                self._fulltext_idx[doc.ID] = doc

    def _buildluindex(self):
        # The number of LUs in Framenet is about 13,000 so this index
        # should not be very large
        self._lu_idx = {}
--
            self._lu_idx[lu['ID']] = lu # populate with LU index entries. if any of these 
            # are looked up they will be replaced by full LU objects.

    def _buildrelationindex(self):
        #print('building relation index...', file=sys.stderr)
        freltypes = PrettyList(x for x in XMLCorpusView(self.abspath("frRelation.xml"),
                                            'frameRelations/frameRelationType',
                                            self._handle_framerelationtype_elt))
        self._freltyp_idx = {}
        self._frel_idx = {}
        self._frel_f_idx = defaultdict(set)
        self._ferel_idx = {}
        
        for freltyp in freltypes:
--
                    self._ferel_idx[ferel.ID] = ferel
        #print('...done building relation index', file=sys.stderr)

    def readme(self):
        """
        Return the contents of the corpus README.txt (or README) file.
        """
--
        except IOError:
            return self.open("README").read()

    def buildindexes(self):
        """
        Build the internal indexes to make look-ups faster.
        """
--
        # frame and FE relations
        self._buildrelationindex()

    def annotated_document(self, fn_docid):
        """
        Returns the annotated document whose id number is
        ``fn_docid``. This id number can be obtained by calling the
--
        elt = XMLCorpusView(locpath, 'fullTextAnnotation')[0]
        return self._handle_fulltextannotation_elt(elt)

    def frame_by_id(self, fn_fid, ignorekeys=[]):
        """
        Get the details for the specified Frame using the frame's id
        number.
--
        256
        >>> f.name
        'Medical_specialties'
        >>> f.definition
        "This frame includes words that name ..."

        :param fn_fid: The Framenet id number of the frame
--

        return self.frame_by_name(name, ignorekeys, check_cache=False)

    def frame_by_name(self, fn_fname, ignorekeys=[], check_cache=True):
        """
        Get the details for the specified Frame using the frame's name.

--
        256
        >>> f.name
        'Medical_specialties'
        >>> f.definition
        "This frame includes words that name ..."

        :param fn_fname: The name of the frame
--
        '''
        return fentry

    def frame(self, fn_fid_or_fname, ignorekeys=[]):
        """
        Get the details for the specified Frame using the frame's name
        or id number.
--
        >>> f = fn.frame('Medical_specialties')
        >>> f.ID
        256
        >>> # ensure non-ASCII character in definition doesn't trigger an encoding error:
        >>> fn.frame('Imposing_obligation')
        frame (1494): Imposing_obligation...

--
        following information about the Frame:

        - 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)
        - 'definition' : textual definition of the Frame
        - 'ID'         : the internal ID number of the Frame
        - 'semTypes'   : a list of semantic types for this frame
           - Each item in the list is a dict containing the following keys:
--
        - 'FE' : a dict containing the Frame Elements that are part of this frame
                 The keys in this dict are the names of the FEs (e.g. 'Body_system')
                 and the values are dicts containing the following keys
              - 'definition' : The definition of the FE
              - 'name'       : The name of the FE e.g. 'Body_system'
              - 'ID'         : The id number
              - '_type'      : 'fe'
--

        return f

    def frames_by_lemma(self, pat):
        """
        Returns a list of all frames that contain LUs in which the
        ``name`` attribute of the LU matchs the given regular expression
--
        """
        return PrettyList(f for f in self.frames() if any(re.search(pat, luName) for luName in f.lexUnit))

    def lu_basic(self, fn_luid):
        """
        Returns basic information about the LU whose id is
        ``fn_luid``. This is basically just a wrapper around the
--
        {'ID': 256,
         'POS': 'V',
         '_type': 'lu',
         'definition': 'COD: be aware of beforehand; predict.',
         'frame': <frame ID=26 name=Expectation>,
         'lemmaID': 15082,
         'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],
--
        """
        return self.lu(fn_luid, ignorekeys=['subCorpus'])

    def lu(self, fn_luid, ignorekeys=[]):
        """
        Get information about a specific Lexical Unit using the id number
        ``fn_luid``. This function reads the LU information from the xml
--
        >>> from nltk.corpus import framenet as fn
        >>> fn.lu(256).name
        'foresee.v'
        >>> fn.lu(256).definition
        'COD: be aware of beforehand; predict.'
        >>> fn.lu(256).frame.name
        'Expectation'
--
        'incorporatedFE' may be missing in some LUs:

        - 'name'       : the name of the LU (e.g. 'merger.n')
        - 'definition' : textual definition of the LU
        - 'ID'         : the internal ID number of the LU
        - '_type'      : 'lu'
        - 'status'     : e.g. 'Created'
--
                                      - 'name': name of label (e.g. 'NN1')

        Under the hood, this implementation looks up the lexical unit information 
        in the *frame* definition file. That file does not contain 
        corpus annotations, so the LU files will be accessed on demand if those are 
        needed. In principle, valence patterns could be loaded here too, 
        though these are not currently supported.
--

        return luinfo

    def _lu_file(self, lu, ignorekeys=[]):
        """
        Augment the LU information that was loaded from the frame file 
        with additional information from the LU file.
--
        
        return lu.subCorpus

    def _loadsemtypes(self):
        """Create the semantic types index."""
        self._semtypes = AttrDict()
        semtypeXML = [x for x in XMLCorpusView(self.abspath("semTypes.xml"),
--
                queue.append(child)
        #self.propagate_semtypes()  # apply inferencing over FE relations

    def propagate_semtypes(self):
        """
        Apply inference rules to distribute semtypes over relations between FEs. 
        For FrameNet 1.5, this results in 1011 semtypes being propagated.
        (Not done by default because it requires loading all frame files, 
        which takes several seconds. If this needed to be fast, it could be rewritten 
        to traverse the neighboring relations on demand for each FE semtype.)
        
--
                    continue
            #print(i, nPropagations, file=sys.stderr)

    def semtype(self, key):
        """
        >>> from nltk.corpus import framenet as fn
        >>> fn.semtype(233).name
--

        return st
        
    def semtype_inherits(self, st, superST):
        if not isinstance(st, dict):
            st = self.semtype(st)
        if not isinstance(superST, dict):
--
            par = par.superType
        return False

    def frames(self, name=None):
        """
        Obtain details for a specific frame.

--
        frame-evoking words are called "lexical units" (LUs).

        FrameNet includes relations between Frames. Several types of
        relations are defined, of which the most important are:

           - Inheritance: An IS-A relation. The child frame is a subtype
             of the parent frame, and each FE in the parent is bound to
--
        else:
            return PrettyLazyMap(self.frame, fIDs)

    def frame_ids_and_names(self, name=None):
        """
        Uses the frame index, which is much faster than looking up each frame definition 
        if only the names and IDs are needed.
        """
        if not self._frame_idx:
            self._buildframeindex()
        return dict((fID, finfo.name) for fID,finfo in self._frame_idx.items() if name is None or re.search(name, finfo.name) is not None)

    def lus(self, name=None):
        """
        Obtain details for a specific lexical unit.

--
           - Absorb_heat: "The potatoes have to bake for more than 30 minutes."

        These constitute three different LUs, with different
        definitions.

        Multiword expressions such as "given name" and hyphenated words
        like "shut-eye" can also be LUs. Idiomatic phrases such as
        "middle of nowhere" and "give the slip (to)" are also defined as
        LUs in the appropriate frames ("Isolated_places" and "Evading",
        respectively), and their internal structure is not analyzed.

--
        else:
            return PrettyLazyMap(self.lu, luIDs)

    def lu_ids_and_names(self, name=None):
        """
        Uses the LU index, which is much faster than looking up each LU definition 
        if only the names and IDs are needed.
        """
        if not self._lu_idx:
            self._buildluindex()
        return dict((luID, luinfo.name) for luID,luinfo in self._lu_idx.items() if name is None or re.search(name, luinfo.name) is not None)

    def documents(self, name=None):
        """
        Return a list of the annotated documents in Framenet.

--
        else:
            return PrettyList(x for x in ftlist if re.search(name, x['filename']) is not None)

    def frame_relation_types(self):
        """
        Obtain a list of frame relation types.

--
            self._buildrelationindex()
        return self._freltyp_idx.values()

    def frame_relations(self, frame=None, frame2=None, type=None):
        """
        :param frame: (optional) frame object, name, or ID; only relations involving 
        this frame will be returned
--
        return PrettyList(sorted(rels, 
                key=lambda frel: (frel.type.ID, frel.superFrameName, frel.subFrameName)))
        
    def fe_relations(self):
        """
        Obtain a list of frame element relations.

--
                key=lambda ferel: (ferel.type.ID, ferel.frameRelation.superFrameName, 
                    ferel.superFEName, ferel.frameRelation.subFrameName, ferel.subFEName)))

    def semtypes(self):
        """
        Obtain a list of semantic types.

--
        >>> len(stypes)
        73
        >>> sorted(stypes[0].keys())
        ['ID', '_type', 'abbrev', 'definition', 'name', 'rootType', 'subTypes', 'superType']

        :return: A list of all of the semantic types in framenet
        :rtype: list(dict)
--
            self._loadsemtypes()
        return PrettyList(self._semtypes[i] for i in self._semtypes if isinstance(i, int))

    def _load_xml_attributes(self, d, elt):
        """
        Extracts a subset of the attributes from the given element and
        returns them in a dictionary.
--

        return d

    def _strip_tags(self, data):
        """
        Gets rid of all tags and newline characters from the given input

--
            data = data.replace('<x>', '')
            data = data.replace('</x>', '')

            # Get rid of <def-root> and </def-root> tags
            data = data.replace('<def-root>', '')
            data = data.replace('</def-root>', '')

            data = data.replace('\n', ' ')
        except AttributeError:
--

        return data

    def _handle_elt(self, elt, tagspec=None):
        """Extracts and returns the attributes of the given element"""
        return self._load_xml_attributes(AttrDict(), elt)

    def _handle_fulltextindex_elt(self, elt, tagspec=None):
        """
        Extracts corpus/document info from the fulltextIndex.xml file.

--

        return retlist

    def _handle_frame_elt(self, elt, ignorekeys=[]):
        """Load the info for a Frame from an frame xml file"""
        frinfo = self._load_xml_attributes(AttrDict(), elt)

        frinfo['_type'] = 'frame'
        frinfo['definition'] = ""
        frinfo['FE'] = PrettyDict()
        frinfo['FEcoreSets'] = []
        frinfo['lexUnit'] = PrettyDict()
--
                del frinfo[k]

        for sub in elt:
            if sub.tag.endswith('definition') and 'definition' not in ignorekeys:
                frinfo['definition'] = self._strip_tags(sub.text)
            elif sub.tag.endswith('FE') and 'FE' not in ignorekeys:
                feinfo = self._handle_fe_elt(sub)
                frinfo['FE'][feinfo.name] = feinfo
--
        
        return frinfo

    def _handle_fecoreset_elt(self, elt):
        """Load fe coreset info from xml."""
        info = self._load_xml_attributes(AttrDict(), elt)
        tmp = []
--

        return tmp

    def _handle_framerelationtype_elt(self, elt, *args):
        """Load frame-relation element and its child fe-relation elements from frRelation.xml."""
        info = self._load_xml_attributes(AttrDict(), elt)
        info['_type'] = 'framerelationtype'
--

        return info
    
    def _handle_framerelation_elt(self, elt):
        """Load frame-relation element and its child fe-relation elements from frRelation.xml."""
        info = self._load_xml_attributes(AttrDict(), elt)
        assert info['superFrameName']!=info['subFrameName'],(elt,info)
--

        return info

    def _handle_fulltextannotation_elt(self, elt):
        """Load full annotation info for a document from its xml
        file. The main element (fullTextAnnotation) contains a 'header'
        element (which we ignore here) and a bunch of 'sentence'
--

        return info

    def _handle_fulltext_sentence_elt(self, elt):
        """Load information from the given 'sentence' element. Each
        'sentence' element contains a "text" and an "annotationSet" sub
        element."""
--

        return info

    def _handle_fulltextannotationset_elt(self, elt):
        """Load information from the given 'annotationSet' element. Each
        'annotationSet' contains several "layer" elements."""
        info = self._load_xml_attributes(AttrDict(), elt)
--

        return info

    def _handle_fulltextlayer_elt(self, elt):
        """Load information from the given 'layer' element. Each
        'layer' contains several "label" elements."""
        info = self._load_xml_attributes(AttrDict(), elt)
--

        return info

    def _handle_framelexunit_elt(self, elt):
        """Load the lexical unit info from an xml element in a frame's xml file."""
        luinfo = AttrDict()
        luinfo['_type'] = 'lu'
        luinfo = self._load_xml_attributes(luinfo, elt)
        luinfo["definition"] = ""
        luinfo["sentenceCount"] = PrettyDict()
        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes
        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes

        for sub in elt:
            if sub.tag.endswith('definition'):
                luinfo['definition'] = self._strip_tags(sub.text)
            elif sub.tag.endswith('sentenceCount'):
                luinfo['sentenceCount'] = self._load_xml_attributes(
                    PrettyDict(), sub)
--

        return luinfo

    def _handle_lexunit_elt(self, elt, ignorekeys):
        """
        Load full info for a lexical unit from its xml file. 
        This should only be called when accessing corpus annotations 
--
        """
        luinfo = self._load_xml_attributes(AttrDict(), elt)
        luinfo['_type'] = 'lu'
        luinfo['definition'] = ""
        luinfo['subCorpus'] = PrettyList()
        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes
        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes
--
                continue  # not used
            elif sub.tag.endswith('valences'):
                continue  # not used
            elif sub.tag.endswith('definition') and 'definition' not in ignorekeys:
                luinfo['definition'] = self._strip_tags(sub.text)
            elif sub.tag.endswith('subCorpus') and 'subCorpus' not in ignorekeys:
                sc = self._handle_lusubcorpus_elt(sub)
                if sc is not None:
--

        return luinfo

    def _handle_lusubcorpus_elt(self, elt):
        """Load a subcorpus of a lexical unit from the given xml."""
        sc = AttrDict()
        try:
--

        return sc

    def _handle_lusentence_elt(self, elt):
        """Load a sentence from a subcorpus of an LU from xml."""
        info = self._load_xml_attributes(AttrDict(), elt)
        info['_type'] = 'lusentence'
--
                    info['annotationSet'].append(annset)
        return info

    def _handle_luannotationset_elt(self, elt):
        """Load an annotation set from a sentence in an subcorpus of an LU"""
        info = self._load_xml_attributes(AttrDict(), elt)
        info['_type'] = 'luannotationset'
--
                    info['layer'].append(l)
        return info

    def _handle_lulayer_elt(self, elt):
        """Load a layer from an annotation set"""
        layer = self._load_xml_attributes(AttrDict(), elt)
        layer['_type'] = 'lulayer'
--
                    layer['label'].append(l)
        return layer

    def _handle_fe_elt(self, elt):
        feinfo = self._load_xml_attributes(AttrDict(), elt)
        feinfo['_type'] = 'fe'
        feinfo['definition'] = ""
        feinfo['semType'] = None
        feinfo['requiresFE'] = None
        feinfo['excludesFE'] = None
        for sub in elt:
            if sub.tag.endswith('definition'):
                feinfo['definition'] = self._strip_tags(sub.text)
            elif sub.tag.endswith('semType'):
                stinfo = self._load_xml_attributes(AttrDict(), sub)
                feinfo['semType'] = self.semtype(stinfo.ID)
--

        return feinfo

    def _handle_semtype_elt(self, elt, tagspec=None):
        semt = self._load_xml_attributes(AttrDict(), elt)
        semt['_type'] = 'semtype'
        semt['superType'] = None
        semt['subTypes'] = PrettyList()
        for sub in elt:
            if sub.text is not None:
                semt['definition'] = self._strip_tags(sub.text)
            else:
                supertypeinfo = self._load_xml_attributes(AttrDict(), sub)
                semt['superType'] = supertypeinfo
--
#
# Demo
#
def demo():
    from nltk.corpus import framenet as fn

    #
--
    """
    Reader for simple part-of-speech tagged corpora.  Paragraphs are
    assumed to be split using blank lines.  Sentences and words can be
    tokenized using the default tokenizers, or by custom tokenizers
    specified as parameters to the constructor.  Words are parsed
    using ``nltk.tag.str2tuple``.  By default, ``'/'`` is used as the
    separator.  I.e., words should have the form::

       word1/tag1 word2/tag2 word3/tag3 ...
--
    constructor.  Part of speech tags are case-normalized to upper
    case.
    """
    def __init__(self, root, fileids,
                 sep='/', word_tokenizer=WhitespaceTokenizer(),
                 sent_tokenizer=RegexpTokenizer('\n', gaps=True),
                 para_block_reader=read_blankline_block,
--
        self._para_block_reader = para_block_reader
        self._tagset = tagset

    def raw(self, fileids=None):
        """
        :return: the given file(s) as a single string.
        :rtype: str
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        """
        :return: the given file(s) as a list of words
            and punctuation symbols.
--
                                        None)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
--
                                        None)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def paras(self, fileids=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
                                        None)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_words(self, fileids=None, tagset=None):
        """
        :return: the given file(s) as a list of tagged
            words and punctuation symbols, encoded as tuples
--
                                        tag_mapping_function)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_sents(self, fileids=None, tagset=None):
        """
        :return: the given file(s) as a list of
            sentences, each encoded as a list of ``(word,tag)`` tuples.
--
                                        tag_mapping_function)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def tagged_paras(self, fileids=None, tagset=None):
        """
        :return: the given file(s) as a list of
            paragraphs, each encoded as a list of sentences, which are
--
    A reader for part-of-speech tagged corpora whose documents are
    divided into categories based on their file identifiers.
    """
    def __init__(self, *args, **kwargs):
        """
        Initialize the corpus reader.  Categorization arguments
        (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to
--
        CategorizedCorpusReader.__init__(self, kwargs)
        TaggedCorpusReader.__init__(self, *args, **kwargs)

    def _resolve(self, fileids, categories):
        if fileids is not None and categories is not None:
            raise ValueError('Specify fileids or categories, not both')
        if categories is not None:
            return self.fileids(categories)
        else:
            return fileids
    def raw(self, fileids=None, categories=None):
        return TaggedCorpusReader.raw(
            self, self._resolve(fileids, categories))
    def words(self, fileids=None, categories=None):
        return TaggedCorpusReader.words(
            self, self._resolve(fileids, categories))
    def sents(self, fileids=None, categories=None):
        return TaggedCorpusReader.sents(
            self, self._resolve(fileids, categories))
    def paras(self, fileids=None, categories=None):
        return TaggedCorpusReader.paras(
            self, self._resolve(fileids, categories))
    def tagged_words(self, fileids=None, categories=None, tagset=None):
        return TaggedCorpusReader.tagged_words(
            self, self._resolve(fileids, categories), tagset)
    def tagged_sents(self, fileids=None, categories=None, tagset=None):
        return TaggedCorpusReader.tagged_sents(
            self, self._resolve(fileids, categories), tagset)
    def tagged_paras(self, fileids=None, categories=None, tagset=None):
        return TaggedCorpusReader.tagged_paras(
            self, self._resolve(fileids, categories), tagset)

--
    ``TaggedCorpusView`` objects are typically created by
    ``TaggedCorpusReader`` (not directly by nltk users).
    """
    def __init__(self, corpus_file, encoding, tagged, group_by_sent,
                 group_by_para, sep, word_tokenizer, sent_tokenizer,
                 para_block_reader, tag_mapping_function=None):
        self._tagged = tagged
--
        self._tag_mapping_function = tag_mapping_function
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)

    def read_block(self, stream):
        """Reads one paragraph at a time."""
        block = []
        for para_str in self._para_block_reader(stream):
--
    ``self.paras()`` and ``self.tagged_paras()`` contains a single
    sentence.
    """
    def __init__(self, root, fileids, encoding='utf8', tagset=None):
        TaggedCorpusReader.__init__(
            self, root, fileids, sep='_',
            word_tokenizer=LineTokenizer(),
--
            encoding=encoding,
            tagset=tagset)

    def _read_block(self, stream):
        return read_regexp_block(stream, r'.*', r'.*_\.')

class TimitTaggedCorpusReader(TaggedCorpusReader):
    """
    A corpus reader for tagged sentences that are included in the TIMIT corpus.
    """
    def __init__(self, *args, **kwargs):
        TaggedCorpusReader.__init__(
            self, para_block_reader=read_timit_block, *args, **kwargs)

    def paras(self):
        raise NotImplementedError('use sents() instead')

    def tagged_paras(self):
        raise NotImplementedError('use tagged_sents() instead')
--

import os
import re
from collections import defaultdict

from nltk import compat
from nltk.data import PathPointer, FileSystemPathPointer, ZipFilePathPointer
--
    identified by its ``file identifier``, which is the relative path
    to the file from the root directory.

    A separate subclass is be defined for each corpus format.  These
    subclasses define one or more methods that provide 'views' on the
    corpus contents, such as ``words()`` (for a list of words) and
    ``parsed_sents()`` (for a list of parsed sentences).  Called with
    no arguments, these methods will return the contents of the entire
    corpus.  For most corpora, these methods define one or more
    selection arguments, such as ``fileids`` or ``categories``, which can
    be used to select which portion of the corpus should be returned.
    """

    def __init__(self, root, fileids, encoding='utf8', tagset=None):
        """
        :type root: PathPointer or str
        :param root: A path pointer identifying the root directory for
--
            strings; or implicitly, as a regular expression over file
            paths.  The absolute path for each file will be constructed
            by joining the reader's root to each file name.
        :param encoding: The default unicode encoding for the files
            that make up the corpus.  The value of ``encoding`` can be any
            of the following:
            - A string: ``encoding`` is the encoding name for all files.
--
            encoding = encoding_dict

        self._encoding = encoding
        """The default unicode encoding for the fileids that make up
           this corpus.  If ``encoding`` is None, then the file
           contents are processed using byte strings."""
        self._tagset = tagset

    def __repr__(self):
        if isinstance(self._root, ZipFilePathPointer):
            path = '%s/%s' % (self._root.zipfile.filename, self._root.entry)
        else:
            path = '%s' % self._root.path
        return '<%s in %r>' % (self.__class__.__name__, path)

    def readme(self):
        """
        Return the contents of the corpus README file, if it exists.
        """

        return self.open("README").read()

    def fileids(self):
        """
        Return a list of file identifiers for the fileids that make up
        this corpus.
        """
        return self._fileids

    def abspath(self, fileid):
        """
        Return the absolute path for the given file.

--
        """
        return self._root.join(fileid)

    def abspaths(self, fileids=None, include_encoding=False,
                 include_fileid=False):
        """
        Return a list of the absolute paths for all fileids in this corpus;
--
        else:
            return paths

    def open(self, file):
        """
        Return an open stream that can be used to read the given file.
        If the file's encoding is not None, then the stream will
--
        stream = self._root.join(file).open(encoding)
        return stream

    def encoding(self, file):
        """
        Return the unicode encoding for the given corpus file, if known.
        If the encoding is unknown, or if the given file should be
--
        else:
            return self._encoding

    def _get_root(self): return self._root
    root = property(_get_root, doc="""
        The directory where this corpus is stored.

--
class CategorizedCorpusReader(object):
    """
    A mixin class used to aid in the implementation of corpus readers
    for categorized corpora.  This class defines the method
    ``categories()``, which returns a list of the categories for the
    corpus or for a specified set of fileids; and overrides ``fileids()``
    to take a ``categories`` argument, restricting the set of fileids to
--
        select which fileids should be included in the returned view.
    """

    def __init__(self, kwargs):
        """
        Initialize this mapping based on keyword arguments, as
        follows:
--
            raise ValueError('Specify exactly one of: cat_pattern, '
                             'cat_map, cat_file.')

    def _init(self):
        self._f2c = defaultdict(set)
        self._c2f = defaultdict(set)

        if self._pattern is not None:
            for file_id in self._fileids:
--
                for category in categories.split(self._delimiter):
                    self._add(file_id, category)

    def _add(self, file_id, category):
        self._f2c[file_id].add(category)
        self._c2f[category].add(file_id)

    def categories(self, fileids=None):
        """
        Return a list of the categories that are defined for this corpus,
        or for the file(s) if it is given.
        """
        if self._f2c is None:
--
            fileids = [fileids]
        return sorted(set.union(*[self._f2c[d] for d in fileids]))

    def fileids(self, categories=None):
        """
        Return a list of file identifiers for the files that make up
        this corpus, or that make up the given category(s) if specified.
--
class SyntaxCorpusReader(CorpusReader):
    """
    An abstract base class for reading corpora consisting of
    syntactically parsed text.  Subclasses should define:

      - ``__init__``, which specifies the location of the corpus
        and a method for detecting the sentence blocks in corpus files.
--
      - ``_parse``, which takes a block and returns a list of parsed
        sentences.
    """
    def _parse(self, s):
        raise NotImplementedError()
    def _word(self, s):
        raise NotImplementedError()
    def _tag(self, s):
        raise NotImplementedError()
    def _read_block(self, stream):
        raise NotImplementedError()

    def raw(self, fileids=None):
        if fileids is None: fileids = self._fileids
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def parsed_sents(self, fileids=None):
        reader = self._read_parsed_sent_block
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])

    def tagged_sents(self, fileids=None, tagset=None):
        def reader(stream):
            return self._read_tagged_sent_block(stream, tagset)
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        reader = self._read_sent_block
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])

    def tagged_words(self, fileids=None, tagset=None):
        def reader(stream):
            return self._read_tagged_word_block(stream, tagset)
        return concat([StreamBackedCorpusView(fileid, reader, encoding=enc)
                       for fileid, enc in self.abspaths(fileids, True)])

    def words(self, fileids=None):
        return concat([StreamBackedCorpusView(fileid,
                                              self._read_word_block,
                                              encoding=enc)
--
    #------------------------------------------------------------
    #{ Block Readers

    def _read_word_block(self, stream):
        return sum(self._read_sent_block(stream), [])

    def _read_tagged_word_block(self, stream, tagset=None):
        return sum(self._read_tagged_sent_block(stream, tagset), [])

    def _read_sent_block(self, stream):
        return list(filter(None, [self._word(t) for t in self._read_block(stream)]))

    def _read_tagged_sent_block(self, stream, tagset=None):
        return list(filter(None, [self._tag(t, tagset)
                             for t in self._read_block(stream)]))

    def _read_parsed_sent_block(self, stream):
        return list(filter(None, [self._parse(t) for t in self._read_block(stream)]))

    #} End of Block Readers
--
    Reader for corpora of word-aligned sentences.  Tokens are assumed
    to be separated by whitespace.  Sentences begin on separate lines.
    """
    def __init__(self, root, fileids,
                 sep='/', word_tokenizer=WhitespaceTokenizer(),
                 sent_tokenizer=RegexpTokenizer('\n', gaps=True),
                 alignedsent_block_reader=read_alignedsent_block,
--
        self._sent_tokenizer = sent_tokenizer
        self._alignedsent_block_reader = alignedsent_block_reader

    def raw(self, fileids=None):
        """
        :return: the given file(s) as a single string.
        :rtype: str
--
        elif isinstance(fileids, compat.string_types): fileids = [fileids]
        return concat([self.open(f).read() for f in fileids])

    def words(self, fileids=None):
        """
        :return: the given file(s) as a list of words
            and punctuation symbols.
--
                                             self._alignedsent_block_reader)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def sents(self, fileids=None):
        """
        :return: the given file(s) as a list of
            sentences or utterances, each encoded as a list of word
--
                                             self._alignedsent_block_reader)
                       for (fileid, enc) in self.abspaths(fileids, True)])

    def aligned_sents(self, fileids=None):
        """
        :return: the given file(s) as a list of AlignedSent objects.
        :rtype: list(AlignedSent)
--
    ``AlignedSentCorpusView`` objects are typically created by
    ``AlignedCorpusReader`` (not directly by nltk users).
    """
    def __init__(self, corpus_file, encoding, aligned, group_by_sent,
                 word_tokenizer, sent_tokenizer, alignedsent_block_reader):
        self._aligned = aligned
        self._group_by_sent = group_by_sent
--
        self._alignedsent_block_reader = alignedsent_block_reader
        StreamBackedCorpusView.__init__(self, corpus_file, encoding=encoding)

    def read_block(self, stream):
        block = [self._word_tokenizer.tokenize(sent_str)
                 for alignedsent_str in self._alignedsent_block_reader(stream)
                 for sent_str in self._sent_tokenizer.tokenize(alignedsent_str)]
--
    """
    A proxy object which is used to stand in for a corpus object
    before the corpus is loaded.  This allows NLTK to create an object
    for each corpus, but defer the costs associated with loading those
    corpora until the first time that they're actually accessed.

    The first time this object is accessed in any way, it will load
--
    package (or modified ``nltk.data.path`` to point to its location),
    they can then use the corpus object without restarting python.
    """
    def __init__(self, name, reader_cls, *args, **kwargs):
        from nltk.corpus.reader.api import CorpusReader
        assert issubclass(reader_cls, CorpusReader)
        self.__name = self.__name__ = name
--
        self.__args = args
        self.__kwargs = kwargs

    def __load(self):
        # Find the corpus root directory.
        zip_name = re.sub(r'(([^/]*)(/.*)?)', r'\2.zip/\1/', self.__name)
        if TRY_ZIPFILE_FIRST:
--
        # _unload support: assign __dict__ and __class__ back, then do GC.
        # after reassigning __dict__ there shouldn't be any references to
        # corpus data so the memory should be deallocated after gc.collect()
        def _unload(self):
            lazy_reader = LazyCorpusLoader(name, reader_cls, *args, **kwargs)
            self.__dict__ = lazy_reader.__dict__
            self.__class__ = lazy_reader.__class__
--

        self._unload = _make_bound_method(_unload, self)

    def __getattr__(self, attr):

        # Fix for inspect.isclass under Python 2.6
        # (see http://bugs.python.org/issue1225107).
--
        # __class__ to something new:
        return getattr(self, attr)

    def __repr__(self):
        return '<%s in %r (not loaded yet)>' % (
            self.__reader_cls.__name__, '.../corpora/'+self.__name)

    def _unload(self):
        # If an exception occures during corpus loading then
        # '_unload' method may be unattached, so __getattr__ can be called;
        # we shouldn't trigger corpus loading again in this case.
        pass


def _make_bound_method(func, self):
    """
    Magic for creating bound methods (used for _unload).
    """
    class Foo(object):
        def meth(self): pass
    f = Foo()
    bound_method = type(f.meth)

--
from nltk import compat


def _estimator(fdist, bins):
    """
    Default estimator function using a SimpleGoodTuringProbDist.
    """
    # can't be an instance method of NgramModel as they
    # can't be pickled either.
--
    """

    # add cutoff
    def __init__(self, n, train, pad_left=True, pad_right=False,
                 estimator=None, *estimator_args, **estimator_kwargs):
        """
        Create an ngram language model to capture patterns in n consecutive
--
            self._backoff = NgramModel(n-1, train, pad_left, pad_right,
                                       estimator, *estimator_args, **estimator_kwargs)

    def prob(self, word, context):
        """
        Evaluate the probability of this word in this context using Katz Backoff.

--
        else:
            return self._alpha(context) * self._backoff.prob(word, context[1:])

    def _alpha(self, tokens):
        return self._beta(tokens) / self._backoff._beta(tokens[1:])

    def _beta(self, tokens):
        return (self[tokens].discount() if tokens in self else 1)

    def logprob(self, word, context):
        """
        Evaluate the (negative) log probability of this word in this context.

--

        return -log(self.prob(word, context), 2)

    def choose_random_word(self, context):
        '''
        Randomly select a word that is likely to appear in this context.

--

    # NB, this will always start with same word if the model
    # was trained on a single text
    def generate(self, num_words, context=()):
        '''
        Generate random text based on the language model.

--
            text.append(self._generate_one(text))
        return text

    def _generate_one(self, context):
        context = (self._lpad + tuple(context))[-self._n+1:]
        # print "Context (%d): <%s>" % (self._n, ','.join(context))
        if context in self:
--
        else:
            return '.'

    def entropy(self, text):
        """
        Calculate the approximate cross-entropy of the n-gram model for a
        given evaluation text.
--
            e += self.logprob(token, context)
        return e / float(len(text) - (self._n-1))

    def perplexity(self, text):
        """
        Calculates the perplexity of the given text.
        This is simply 2 ** cross-entropy for the text.
--

        return pow(2.0, self.entropy(text))

    def __contains__(self, item):
        return tuple(item) in self._model

    def __getitem__(self, item):
        return self._model[tuple(item)]

    def __repr__(self):
        return '<NgramModel with %d %d-grams>' % (len(self._ngrams), self._n)


def teardown_module(module=None):
    from nltk.corpus import brown
    brown._unload()

--
    A processing interface for assigning a probability to the next word.
    """

    def __init__(self):
        '''Create a new language model.'''
        raise NotImplementedError()

    def prob(self, word, context):
        '''Evaluate the probability of this word in this context.'''
        raise NotImplementedError()

    def logprob(self, word, context):
        '''Evaluate the (negative) log probability of this word in this context.'''
        raise NotImplementedError()

    def choose_random_word(self, context):
        '''Randomly select a word that is likely to appear in this context.'''
        raise NotImplementedError()

    def generate(self, n):
        '''Generate n words of text from the language model.'''
        raise NotImplementedError()

    def entropy(self, text):
        '''Evaluate the total entropy of a message with respect to the model.
        This is the sum of the log probability of each word in the message.'''
        raise NotImplementedError()
--
from __future__ import print_function, division, unicode_literals

from math import log
from collections import defaultdict
from functools import reduce
from itertools import islice
import re
--
class ContextIndex(object):
    """
    A bidirectional index between words and their 'contexts' in a text.
    The context of a word is usually defined to be the words that occur
    in a fixed window around the word; but other definitions may also
    be used by providing a custom context function.
    """
    @staticmethod
    def _default_context(tokens, i):
        """One left token and one right token, normalized to lowercase"""
        left = (tokens[i-1].lower() if i != 0 else '*START*')
        right = (tokens[i+1].lower() if i != len(tokens) - 1 else '*END*')
        return (left, right)

    def __init__(self, tokens, context_func=None, filter=None, key=lambda x:x):
        self._key = key
        self._tokens = tokens
        if context_func:
            self._context_func = context_func
        else:
            self._context_func = self._default_context
        if filter:
            tokens = [t for t in tokens if filter(t)]
        self._word_to_contexts = CFD((self._key(w), self._context_func(tokens, i))
--
        self._context_to_words = CFD((self._context_func(tokens, i), self._key(w))
                                     for i, w in enumerate(tokens))

    def tokens(self):
        """
        :rtype: list(str)
        :return: The document that this context index was
--
        """
        return self._tokens

    def word_similarity_dict(self, word):
        """
        Return a dictionary mapping from words to 'similarity scores,'
        indicating how often these two words occur in the same
--

        return scores

    def similar_words(self, word, n=20):
        scores = defaultdict(int)
        for c in self._word_to_contexts[self._key(word)]:
            for w in self._context_to_words[c]:
                if w != word:
                    scores[w] += self._context_to_words[c][word] * self._context_to_words[c][w]
        return sorted(scores, key=scores.get, reverse=True)[:n]

    def common_contexts(self, words, fail_on_unknown=False):
        """
        Find contexts where the specified words can all appear; and
        return a frequency distribution mapping each context to the
--
    An index that can be used to look up the offset locations at which
    a given word occurs in a document.
    """
    def __init__(self, tokens, key=lambda x:x):
        """
        Construct a new concordance index.

--
        self._key = key
        """Function mapping each token to an index key (or None)."""

        self._offsets = defaultdict(list)
        """Dictionary mapping words (or keys) to lists of offset
           indices."""

--
            word = self._key(word)
            self._offsets[word].append(index)

    def tokens(self):
        """
        :rtype: list(str)
        :return: The document that this concordance index was
--
        """
        return self._tokens

    def offsets(self, word):
        """
        :rtype: list(int)
        :return: A list of the offset positions at which the given
--
        word = self._key(word)
        return self._offsets[word]

    def __repr__(self):
        return '<ConcordanceIndex for %d tokens (%d types)>' % (
            len(self._tokens), len(self._offsets))

    def print_concordance(self, word, width=75, lines=25):
        """
        Print a concordance for ``word`` with the specified context window.

        :param word: The target word
        :type word: str
        :param width: The width of each line, in characters (default=80)
        :type width: int
        :param lines: The number of lines to display (default=25)
        :type lines: int
        """
        half_width = (width - len(word) - 2) // 2
--
    brackets as nongrouping parentheses, in addition to matching the
    token boundaries; and to have ``'.'`` not match the angle brackets.
    """
    def __init__(self, tokens):
        self._raw = ''.join('<'+w+'>' for w in tokens)

    def findall(self, regexp):
        """
        Find instances of the regular expression in the text.
        The text is a list of tokens, and a regexp pattern to match
--
    >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))

    """
    # This defeats lazy loading, but makes things faster.  This
    # *shouldn't* be necessary because the corpus view *should* be
    # doing intelligent caching, but without this it's running slow.
    # Look into whether the caching is working correctly.
    _COPY_TOKENS = True

    def __init__(self, tokens, name=None):
        """
        Create a Text object.

--
    # Support item & slice access
    #////////////////////////////////////////////////////////////

    def __getitem__(self, i):
        if isinstance(i, slice):
            return self.tokens[i.start:i.stop]
        else:
            return self.tokens[i]

    def __len__(self):
        return len(self.tokens)

    #////////////////////////////////////////////////////////////
    # Interactive console methods
    #////////////////////////////////////////////////////////////

    def concordance(self, word, width=79, lines=25):
        """
        Print a concordance for ``word`` with the specified context window.
        Word matching is not case-sensitive.
--

        self._concordance_index.print_concordance(word, width, lines)

    def collocations(self, num=20, window_size=2):
        """
        Print collocations derived from the text, ignoring stopwords.

        :seealso: find_collocations
        :param num: The maximum number of collocations to print.
        :type num: int
        :param window_size: The number of tokens spanned by a collocation (default=2)
        :type window_size: int
        """
        if not ('_collocations' in self.__dict__ and self._num == num and self._window_size == window_size):
--
        colloc_strings = [w1+' '+w2 for w1, w2 in self._collocations]
        print(tokenwrap(colloc_strings, separator="; "))

    def count(self, word):
        """
        Count the number of times this word appears in the text.
        """
        return self.tokens.count(word)

    def index(self, word):
        """
        Find the index of the first occurrence of the word in the text.
        """
        return self.tokens.index(word)

    def readability(self, method):
        # code from nltk_contrib.readability
        raise NotImplementedError

    def generate(self, length=100):
        """
        Print random text, generated using a trigram language model.

        :param length: The length of text to generate (default=100)
        :type length: int
        :seealso: NgramModel
        """
--
        text = self._trigram_model.generate(length)
        print(tokenwrap(text))

    def similar(self, word, num=20):
        """
        Distributional similarity: find other words which appear in the
        same contexts as the specified word; list most similar words first.

        :param word: The word used to seed the similarity search
        :type word: str
        :param num: The number of words to generate (default=20)
        :type num: int
        :seealso: ContextIndex.similar_words()
        """
--
            print("No matches")


    def common_contexts(self, words, num=20):
        """
        Find contexts where the specified words appear; list
        most frequent common contexts first.

        :param word: The word used to seed the similarity search
        :type word: str
        :param num: The number of words to generate (default=20)
        :type num: int
        :seealso: ContextIndex.common_contexts()
        """
--
        except ValueError as e:
            print(e)

    def dispersion_plot(self, words):
        """
        Produce a plot showing the distribution of the words through the text.
        Requires pylab to be installed.
--
        from nltk.draw import dispersion_plot
        dispersion_plot(self, words)

    def plot(self, *args):
        """
        See documentation for FreqDist.plot()
        :seealso: nltk.prob.FreqDist.plot()
        """
        self.vocab().plot(*args)

    def vocab(self):
        """
        :seealso: nltk.prob.FreqDist
        """
--
            self._vocab = FreqDist(self)
        return self._vocab

    def findall(self, regexp):
        """
        Find instances of the regular expression in the text.
        The text is a list of tokens, and a regexp pattern to match
--
    #////////////////////////////////////////////////////////////

    _CONTEXT_RE = re.compile('\w+|[\.\!\?]')
    def _context(self, tokens, i):
        """
        One left & one right token, both case-normalized.  Skip over
        non-sentence-final punctuation.  Used by the ``ContextIndex``
--
    # String Display
    #////////////////////////////////////////////////////////////

    def __str__(self):
        return '<Text: %s>' % self.name

    def __repr__(self):
        return '<Text: %s>' % self.name


--
    Iterating over a TextCollection produces all the tokens of all the
    texts in order.
    """
    def __init__(self, source, name=None):
        if hasattr(source, 'words'): # bridge to the text corpus reader
            source = [source.words(f) for f in source.fileids()]

--
        Text.__init__(self, LazyConcatenation(source))
        self._idf_cache = {}

    def tf(self, term, text, method=None):
        """ The frequency of the term in text. """
        return text.count(term) / len(text)

    def idf(self, term, method=None):
        """ The number of texts in the corpus divided by the
        number of texts that the term appears in.
        If a term does not appear in the corpus, 0.0 is returned. """
--
            self._idf_cache[term] = idf
        return idf

    def tf_idf(self, term, text):
        return self.tf(term, text) * self.idf(term)

def demo():
    from nltk.corpus import brown
    text = Text(brown.words(categories='news'))
    print(text)
--
text9 = Text(gutenberg.words('chesterton-thursday.txt'))
print("text9:", text9.name)

def texts():
    print("text1:", text1.name)
    print("text2:", text2.name)
    print("text3:", text3.name)
--
         "sunset", "side", "of", "London", ",", "as", "red", "and",
         "ragged", "as", "a", "cloud", "of", "sunset", "."]

def sents():
    print("sent1:", " ".join(sent1))
    print("sent2:", " ".join(sent2))
    print("sent3:", " ".join(sent3))
--
    """
    Class for reading and processing standard format marker files and strings.
    """
    def __init__(self, filename=None, encoding=None):
        self._encoding = encoding
        if filename is not None:
            self.open(filename)

    def open(self, sfm_file):
        """
        Open a standard format marker file for sequential reading.

--
        else:
            self._file = codecs.open(sfm_file, 'rU', self._encoding)

    def open_string(self, s):
        """
        Open a standard format marker string for sequential reading.

--
        """
        self._file = StringIO(s)

    def raw_fields(self):
        """
        Return an iterator that returns the next field in a (marker, value)
        tuple. Linebreaks and trailing white space are preserved except
--
        self.line_num += 1
        yield (mkr, join_string.join(value_lines))

    def fields(self, strip=True, unwrap=True, encoding=None, errors='strict', unicode_fields=None):
        """
        Return an iterator that returns the next field in a ``(marker, value)``
        tuple, where ``marker`` and ``value`` are unicode strings if an ``encoding``
--
        :type errors: str
        :param unicode_fields: Set of marker names whose values are UTF-8 encoded.
            Ignored if encoding is None. If the whole file is UTF-8 encoded set
            ``encoding='utf8'`` and leave ``unicode_fields`` with its default
            value of None.
        :type unicode_fields: sequence
        :rtype: iter(tuple(str, str))
--
                val = val.rstrip()
            yield (mkr, val)

    def close(self):
        """Close a previously opened standard format marker file or string."""
        self._file.close()
        try:
--
            pass

class ToolboxData(StandardFormat):
    def parse(self, grammar=None,  **kwargs):
        if grammar:
            return self._chunk_parse(grammar=grammar,  **kwargs)
        else:
            return self._record_parse(**kwargs)

    def _record_parse(self, key=None, **kwargs):
        """
        Returns an element tree structure corresponding to a toolbox data file with
        all markers at the same level.
--
            </toolbox_data>

        :param key: Name of key marker at the start of each record. If set to
            None (the default value) the first marker that doesn't begin with
            an underscore is assumed to be the key.
        :type key: str
        :param kwargs: Keyword arguments passed to ``StandardFormat.fields()``
--
        builder.end('toolbox_data')
        return builder.close()

    def _tree2etree(self, parent):
        from nltk.tree import Tree

        root = Element(parent.node)
--
                e.text = text
        return root

    def _chunk_parse(self, grammar=None, top_node='record', trace=0, **kwargs):
        """
        Returns an element tree structure corresponding to a toolbox data file
        parsed according to the chunk grammar.
--

_is_value = re.compile(r"\S")

def to_sfm_string(tree, encoding=None, errors='strict', unicode_fields=None):
    """
    Return a string with a standard format representation of the toolbox
    data in tree (tree can be a toolbox database or a single record).
--
class ToolboxSettings(StandardFormat):
    """This class is the base class for settings files."""

    def __init__(self):
        super(ToolboxSettings, self).__init__()

    def parse(self, encoding=None, errors='strict', **kwargs):
        """
        Return the contents of toolbox settings file with a nested structure.

--
                builder.end(mkr)
        return builder.close()

def to_settings_string(tree, encoding=None, errors='strict', unicode_fields=None):
    # write XML to file
    l = list()
    _to_settings_string(tree.getroot(), l, encoding=encoding, errors=errors, unicode_fields=unicode_fields)
    return ''.join(l)

def _to_settings_string(node, l, **kwargs):
    # write XML to file
    tag = node.tag
    text = node.text
--
        l.append('\\-%s\n' % tag)
    return

def remove_blanks(elem):
    """
    Remove all elements and subelements with no text and no child elements.

--
            out.append(child)
    elem[:] = out

def add_default_fields(elem, default_fields):
    """
    Add blank elements and subelements specified in default_fields.

    :param elem: toolbox data in an elementtree structure
    :type elem: ElementTree._ElementInterface
    :param default_fields: fields to add to each type of element and subelement
    :type default_fields: dict(tuple)
    """
    for field in default_fields.get(elem.tag,  []):
        if elem.find(field) is None:
            SubElement(elem, field)
    for child in elem:
        add_default_fields(child, default_fields)

def sort_fields(elem, field_orders):
    """
    Sort the elements and subelements in order specified in field_orders.

--
            order_key[subfield] = i
    _sort_fields(elem, order_dicts)

def _sort_fields(elem, orders_dicts):
    """sort the children of elem"""
    try:
        order = orders_dicts[elem.tag]
--
        if len(child):
            _sort_fields(child, orders_dicts)

def add_blank_lines(tree, blanks_before, blanks_between):
    """
    Add blank lines before all elements and subelements specified in blank_before.

--
                add_blank_lines(elem, blanks_before, blanks_between)
            last_elem = elem

def demo():
    from itertools import islice

#    zip_path = find('corpora/toolbox.zip')
--
which count the number of times that each outcome of an experiment
occurs.

The ``ProbDistI`` class defines a standard interface for "probability
distributions", which encode the probability of each outcome for an
experiment.  There are two types of probability distribution:

--
import array
from operator import itemgetter
from itertools import islice
from collections import defaultdict
from functools import reduce
from nltk import compat

--
##  Frequency Distributions
##//////////////////////////////////////////////////////

# [SB] inherit from defaultdict?
# [SB] for NLTK 3.0, inherit from collections.Counter?

@compat.python_2_unicode_compatible
--
    frequency distribution records the number of times each outcome of
    an experiment has occurred.  For example, a frequency distribution
    could be used to record the frequency of each word type in a
    document.  Formally, a frequency distribution can be defined as a
    function mapping from each sample to the number of times that
    sample occurred as an outcome.

--

    _N = 0

    def __init__(self, samples=None):
        """
        Construct a new frequency distribution.  If ``samples`` is
        given, then the frequency distribution will be initialized
--
        if samples:
            self.update(samples)

    def inc(self, sample, count=1):
        """
        Increment this FreqDist's count for the given sample.

--
        if count == 0: return
        self[sample] = self.get(sample,0) + count

    def __setitem__(self, sample, value):
        """
        Set this FreqDist's count for the given sample.

--
        # Invalidate the caches
        self._reset_caches()

    def N(self):
        """
        Return the total number of sample outcomes that have been
        recorded by this FreqDist.  For the number of unique
--
        """
        return self._N

    def B(self):
        """
        Return the total number of sample values (or "bins") that
        have counts greater than zero.  For the total
--
        """
        return len(self)

    def samples(self):
        """
        Return a list (or an iterator under Python 3.x) of all samples
        that have been recorded as outcomes by this frequency distribution.
--
        """
        return self.keys()

    def hapaxes(self):
        """
        Return a list of all samples that occur once (hapax legomena)

--
        """
        return [item for item in self if self[item] == 1]

    def Nr(self, r, bins=None):
        """
        Return the number of samples with count r.

--
        :param bins: The number of possible sample outcomes.  ``bins``
            is used to calculate Nr(0).  In particular, Nr(0) is
            ``bins-self.B()``.  If ``bins`` is not specified, it
            defaults to ``self.B()`` (so Nr(0) will be 0).
        :rtype: int
        """
        if r < 0: raise IndexError('FreqDist.Nr(): r must be non-negative')
--

        return self._Nr_cache.get(r, 0)

    def _Nr_nonzero(self):
        """
        Return (r, Nr(r)) tuples for all r such as Nr(r) > 0 (sorted by r).
        """
--
            self._cache_Nr_values()
        return sorted(self._Nr_cache.items())

    def _cache_Nr_values(self):
        Nr = defaultdict(int)
        for sample in self:
            c = self.get(sample, 0)
            if c == 0 and c not in Nr:
--
            Nr[c] += 1
        self._Nr_cache = Nr

    def _cumulative_frequencies(self, samples=None):
        """
        Return the cumulative frequencies of the specified samples.
        If no samples are specified, all counts are returned, starting
--

    # slightly odd nomenclature freq() if FreqDist does counts and ProbDist does probs,
    # here, freq() does probs
    def freq(self, sample):
        """
        Return the frequency of a given sample.  The frequency of a
        sample is defined as the count of that sample divided by the
        total number of sample outcomes that have been recorded by
        this FreqDist.  The count of a sample is defined as the
        number of times that sample outcome was recorded by this
        FreqDist.  Frequencies are always real numbers in the range
        [0, 1].
--
            return 0
        return float(self[sample]) / self._N

    def max(self):
        """
        Return the sample with the greatest number of outcomes in this
        frequency distribution.  If two or more samples have the same
        number of outcomes, return one of them; which sample is
        returned is undefined.  If no outcomes have occurred in this
        frequency distribution, return None.

        :return: The sample with the maximum number of outcomes in this
--
        """
        if self._max_cache is None:
            if len(self) == 0:
                raise ValueError('A FreqDist must have at least one sample before max is defined.')
            self._max_cache = max((a,b) for (b,a) in self.items())[1]
        return self._max_cache

    def plot(self, *args, **kwargs):
        """
        Plot samples from the frequency distribution
        displaying the most frequent sample first.  If an integer
--

        :param title: The title for the graph
        :type title: str
        :param cumulative: A flag to specify whether the plot is cumulative (default = False)
        :type title: bool
        """
        try:
--
        pylab.ylabel(ylabel)
        pylab.show()

    def tabulate(self, *args, **kwargs):
        """
        Tabulate the given samples from the frequency distribution (cumulative),
        displaying the most frequent sample first.  If an integer
--
        subset of the samples, beginning with m and stopping at n-1.
        (Requires Matplotlib to be installed.)

        :param samples: The samples to plot (default is all samples)
        :type samples: list
        """
        if len(args) == 0:
--
            print("%4d" % freqs[i], end=' ')
        print()

    def _sort_keys_by_value(self):
        if not self._item_cache:
            self._item_cache = sorted(dict.items(self), key=lambda x:(-x[1], x[0]))

    def keys(self):
        """
        Return the samples sorted in decreasing order of frequency.
        Returns a list in Python 2.x, and an iterator in Python 3.x.
--
        self._sort_keys_by_value()
        return map(itemgetter(0), self._item_cache)

    def values(self):
        """
        Return the samples sorted in decreasing order of frequency.
        Returns a list in Python 2.x, and an iterator in Python 3.x.
--
        self._sort_keys_by_value()
        return map(itemgetter(1), self._item_cache)

    def items(self):
        """
        Return the items sorted in decreasing order of frequency.

--
        self._sort_keys_by_value()
        return self._item_cache[:]

    def __iter__(self):
        """
        Return the samples sorted in decreasing order of frequency.

--
        """
        return iter(self.keys())

    def iterkeys(self):
        """
        Return the samples sorted in decreasing order of frequency.

--
        """
        return iter(self.keys())

    def itervalues(self):
        """
        Return the values sorted in decreasing order.

--
        """
        return iter(self.values())

    def iteritems(self):
        """
        Return the items sorted in decreasing order of frequency.

--
        self._sort_keys_by_value()
        return iter(self._item_cache)

    def copy(self):
        """
        Create a copy of this frequency distribution.

--
        """
        return self.__class__(self)

    def update(self, samples):
        """
        Update the frequency distribution with the provided list of samples.
        This is a faster way to add multiple samples to the distribution.
--
        for sample, count in sample_iter:
            self.inc(sample, count=count)

    def pop(self, other):
        self._N -= 1
        self._reset_caches()
        return dict.pop(self, other)

    def popitem(self):
        self._N -= 1
        self._reset_caches()
        return dict.popitem(self)

    def clear(self):
        self._N = 0
        self._reset_caches()
        dict.clear(self)

    def _reset_caches(self):
        self._Nr_cache = None
        self._max_cache = None
        self._item_cache = None

    def __add__(self, other):
        clone = self.copy()
        clone.update(other)
        return clone

    def __le__(self, other):
        if not isinstance(other, FreqDist):
            raise_unorderable_types("<=", self, other)
        return set(self).issubset(other) and all(self[key] <= other[key] for key in self)
--
    __lt__ = lambda self, other: self <= other and not self == other
    __gt__ = lambda self, other: not self <= other

    def __repr__(self):
        """
        Return a string representation of this FreqDist.

--
        """
        return '<FreqDist with %d samples and %d outcomes>' % (len(self), self.N())

    def __str__(self):
        """
        Return a string representation of this FreqDist.

--
            items.append('...')
        return '<FreqDist: %s>' % ', '.join(items)

    def __getitem__(self, sample):
        return self.get(sample, 0)

##//////////////////////////////////////////////////////
--
    experiment will have any given outcome.  For example, a
    probability distribution could be used to predict the probability
    that a token in a document will have a given type.  Formally, a
    probability distribution can be defined as a function mapping from
    samples to nonnegative real numbers, such that the sum of every
    number in the function's range is 1.0.  A ``ProbDist`` is often
    used to model the probability distribution of the experiment used
--
    """True if the probabilities of the samples in this probability
       distribution will always sum to one."""

    def __init__(self):
        if self.__class__ == ProbDistI:
            raise NotImplementedError("Interfaces can't be instantiated")

    def prob(self, sample):
        """
        Return the probability for a given sample.  Probabilities
        are always real numbers in the range [0, 1].
--
        """
        raise NotImplementedError()

    def logprob(self, sample):
        """
        Return the base 2 logarithm of the probability for a given sample.

--
        :type sample: any
        :rtype: float
        """
        # Default definition, in terms of prob()
        p = self.prob(sample)
        return (math.log(p, 2) if p != 0 else _NINF)

    def max(self):
        """
        Return the sample with the greatest probability.  If two or
        more samples have the same probability, return one of them;
        which sample is returned is undefined.

        :rtype: any
        """
        raise NotImplementedError()

    def samples(self):
        """
        Return a list of all samples that have nonzero probabilities.
        Use ``prob`` to find the probability of each sample.
--
        raise NotImplementedError()

    # cf self.SUM_TO_ONE
    def discount(self):
        """
        Return the ratio by which counts are discounted on average: c*/c

--
        """
        return 0.0

    # Subclasses should define more efficient implementations of this,
    # where possible.
    def generate(self):
        """
        Return a randomly selected sample from this probability distribution.
        The probability of returning each sample ``samp`` is equal to
--
    sample in a given set; and a zero probability to all other
    samples.
    """
    def __init__(self, samples):
        """
        Construct a new uniform probability distribution, that assigns
        equal probability to each sample in ``samples``.
--
        self._prob = 1.0/len(self._sampleset)
        self._samples = list(self._sampleset)

    def prob(self, sample):
        return (self._prob if sample in self._sampleset else 0)

    def max(self):
        return self._samples[0]

    def samples(self):
        return self._samples

    def __repr__(self):
        return '<UniformProbDist with %d samples>' % len(self._sampleset)


--
    will be between 0 and 1 with equal probability (uniform random distribution.
    Also called a continuous uniform distribution).
    """
    def __init__(self, samples):
        if len(samples) == 0:
            raise ValueError('A probability distribution must '+
                             'have at least one sample.')
--
        self._samples = list(self._probs.keys())

    @classmethod
    def unirand(cls, samples):
        """
        The key function that creates a randomized initial distribution
        that still sums to 1. Set as a dictionary of prob values so that
--

        return dict((s, randrow[i]) for i, s in enumerate(samples))

    def prob(self, sample):
        return self._probs.get(sample, 0)

    def samples(self):
        return self._samples

    def __repr__(self):
        return '<RandomUniformProbDist with %d samples>' %len(self._probs)


--
    specified by a given dictionary.  The given dictionary maps
    samples to probabilities.
    """
    def __init__(self, prob_dict=None, log=False, normalize=False):
        """
        Construct a new probability distribution from the given
        dictionary, which maps values to probabilities (or to log
--
                    for (x, p) in self._prob_dict.items():
                        self._prob_dict[x] *= norm_factor

    def prob(self, sample):
        if self._log:
            return (2**(self._prob_dict[sample]) if sample in self._prob_dict else 0)
        else:
            return self._prob_dict.get(sample, 0)

    def logprob(self, sample):
        if self._log:
            return self._prob_dict.get(sample, _NINF)
        else:
--
            elif self._prob_dict[sample] == 0: return _NINF
            else: return math.log(self._prob_dict[sample], 2)

    def max(self):
        if not hasattr(self, '_max'):
            self._max = max((p,v) for (v,p) in self._prob_dict.items())[1]
        return self._max
    def samples(self):
        return self._prob_dict.keys()
    def __repr__(self):
        return '<ProbDist with %d samples>' % len(self._prob_dict)


--
    each sample as the frequency of that sample in the frequency
    distribution.
    """
    def __init__(self, freqdist, bins=None):
        """
        Use the maximum likelihood estimate to create a probability
        distribution for the experiment used to generate ``freqdist``.
--
        """
        self._freqdist = freqdist

    def freqdist(self):
        """
        Return the frequency distribution that this probability
        distribution is based on.
--
        """
        return self._freqdist

    def prob(self, sample):
        return self._freqdist.freq(sample)

    def max(self):
        return self._freqdist.max()

    def samples(self):
        return self._freqdist.keys()

    def __repr__(self):
        """
        :rtype: str
        :return: A string representation of this ``ProbDist``.
--
    likelihood estimate of the resulting frequency distribution.
    """
    SUM_TO_ONE = False
    def __init__(self, freqdist, gamma, bins=None):
        """
        Use the Lidstone estimate to create a probability distribution
        for the experiment used to generate ``freqdist``.
--
            by the experiment that is described by the probability
            distribution.  This value must be correctly set for the
            probabilities of the sample values to sum to one.  If
            ``bins`` is not specified, it defaults to ``freqdist.B()``.
        """
        if (bins == 0) or (bins is None and freqdist.N() == 0):
            name = self.__class__.__name__[:-8]
--
            self._gamma = 0
            self._divisor = 1

    def freqdist(self):
        """
        Return the frequency distribution that this probability
        distribution is based on.
--
        """
        return self._freqdist

    def prob(self, sample):
        c = self._freqdist[sample]
        return (c + self._gamma) / self._divisor

    def max(self):
        # For Lidstone distributions, probability is monotonic with
        # frequency, so the most probable sample is the one that
        # occurs most frequently.
        return self._freqdist.max()

    def samples(self):
        return self._freqdist.keys()

    def discount(self):
        gb = self._gamma * self._bins
        return gb / (self._N + gb)

    def __repr__(self):
        """
        Return a string representation of this ``ProbDist``.

--
    each bin, and taking the maximum likelihood estimate of the
    resulting frequency distribution.
    """
    def __init__(self, freqdist, bins=None):
        """
        Use the Laplace estimate to create a probability distribution
        for the experiment used to generate ``freqdist``.
--
            by the experiment that is described by the probability
            distribution.  This value must be correctly set for the
            probabilities of the sample values to sum to one.  If
            ``bins`` is not specified, it defaults to ``freqdist.B()``.
        """
        LidstoneProbDist.__init__(self, freqdist, 1, bins)

    def __repr__(self):
        """
        :rtype: str
        :return: A string representation of this ``ProbDist``.
--
    to the count for each bin, and taking the maximum likelihood
    estimate of the resulting frequency distribution.
    """
    def __init__(self, freqdist, bins=None):
        """
        Use the expected likelihood estimate to create a probability
        distribution for the experiment used to generate ``freqdist``.
--
            by the experiment that is described by the probability
            distribution.  This value must be correctly set for the
            probabilities of the sample values to sum to one.  If
            ``bins`` is not specified, it defaults to ``freqdist.B()``.
        """
        LidstoneProbDist.__init__(self, freqdist, 0.5, bins)

    def __repr__(self):
        """
        Return a string representation of this ``ProbDist``.

--
        large ``_estimate`` must be.
    """
    SUM_TO_ONE = False
    def __init__(self, base_fdist, heldout_fdist, bins=None):
        """
        Use the heldout estimate to create a probability distribution
        for the experiment used to generate ``base_fdist`` and
--
            by the experiment that is described by the probability
            distribution.  This value must be correctly set for the
            probabilities of the sample values to sum to one.  If
            ``bins`` is not specified, it defaults to ``freqdist.B()``.
        """

        self._base_fdist = base_fdist
--
        # each value of r.
        self._estimate = self._calculate_estimate(Tr, Nr, N)

    def _calculate_Tr(self):
        """
        Return the list *Tr*, where *Tr[r]* is the total count in
        ``heldout_fdist`` for all samples that occur *r*
--
            Tr[r] += self._heldout_fdist[sample]
        return Tr

    def _calculate_estimate(self, Tr, Nr, N):
        """
        Return the list *estimate*, where *estimate[r]* is the probability
        estimate for any sample that occurs *r* times in the base frequency
        distribution.  In particular, *estimate[r]* is *Tr[r]/(N[r].N)*.
        In the special case that *N[r]=0*, *estimate[r]* will never be used;
        so we define *estimate[r]=None* for those cases.

        :rtype: list(float)
        :type Tr: list(float)
--
            else: estimate.append(Tr[r]/(Nr[r]*N))
        return estimate

    def base_fdist(self):
        """
        Return the base frequency distribution that this probability
        distribution is based on.
--
        """
        return self._base_fdist

    def heldout_fdist(self):
        """
        Return the heldout frequency distribution that this
        probability distribution is based on.
--
        """
        return self._heldout_fdist

    def samples(self):
        return self._base_fdist.keys()

    def prob(self, sample):
        # Use our precomputed probability estimate.
        r = self._base_fdist[sample]
        return self._estimate[r]

    def max(self):
        # Note: the Heldout estimation is *not* necessarily monotonic;
        # so this implementation is currently broken.  However, it
        # should give the right answer *most* of the time. :)
        return self._base_fdist.max()

    def discount(self):
        raise NotImplementedError()

    def __repr__(self):
        """
        :rtype: str
        :return: A string representation of this ``ProbDist``.
--
    each pair of frequency distributions.
    """
    SUM_TO_ONE = False
    def __init__(self, freqdists, bins):
        """
        Use the cross-validation estimate to create a probability
        distribution for the experiment used to generate
--
            by the experiment that is described by the probability
            distribution.  This value must be correctly set for the
            probabilities of the sample values to sum to one.  If
            ``bins`` is not specified, it defaults to ``freqdist.B()``.
        """
        self._freqdists = freqdists

--
                    probdist = HeldoutProbDist(fdist1, fdist2, bins)
                    self._heldout_probdists.append(probdist)

    def freqdists(self):
        """
        Return the list of frequency distributions that this ``ProbDist`` is based on.

--
        """
        return self._freqdists

    def samples(self):
        # [xx] nb: this is not too efficient
        return set(sum([fd.keys() for fd in self._freqdists], []))

    def prob(self, sample):
        # Find the average probability estimate returned by each
        # heldout distribution.
        prob = 0.0
--
            prob += heldout_probdist.prob(sample)
        return prob/len(self._heldout_probdists)

    def discount(self):
        raise NotImplementedError()

    def __repr__(self):
        """
        Return a string representation of this ``ProbDist``.

--
        - *p = c / (N + T)*, otherwise
    """

    def __init__(self, freqdist, bins=None):
        """
        Creates a distribution of Witten-Bell probability estimates.  This
        distribution allocates uniform probability mass to as yet unseen
--
        else:
            self._P0 = self._T / float(self._Z * (self._N + self._T))

    def prob(self, sample):
        # inherit docs from ProbDistI
        c = self._freqdist[sample]
        return (c / float(self._N + self._T) if c != 0 else self._P0)

    def max(self):
        return self._freqdist.max()

    def samples(self):
        return self._freqdist.keys()

    def freqdist(self):
        return self._freqdist

    def discount(self):
        raise NotImplementedError()

    def __repr__(self):
        """
        Return a string representation of this ``ProbDist``.

--
    - slope: b = sigma ((xi-E(x)(yi-E(y))) / sigma ((xi-E(x))(xi-E(x)))
    - intercept: a = E(y) - b.E(x)
    """
    def __init__(self, freqdist, bins=None):
        """
        :param freqdist: The frequency counts upon which to base the
            estimation.
--
        self._switch(r, nr)
        self._renormalize(r, nr)

    def _r_Nr(self):
        """
        Split the frequency distribution in two list (r, Nr), where Nr(r) > 0
        """
--
            return [], []
        return zip(*nonzero)

    def find_best_fit(self, r, nr):
        """
        Use simple linear regression to tune parameters self._slope and
        self._intercept in the log-log space based on count and Nr(count)
--
        self._slope = (xy_cov / x_var if x_var != 0 else 0.0)
        self._intercept = y_mean - self._slope * x_mean

    def _switch(self, r, nr):
        """
        Calculate the r frontier where we must switch from Nr to Sr
        when estimating E[Nr].
--
                self._switch_at = r_
                break

    def _variance(self, r, nr, nr_1):
        r = float(r)
        nr = float(nr)
        nr_1 = float(nr_1)
        return (r + 1.0)**2 * (nr_1 / nr**2) * (1.0 + nr_1 / nr)

    def _renormalize(self, r, nr):
        """
        It is necessary to renormalize all the probability estimates to
        ensure a proper probability distribution results. This can be done
--
        if prob_cov:
            self._renormal = (1 - self._prob_measure(0)) / prob_cov

    def smoothedNr(self, r):
        """
        Return the number of samples with count r.

--

        return math.exp(self._intercept + self._slope * math.log(r))

    def prob(self, sample):
        """
        Return the sample's probability.

--
            p = p * self._renormal
        return p

    def _prob_measure(self, count):
        if count == 0 and self._freqdist.N() == 0 :
            return 1.0
        elif count == 0 and self._freqdist.N() != 0:
--
        r_star = (count + 1) * Er_1 / Er
        return r_star / self._freqdist.N()

    def check(self):
        prob_sum = 0.0
        for i in  range(0, len(self._Nr)):
            prob_sum += self._Nr[i] * self._prob_measure(i) / self._renormal
        print("Probability Sum:", prob_sum)
        #assert prob_sum != 1.0, "probability sum should be one!"

    def discount(self):
        """
        This function returns the total mass of probability transfers from the
        seen samples to the unseen samples.
        """
        return  1.0 * self.smoothedNr(1) / self._freqdist.N()

    def max(self):
        return self._freqdist.max()

    def samples(self):
        return self._freqdist.keys()

    def freqdist(self):
        return self._freqdist

    def __repr__(self):
        """
        Return a string representation of this ``ProbDist``.

--
    mutable dictionary and providing an update method.
    """

    def __init__(self, prob_dist, samples, store_logs=True):
        """
        Creates the mutable probdist based on the given prob_dist and using
        the list of samples given. These values are stored as log
--
                self._data[i] = prob_dist.prob(samples[i])
        self._logs = store_logs

    def samples(self):
        # inherit documentation
        return self._samples

    def prob(self, sample):
        # inherit documentation
        i = self._sample_dict.get(sample)
        if i is None:
            return 0.0
        return (2**(self._data[i]) if self._logs else self._data[i])

    def logprob(self, sample):
        # inherit documentation
        i = self._sample_dict.get(sample)
        if i is None:
            return float('-inf')
        return (self._data[i] if self._logs else math.log(self._data[i], 2))

    def update(self, sample, prob, log=True):
        """
        Update the probability for the given sample. This may cause the object
        to stop being the valid probability distribution - the user must
--
    Kneser-Ney estimate of a probability distribution. This is a version of
    back-off that counts how likely an n-gram is provided the n-1-gram had
    been seen in training. Extends the ProbDistI interface, requires a trigram
    FreqDist instance to train on. Optionally, a different from default discount
    value can be specified. The default discount is set to 0.75.

    """
    def __init__(self, freqdist, bins=None, discount=0.75):
        """
        :param trigrams: The trigram frequency distribution upon which to base
            the estimation
--
        self._cache = {}

        # internal bigram and trigram frequency distributions
        self._bigrams = defaultdict(int)
        self._trigrams = freqdist

        # helper dictionaries used to calculate probabilities
        self._wordtypes_after = defaultdict(float)
        self._trigrams_contain = defaultdict(float)
        self._wordtypes_before = defaultdict(float)
        for w0, w1, w2 in freqdist:
            self._bigrams[(w0,w1)] += freqdist[(w0, w1, w2)]
            self._wordtypes_after[(w0,w1)] += 1
            self._trigrams_contain[w1] += 1
            self._wordtypes_before[(w1,w2)] += 1

    def prob(self, trigram):
        # sample must be a triple
        if len(trigram) != 3:
            raise ValueError('Expected an iterable with 3 members.')
--
            self._cache[trigram] = prob
            return prob

    def discount(self):
        """
        Return the value by which counts are discounted. By default set to 0.75.

        :rtype: float
        """
        return self._D

    def set_discount(self, discount):
        """
        Set the value by which counts are discounted to the value of discount.

--
        """
        self._D = discount

    def samples(self):
        return self._trigrams.keys()

    def max(self):
        return self._trigrams.max()

    def __repr__(self):
        '''
        Return a string representation of this ProbDist

--
##  Probability Distribution Operations
##//////////////////////////////////////////////////////

def log_likelihood(test_pdist, actual_pdist):
    if (not isinstance(test_pdist, ProbDistI) or
        not isinstance(actual_pdist, ProbDistI)):
        raise ValueError('expected a ProbDist.')
--
    return sum(actual_pdist.prob(s) * math.log(test_pdist.prob(s), 2)
               for s in actual_pdist)

def entropy(pdist):
    probs = (pdist.prob(s) for s in pdist.samples())
    return -sum(p * math.log(p,2) for p in probs)

--
##//////////////////////////////////////////////////////

@compat.python_2_unicode_compatible
class ConditionalFreqDist(defaultdict):
    """
    A collection of frequency distributions for a single experiment
    run under different conditions.  Conditional frequency
--
    For example, a conditional frequency distribution could be used to
    record the frequency of each word (type) in a document, given its
    length.  Formally, a conditional frequency distribution can be
    defined as a function that maps from each condition to the
    FreqDist for the experiment under that condition.

    Conditional frequency distributions are typically constructed by
--
    condition.

    """
    def __init__(self, cond_samples=None):
        """
        Construct a new empty conditional frequency distribution.  In
        particular, the count for every sample, under every condition,
--
            frequency distribution with
        :type cond_samples: Sequence of (condition, sample) tuples
        """
        defaultdict.__init__(self, FreqDist)
        if cond_samples:
            for (cond, sample) in cond_samples:
                self[cond].inc(sample)

    def __reduce__(self):
        kv_pairs = ((cond, self[cond]) for cond in self.conditions())
        return (self.__class__, (), None, None, kv_pairs)

    def conditions(self):
        """
        Return a list of the conditions that have been accessed for
        this ``ConditionalFreqDist``.  Use the indexing operator to
--
        """
        return sorted(self.keys())

    def N(self):
        """
        Return the total number of sample outcomes that have been
        recorded by this ``ConditionalFreqDist``.
--
        """
        return sum(fdist.N() for fdist in compat.itervalues(self))

    def plot(self, *args, **kwargs):
        """
        Plot the given samples from the conditional frequency distribution.
        For a cumulative plot, specify cumulative=True.
--
        :type samples: list
        :param title: The title for the graph
        :type title: str
        :param conditions: The conditions to plot (default is all)
        :type conditions: list
        """
        try:
--
        pylab.ylabel(ylabel)
        pylab.show()

    def tabulate(self, *args, **kwargs):
        """
        Tabulate the given samples from the conditional frequency distribution.

--
        :type samples: list
        :param title: The title for the graph
        :type title: str
        :param conditions: The conditions to plot (default is all)
        :type conditions: list
        """

--
            print()

    # @total_ordering doesn't work here, since the class inherits from a builtin class
    def __le__(self, other):
        if not isinstance(other, ConditionalFreqDist):
            raise_unorderable_types("<=", self, other)
        return set(self.conditions()).issubset(other.conditions()) \
               and all(self[c] <= other[c] for c in self.conditions())
    def __lt__(self, other):
        if not isinstance(other, ConditionalFreqDist):
            raise_unorderable_types("<", self, other)
        return self <= other and self != other
    def __ge__(self, other):
        if not isinstance(other, ConditionalFreqDist):
            raise_unorderable_types(">=", self, other)
        return other <= self
    def __gt__(self, other):
        if not isinstance(other, ConditionalFreqDist):
            raise_unorderable_types(">", self, other)
        return other < self

    def __repr__(self):
        """
        Return a string representation of this ``ConditionalFreqDist``.

--
    example, a conditional probability distribution could be used to
    estimate the probability of each word type in a document, given
    the length of the word type.  Formally, a conditional probability
    distribution can be defined as a function that maps from each
    condition to the ``ProbDist`` for the experiment under that
    condition.
    """
    def __init__(self):
        raise NotImplementedError("Interfaces can't be instantiated")

    def conditions(self):
        """
        Return a list of the conditions that are represented by
        this ``ConditionalProbDist``.  Use the indexing operator to
--
        """
        return self.keys()

    def __repr__(self):
        """
        Return a string representation of this ``ConditionalProbDist``.

--
        0.423...

    """
    def __init__(self, cfdist, probdist_factory,
                 *factory_args, **factory_kw_args):
        """
        Construct a new conditional probability distribution, based on
--
            self[condition] = probdist_factory(cfdist[condition],
                                               *factory_args, **factory_kw_args)

    def __missing__(self, key):
        self[key] = self._probdist_factory(FreqDist(),
                                           *self._factory_args,
                                           **self._factory_kw_args)
--
    ProbDists rather than creating these from FreqDists.
    """

    def __init__(self, probdist_dict):
        """
        :param probdist_dict: a dictionary containing the probdists indexed
            by the conditions
--
        """
        self.update(probdist_dict)

    def __missing__(self, key):
        self[key] = DictionaryProbDist()
        return self[key]

--
# If the difference is bigger than this, then just take the bigger one:
_ADD_LOGS_MAX_DIFF = math.log(1e-30, 2)

def add_logs(logx, logy):
    """
    Given two numbers ``logx`` = *log(x)* and ``logy`` = *log(y)*, return
    *log(x+y)*.  Conceptually, this is the same as returning
--
    base = min(logx, logy)
    return base + math.log(2**(logx-base) + 2**(logy-base), 2)

def sum_logs(logs):
    return (reduce(add_logs, logs[1:], logs[0]) if len(logs) != 0 else _NINF)

##//////////////////////////////////////////////////////
--
    """
    A mix-in class to associate probabilities with other classes
    (trees, rules, etc.).  To use the ``ProbabilisticMixIn`` class,
    define a new class that derives from an existing class and from
    ProbabilisticMixIn.  You will need to define a new constructor for
    the new class, which explicitly calls the constructors of both its
    parent classes.  For example:

        >>> from nltk.probability import ProbabilisticMixIn
        >>> class A:
        ...     def __init__(self, x, y): self.data = (x,y)
        ...
        >>> class ProbabilisticA(A, ProbabilisticMixIn):
        ...     def __init__(self, x, y, **prob_kwarg):
        ...         A.__init__(self, x, y)
        ...         ProbabilisticMixIn.__init__(self, **prob_kwarg)

--
    ``constructor<__init__>`` for information about the arguments it
    expects.

    You should generally also redefine the string representation
    methods, the comparison methods, and the hashing method.
    """
    def __init__(self, **kwargs):
        """
        Initialize this object's probability.  This initializer should
        be called by subclass constructors.  ``prob`` should generally be
--
        else:
            self.__prob = self.__logprob = None

    def set_prob(self, prob):
        """
        Set the probability associated with this object to ``prob``.

--
        self.__prob = prob
        self.__logprob = None

    def set_logprob(self, logprob):
        """
        Set the log probability associated with this object to
        ``logprob``.  I.e., set the probability associated with this
--
        self.__logprob = logprob
        self.__prob = None

    def prob(self):
        """
        Return the probability associated with this object.

--
            self.__prob = 2**(self.__logprob)
        return self.__prob

    def logprob(self):
        """
        Return ``log(p)``, where ``p`` is the probability associated
        with this object.
--
        return self.__logprob

class ImmutableProbabilisticMixIn(ProbabilisticMixIn):
    def set_prob(self, prob):
        raise ValueError('%s is immutable' % self.__class__.__name__)
    def set_logprob(self, prob):
        raise ValueError('%s is immutable' % self.__class__.__name__)

## Helper function for processing keyword arguments

def _get_kwarg(kwargs, key, default):
    if key in kwargs:
        arg = kwargs[key]
        del kwargs[key]
    else:
        arg = default
    return arg

##//////////////////////////////////////////////////////
##  Demonstration
##//////////////////////////////////////////////////////

def _create_rand_fdist(numsamples, numoutcomes):
    """
    Create a new frequency distribution, with random samples.  The
    samples are numbers from 1 to ``numsamples``, and are generated by
--
        fdist.inc(y)
    return fdist

def _create_sum_pdist(numsamples):
    """
    Return the true probability distribution for the experiment
    ``_create_rand_fdist(numsamples, x)``.
--
            fdist.inc(x+y)
    return MLEProbDist(fdist)

def demo(numsamples=6, numoutcomes=500):
    """
    A demonstration of frequency distributions and probability
    distributions.  This demonstration creates three frequency
--
        print('%20s %s' % (pdist.__class__.__name__[:20], ("%s" % fdist)[:55]))
    print()

def gt_demo():
    from nltk import corpus
    emma_words = corpus.gutenberg.words('austen-emma.txt')
    fd = FreqDist(emma_words)
--
    identical interface.
    """

    def __init__(self, word_fd, ngram_fd):
        self.word_fd = word_fd
        self.ngram_fd = ngram_fd

    @classmethod
    def from_documents(cls, documents):
        """Constructs a collocation finder given a collection of documents,
        each of which is a list (or iterable) of tokens.
        """
        return cls.from_words(_itertools.chain(*documents))

    @staticmethod
    def _ngram_freqdist(words, n):
        return FreqDist(tuple(words[i:i+n]) for i in range(len(words)-1))

    def _apply_filter(self, fn=lambda ngram, freq: False):
        """Generic filter removes ngrams from the frequency distribution
        if the function returns True when passed an ngram tuple.
        """
--
                except KeyError:
                    pass

    def apply_freq_filter(self, min_freq):
        """Removes candidate ngrams which have frequency less than min_freq."""
        self._apply_filter(lambda ng, freq: freq < min_freq)

    def apply_ngram_filter(self, fn):
        """Removes candidate ngrams (w1, w2, ...) where fn(w1, w2, ...)
        evaluates to True.
        """
        self._apply_filter(lambda ng, f: fn(*ng))

    def apply_word_filter(self, fn):
        """Removes candidate ngrams (w1, w2, ...) where any of (fn(w1), fn(w2),
        ...) evaluates to True.
        """
        self._apply_filter(lambda ng, f: any(fn(w) for w in ng))

    def _score_ngrams(self, score_fn):
        """Generates of (ngram, score) pairs as determined by the scoring
        function provided.
        """
--
            if score is not None:
                yield tup, score

    def score_ngrams(self, score_fn):
        """Returns a sequence of (ngram, score) pairs ordered from highest to
        lowest score, as determined by the scoring function provided.
        """
        return sorted(self._score_ngrams(score_fn),
                      key=_itemgetter(1), reverse=True)

    def nbest(self, score_fn, n):
        """Returns the top n ngrams when scored by the given function."""
        return [p for p,s in self.score_ngrams(score_fn)[:n]]

    def above_score(self, score_fn, min_score):
        """Returns a sequence of ngrams, ordered by decreasing score, whose
        scores each exceed the given minimum score.
        """
--
    constructing an instance directly.
    """

    def __init__(self, word_fd, bigram_fd, window_size=2):
        """Construct a BigramCollocationFinder, given FreqDists for
        appearances of words and (possibly non-contiguous) bigrams.
        """
--
        self.window_size = window_size

    @classmethod
    def from_words(cls, words, window_size=2):
        """Construct a BigramCollocationFinder for all bigrams in the given
        sequence.  When window_size > 2, count non-contiguous bigrams, in the
        style of Church and Hanks's (1990) association ratio.
--
                    bfd.inc((w1, w2))
        return cls(wfd, bfd, window_size=window_size)

    def score_ngram(self, score_fn, w1, w2):
        """Returns the score for a given bigram using the given scoring
        function.  Following Church and Hanks (1990), counts are scaled by 
        a factor of 1/(window_size - 1).
--
    constructing an instance directly.
    """

    def __init__(self, word_fd, bigram_fd, wildcard_fd, trigram_fd):
        """Construct a TrigramCollocationFinder, given FreqDists for
        appearances of words, bigrams, two words with any word between them,
        and trigrams.
--
        self.bigram_fd = bigram_fd

    @classmethod
    def from_words(cls, words):
        """Construct a TrigramCollocationFinder for all trigrams in the given
        sequence.
        """
--
            tfd.inc((w1, w2, w3))
        return cls(wfd, bfd, wildfd, tfd)

    def bigram_finder(self):
        """Constructs a bigram collocation finder with the bigram and unigram
        data from this finder. Note that this does not include any filtering
        applied to this finder.
        """
        return BigramCollocationFinder(self.word_fd, self.bigram_fd)

    def score_ngram(self, score_fn, w1, w2, w3):
        """Returns the score for a given trigram using the given scoring
        function.
        """
--
                        n_all)


def demo(scorer=None, compare_scorer=None):
    """Finds bigram collocations in the files of the WebText corpus."""
    from nltk.metrics import BigramAssocMeasures, spearman_correlation, ranks_from_scores

--
    """
    _interpformat_bin = None

    def __init__(self, goal=None, assumptions=None, max_models=500, model_builder=None):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--
        BaseModelBuilderCommand.__init__(self, model_builder, goal, assumptions)

    @property
    def valuation(mbc): return mbc.model('valuation')

    def _convert2val(self, valuation_str):
        """
        Transform the output file into an NLTK-style Valuation.

--
        return Valuation(val)

    @staticmethod
    def _make_relation_set(num_entities, values):
        """
        Convert a Mace4-style relation table into a dictionary.

--
        return r

    @staticmethod
    def _make_relation_tuple(position, values, num_entities):
        if len(values) == 1:
            return []
        else:
--
                                                    num_entities)

    @staticmethod
    def _make_model_var(value):
        """
        Pick an alphabetic character as identifier for an entity in the model.

--
        num = int(value) / 26
        return (letter + str(num) if num > 0 else letter)

    def _decorate_model(self, valuation_str, format):
        """
        Print out a Mace4 model using any Mace4 ``interpformat`` format.
        See http://www.cs.unm.edu/~mccune/mace4/manual/ for details.

        :param valuation_str: str with the model builder's output
        :param format: str indicating the format for displaying
        models. Defaults to 'standard' format.
        :return: str
        """
        if not format:
--
        else:
            return self._transform_output(valuation_str, format)

    def _transform_output(self, valuation_str, format):
        """
        Transform the output file into any Mace4 ``interpformat`` format.

--
        else:
            raise LookupError("The specified format does not exist")

    def _call_interpformat(self, input_str, args=[], verbose=False):
        """
        Call the ``interpformat`` binary with the given input.

--
class Mace(Prover9Parent, ModelBuilder):
    _mace4_bin = None

    def __init__(self, end_size=500):
        self._end_size = end_size
        """The maximum model size that Mace will try before
           simply returning false. (Use -1 for no maximum.)"""

    def _build_model(self, goal=None, assumptions=None, verbose=False):
        """
        Use Mace4 to build a first order model.

--
                                              verbose=verbose)
        return (returncode == 0, stdout)

    def _call_mace4(self, input_str, args=[], verbose=False):
        """
        Call the ``mace4`` binary with the given input.

--
        return self._call(updated_input_str, self._mace4_bin, args, verbose)


def spacer(num=30):
    print('-' * num)

def decode_result(found):
    """
    Decode the result of model_found()

--
    """
    return {True: 'Countermodel found', False: 'No countermodel found', None: 'None'}[found]

def test_model_found(arguments):
    """
    Try some proofs and exhibit the results.
    """
--
        print('|- %s: %s\n' % (g, decode_result(found)))


def test_build_model(arguments):
    """
    Try to build a ``nltk.sem.Valuation``.
    """
--
    spacer()
    print(m.valuation, '\n')

def test_transform_output(argument_pair):
    """
    Transform the model into various Mace4 ``interpformat`` formats.
    """
--
        spacer()
        print(m.model(format=format))

def test_make_relation_set():
    print(MaceCommand._make_relation_set(num_entities=3, values=[1,0,1]) == set([('c',), ('a',)]))
    print(MaceCommand._make_relation_set(num_entities=3, values=[0,0,0,0,0,0,1,0,0]) == set([('c', 'a')]))
    print(MaceCommand._make_relation_set(num_entities=2, values=[0,0,1,0,0,0,1,0]) == set([('a', 'b', 'a'), ('b', 'b', 'a')]))
--
    ('(not mortal(Socrates))', ['all x.(man(x) -> mortal(x))', 'man(Socrates)'])
]

def demo():
    test_model_found(arguments)
    test_build_model(arguments)
    test_transform_output(arguments[1])
--
class TableauProver(Prover):
    _assume_false=False

    def _prove(self, goal=None, assumptions=None, verbose=False):
        if not assumptions:
            assumptions = []

--
                    raise e
        return (result, '\n'.join(debugger.lines))

    def _attempt_proof(self, agenda, accessible_vars, atoms, debug):
        (current, context), category = agenda.pop_first()

        #if there's nothing left in the agenda, and we haven't closed the path
--
        debug.line((current, context))
        return proof_method(current, context, agenda, accessible_vars, atoms, debug)

    def _attempt_proof_atom(self, current, context, agenda, accessible_vars, atoms, debug):
        # Check if the branch is closed.  Return 'True' if it is
        if (current, True) in atoms:
            debug.line('CLOSED', 1)
--
            agenda.mark_alls_fresh();
            return self._attempt_proof(agenda, accessible_vars|set(current.args), atoms|set([(current, False)]), debug+1)

    def _attempt_proof_n_atom(self, current, context, agenda, accessible_vars, atoms, debug):
        # Check if the branch is closed.  Return 'True' if it is
        if (current.term, False) in atoms:
            debug.line('CLOSED', 1)
--
            agenda.mark_alls_fresh();
            return self._attempt_proof(agenda, accessible_vars|set(current.term.args), atoms|set([(current.term, True)]), debug+1)

    def _attempt_proof_prop(self, current, context, agenda, accessible_vars, atoms, debug):
        # Check if the branch is closed.  Return 'True' if it is
        if (current, True) in atoms:
            debug.line('CLOSED', 1)
--
        agenda.mark_alls_fresh();
        return self._attempt_proof(agenda, accessible_vars, atoms|set([(current, False)]), debug+1)

    def _attempt_proof_n_prop(self, current, context, agenda, accessible_vars, atoms, debug):
        # Check if the branch is closed.  Return 'True' if it is
        if (current.term, False) in atoms:
            debug.line('CLOSED', 1)
--
        agenda.mark_alls_fresh();
        return self._attempt_proof(agenda, accessible_vars, atoms|set([(current.term, True)]), debug+1)

    def _attempt_proof_app(self, current, context, agenda, accessible_vars, atoms, debug):
        f, args = current.uncurry()
        for i, arg in enumerate(args):
            if not TableauProver.is_atom(arg):
--
                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)
        raise Exception('If this method is called, there must be a non-atomic argument')

    def _attempt_proof_n_app(self, current, context, agenda, accessible_vars, atoms, debug):
        f, args = current.term.uncurry()
        for i, arg in enumerate(args):
            if not TableauProver.is_atom(arg):
--
                return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)
        raise Exception('If this method is called, there must be a non-atomic argument')

    def _attempt_proof_n_eq(self, current, context, agenda, accessible_vars, atoms, debug):
        ###########################################################################
        # Since 'current' is of type '~(a=b)', the path is closed if 'a' == 'b'
        ###########################################################################
--
        current._exhausted = True
        return self._attempt_proof(agenda, accessible_vars|set([current.term.first, current.term.second]), atoms, debug+1)

    def _attempt_proof_d_neg(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda.put(current.term.term, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_all(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda[Categories.EXISTS].add((ExistsExpression(current.term.variable, -current.term.term), context))
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_some(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda[Categories.ALL].add((AllExpression(current.term.variable, -current.term.term), context))
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_and(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda.put(current.first, context)
        agenda.put(current.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_or(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda.put(-current.term.first, context)
        agenda.put(-current.term.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_imp(self, current, context, agenda, accessible_vars, atoms, debug):
        agenda.put(current.term.first, context)
        agenda.put(-current.term.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_or(self, current, context, agenda, accessible_vars, atoms, debug):
        new_agenda = agenda.clone()
        agenda.put(current.first, context)
        new_agenda.put(current.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \
                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_imp(self, current, context, agenda, accessible_vars, atoms, debug):
        new_agenda = agenda.clone()
        agenda.put(-current.first, context)
        new_agenda.put(current.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \
                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_and(self, current, context, agenda, accessible_vars, atoms, debug):
        new_agenda = agenda.clone()
        agenda.put(-current.term.first, context)
        new_agenda.put(-current.term.second, context)
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \
                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_iff(self, current, context, agenda, accessible_vars, atoms, debug):
        new_agenda = agenda.clone()
        agenda.put(current.first, context)
        agenda.put(current.second, context)
--
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \
                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_n_iff(self, current, context, agenda, accessible_vars, atoms, debug):
        new_agenda = agenda.clone()
        agenda.put(current.term.first, context)
        agenda.put(-current.term.second, context)
--
        return self._attempt_proof(agenda, accessible_vars, atoms, debug+1) and \
                self._attempt_proof(new_agenda, accessible_vars, atoms, debug+1)

    def _attempt_proof_eq(self, current, context, agenda, accessible_vars, atoms, debug):
        #########################################################################
        # Since 'current' is of the form '(a = b)', replace ALL free instances
        # of 'a' with 'b'
--
        agenda.mark_neqs_fresh();
        return self._attempt_proof(agenda, accessible_vars, set(), debug+1)

    def _attempt_proof_some(self, current, context, agenda, accessible_vars, atoms, debug):
        new_unique_variable = VariableExpression(unique_variable())
        agenda.put(current.term.replace(current.variable, new_unique_variable), context)
        agenda.mark_alls_fresh()
        return self._attempt_proof(agenda, accessible_vars|set([new_unique_variable]), atoms, debug+1)

    def _attempt_proof_all(self, current, context, agenda, accessible_vars, atoms, debug):
        try:
            current._used_vars
        except AttributeError:
--
            return self._attempt_proof(agenda, accessible_vars|set([new_unique_variable]), atoms, debug+1)

    @staticmethod
    def is_atom(e):
        if isinstance(e, NegatedExpression):
            e = e.term

--


class TableauProverCommand(BaseProverCommand):
    def __init__(self, goal=None, assumptions=None, prover=None):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--


class Agenda(object):
    def __init__(self):
        self.sets = tuple(set() for i in range(21))

    def clone(self):
        new_agenda = Agenda()
        set_list = [s.copy() for s in self.sets]

--
        new_agenda.sets = tuple(set_list)
        return new_agenda

    def __getitem__(self, index):
        return self.sets[index]

    def put(self, expression, context=None):
        if isinstance(expression, AllExpression):
            ex_to_add = AllExpression(expression.variable, expression.term)
            try:
--
            ex_to_add = expression
        self.sets[self._categorize_expression(ex_to_add)].add((ex_to_add, context))

    def put_all(self, expressions):
        for expression in expressions:
            self.put(expression)

    def put_atoms(self, atoms):
        for atom, neg in atoms:
            if neg:
                self[Categories.N_ATOM].add((-atom,None))
            else:
                self[Categories.ATOM].add((atom,None))

    def pop_first(self):
        """ Pop the first expression that appears in the agenda """
        for i,s in enumerate(self.sets):
            if s:
--
                    return (s.pop(), i)
        return ((None, None), None)

    def replace_all(self, old, new):
        for s in self.sets:
            for ex,ctx in s:
                ex.replace(old.variable, new)
                if ctx is not None:
                    ctx.replace(old.variable, new)

    def mark_alls_fresh(self):
        for u,_ in self.sets[Categories.ALL]:
            u._exhausted = False

    def mark_neqs_fresh(self):
        for neq,_ in self.sets[Categories.N_EQ]:
            neq._exhausted = False

    def _categorize_expression(self, current):
        if isinstance(current, NegatedExpression):
            return self._categorize_NegatedExpression(current)
        elif isinstance(current, FunctionVariableExpression):
--
            raise ProverParseError("cannot categorize %s" % \
                                   current.__class__.__name__)

    def _categorize_NegatedExpression(self, current):
        negated = current.term

        if isinstance(negated, NegatedExpression):
--


class Debug(object):
    def __init__(self, verbose, indent=0, lines=None):
        self.verbose = verbose
        self.indent = indent

        if not lines: lines = []
        self.lines = lines

    def __add__(self, increment):
        return Debug(self.verbose, self.indent+1, self.lines)

    def line(self, data, indent=0):
        if isinstance(data, tuple):
            ex, ctx = data
            if ctx:
--
    ALL      = 20


def testTableauProver():
    tableau_test('P | -P')
    tableau_test('P & -P')
    tableau_test('Q', ['P', '(P -> Q)'])
--
#    tableau_test(c, [p])


def testHigherOrderTableauProver():
    tableau_test('believe(j, -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])
    tableau_test('believe(j, lie(b) & cheat(b))', ['believe(j, lie(b))'])
    tableau_test('believe(j, lie(b))', ['lie(b)']) #how do we capture that John believes all things that are true
--
    tableau_test('believe(j, -cheat(b) & -lie(b))', ['believe(j, -lie(b) & -cheat(b))'])


def tableau_test(c, ps=None, verbose=False):
    lp = LogicParser()
    pc = lp.parse(c)
    pps = ([lp.parse(p) for p in ps] if ps else [])
--
        ps = []
    print('%s |- %s: %s' % (', '.join(ps), pc, TableauProver().prove(pc, pps, verbose=verbose)))

def demo():
    testTableauProver()
    testHigherOrderTableauProver()

--
from __future__ import print_function, unicode_literals

import operator
from collections import defaultdict
from functools import reduce

from nltk.sem import skolemize
--
    ANSWER_KEY = 'ANSWER'
    _assume_false=True

    def _prove(self, goal=None, assumptions=None, verbose=False):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--
                    raise e
        return (result, clauses)

    def _attempt_proof(self, clauses):
        #map indices to lists of indices, to store attempted unifications
        tried = defaultdict(list)

        i = 0
        while i < len(clauses):
--
        return (False, clauses)

class ResolutionProverCommand(BaseProverCommand):
    def __init__(self, goal=None, assumptions=None, prover=None):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--
        BaseProverCommand.__init__(self, prover, goal, assumptions)
        self._clauses = None

    def prove(self, verbose=False):
        """
        Perform the actual proof.  Store the result to prevent unnecessary
        re-proving.
--
            self._proof = ResolutionProverCommand._decorate_clauses(clauses)
        return self._result

    def find_answers(self, verbose=False):
        self.prove(verbose)

        answers = set()
--
        return answers

    @staticmethod
    def _decorate_clauses(clauses):
        """
        Decorate the proof output.
        """
--

@python_2_unicode_compatible
class Clause(list):
    def __init__(self, data):
        list.__init__(self, data)
        self._is_tautology = None
        self._parents = None

    def unify(self, other, bindings=None, used=None, skipped=None, debug=False):
        """
        Attempt to unify this Clause with the other, returning a list of
        resulting, unified, Clauses.
--

        return result

    def isSubsetOf(self, other):
        """
        Return True iff every term in 'self' is a term in 'other'.

--
                return False
        return True

    def subsumes(self, other):
        """
        Return True iff 'self' subsumes 'other', this is, if there is a
        substitution such that every term in 'self' can be unified with a term
--
                                      skipped, _subsumes_finalize,
                                      debug)) > 0

    def __getslice__(self, start, end):
        return Clause(list.__getslice__(self, start, end))

    def __sub__(self, other):
        return Clause([a for a in self if a not in other])

    def __add__(self, other):
        return Clause(list.__add__(self, other))

    def is_tautology(self):
        """
        Self is a tautology if it contains ground terms P and -P.  The ground
        term, P, must be an exact match, ie, not using unification.
--
        self._is_tautology = False
        return False

    def free(self):
        return reduce(operator.or_, ((atom.free() | atom.constants()) for atom in self))

    def replace(self, variable, expression):
        """
        Replace every instance of variable with expression across every atom
        in the clause
--
        """
        return Clause([atom.replace(variable, expression) for atom in self])

    def substitute_bindings(self, bindings):
        """
        Replace every binding

--
        """
        return Clause([atom.substitute_bindings(bindings) for atom in self])

    def __str__(self):
        return '{' + ', '.join("%s" % item for item in self) + '}'

    def __repr__(self):
        return "%s" % self

def _iterate_first(first, second, bindings, used, skipped, finalize_method, debug):
    """
    This method facilitates movement through the terms of 'self'
    """
--

        return result

def _iterate_second(first, second, bindings, used, skipped, finalize_method, debug):
    """
    This method facilitates movement through the terms of 'other'
    """
--

        return result

def _unify_terms(a, b, bindings=None, used=None):
    """
    This method attempts to unify two terms.  Two expressions are unifiable
    if there exists a substitution function S such that S(a) == S(-b).
--

    return newbindings, newused, unused

def _complete_unify_path(first, second, bindings, used, skipped, debug):
    if used[0] or used[1]: #if bindings were made along the path
        newclause = Clause(skipped[0] + skipped[1] + first + second)
        debug.line('  -> New Clause: %s' % newclause)
--
        debug.line('  -> End')
        return []

def _subsumes_finalize(first, second, bindings, used, skipped, debug):
    if not len(skipped[0]) and not len(first):
        #If there are no skipped terms and no terms left in 'first', then
        #all of the terms in the original 'self' were unified with terms
        #in 'other'.  Therefore, there exists a binding (this one) such that
        #every term in self can be unified with a term in other, which
        #is the definition of subsumption.
        return [True]
    else:
        return []

def clausify(expression):
    """
    Skolemize, clausify, and standardize the variables apart.
    """
--
        clause_list.append(clause)
    return clause_list

def _clausify(expression):
    """
    :param expression: a skolemized expression in CNF
    """
--

@python_2_unicode_compatible
class BindingDict(object):
    def __init__(self, binding_list=None):
        """
        :param binding_list: list of (``AbstractVariableExpression``, ``AtomicExpression``) to initialize the dictionary
        """
--
            for (v, b) in binding_list:
                self[v] = b

    def __setitem__(self, variable, binding):
        """
        A binding is consistent with the dict if its variable is not already bound, OR if its
        variable is already bound to its argument.
--
            raise BindingException('Variable %s already bound to another '
                                   'value' % (variable))

    def __getitem__(self, variable):
        """
        Return the expression to which 'variable' is bound
        """
--
            except KeyError:
                return intermediate

    def __contains__(self, item):
        return item in self.d

    def __add__(self, other):
        """
        :param other: ``BindingDict`` The dict with which to combine self
        :return: ``BindingDict`` A new dict containing all the elements of both parameters
--
                                   "BindingDicts: '%s' and '%s'"
                                   % (self, other))

    def __len__(self):
        return len(self.d)

    def __str__(self):
        data_str = ', '.join('%s: %s' % (v, self.d[v]) for v in sorted(self.d.keys()))
        return '{' + data_str + '}'

    def __repr__(self):
        return "%s" % self


def most_general_unification(a, b, bindings=None):
    """
    Find the most general unification of the two given expressions

--
               most_general_unification(a.argument, b.argument, bindings)
    raise BindingException((a, b))

def _mgu_var(var, expression, bindings):
    if var.variable in expression.free()|expression.constants():
        raise BindingException((var, expression))
    else:
--


class BindingException(Exception):
    def __init__(self, arg):
        if isinstance(arg, tuple):
            Exception.__init__(self, "'%s' cannot be bound to '%s'" % arg)
        else:
            Exception.__init__(self, arg)

class UnificationException(Exception):
    def __init__(self, a, b):
        Exception.__init__(self, "'%s' cannot unify with '%s'" % (a,b))


class DebugObject(object):
    def __init__(self, enabled=True, indent=0):
        self.enabled = enabled
        self.indent = indent

    def __add__(self, i):
        return DebugObject(self.enabled, self.indent+i)

    def line(self, line):
        if self.enabled:
            print('    '*self.indent + line)


def testResolutionProver():
    resolution_test(r'man(x)')
    resolution_test(r'(man(x) -> man(x))')
    resolution_test(r'(man(x) -> --man(x))')
--
    c = LogicParser().parse(r'some e0.walk(e0,mary)')
    print('%s |- %s: %s' % (p, c, ResolutionProver().prove(c, [p])))

def resolution_test(e):
    f = LogicParser().parse(e)
    t = ResolutionProver().prove(f)
    print('|- %s: %s' % (f, t))

def test_clausify():
    lp = LogicParser()

    print(clausify(lp.parse('P(x) | Q(x)')))
--
    print(clausify(lp.parse('all x.(all y.P(x,y) -> -all y.(Q(x,y) -> R(x,y)))')))


def demo():
    test_clausify()
    print()
    testResolutionProver()
--
from __future__ import print_function, unicode_literals

from .prover9 import Prover9, Prover9Command
from collections import defaultdict
from functools import reduce

from nltk.sem.logic import (VariableExpression, EqualityExpression,
--

class ProverParseError(Exception): pass

def get_domain(goal, assumptions):
    if goal is None:
        all_expressions = assumptions
    else:
--
    This is a prover decorator that adds domain closure assumptions before
    proving.
    """
    def assumptions(self):
        assumptions = [a for a in self._command.assumptions()]
        goal = self._command.goal()
        domain = get_domain(goal, assumptions)
        return [self.replace_quants(ex, domain) for ex in assumptions]

    def goal(self):
        goal = self._command.goal()
        domain = get_domain(goal, self._command.assumptions())
        return self.replace_quants(goal, domain)

    def replace_quants(self, ex, domain):
        """
        Apply the closed domain assumption to the expression
         - Domain = union([e.free()|e.constants() for e in all_expressions])
--
    This is a prover decorator that adds unique names assumptions before
    proving.
    """
    def assumptions(self):
        """
         - Domain = union([e.free()|e.constants() for e in all_expressions])
         - if "d1 = d2" cannot be proven from the premises, then add "d1 != d2"
--
    """
    A list of sets of Variables.
    """
    def __getitem__(self, item):
        """
        :param item: ``Variable``
        :return: the set containing 'item'
--
    -------------------
    -bird(Sam)
    """
    def assumptions(self):
        assumptions = self._command.assumptions()

        predicates = self._make_predicate_dict(assumptions)
--

        return assumptions + new_assumptions

    def _make_unique_signature(self, predHolder):
        """
        This method figures out how many arguments the predicate takes and
        returns a tuple containing that number of unique variables.
        """
        return tuple(unique_variable() for i in range(predHolder.signature_len))

    def _make_antecedent(self, predicate, signature):
        """
        Return an application expression with 'predicate' as the predicate
        and 'signature' as the list of arguments.
--
            antecedent = antecedent(VariableExpression(v))
        return antecedent

    def _make_predicate_dict(self, assumptions):
        """
        Create a dictionary of predicates from the assumptions.

        :param assumptions: a list of ``Expression``s
        :return: dict mapping ``AbstractVariableExpression`` to ``PredHolder``
        """
        predicates = defaultdict(PredHolder)
        for a in assumptions:
            self._map_predicates(a, predicates)
        return predicates

    def _map_predicates(self, expression, predDict):
        if isinstance(expression, ApplicationExpression):
            func, args = expression.uncurry()
            if isinstance(func, AbstractVariableExpression):
--
    This class will be used by a dictionary that will store information
    about predicates to be used by the ``ClosedWorldProver``.

    The 'signatures' property is a list of tuples defining signatures for
    which the predicate is true.  For instance, 'see(john, mary)' would be
    result in the signature '(john,mary)' for 'see'.

--
    'all x.all y.(see(x,y) -> know(x,y))' would result in "((x,y),('see(x,y)'))"
    for 'know'.
    """
    def __init__(self):
        self.signatures = []
        self.properties = []
        self.signature_len = None

    def append_sig(self, new_sig):
        self.validate_sig_len(new_sig)
        self.signatures.append(new_sig)

    def append_prop(self, new_prop):
        self.validate_sig_len(new_prop[0])
        self.properties.append(new_prop)

    def validate_sig_len(self, new_sig):
        if self.signature_len is None:
            self.signature_len = len(new_sig)
        elif self.signature_len != len(new_sig):
            raise Exception("Signature lengths do not match")

    def __str__(self):
        return '(%s,%s,%s)' % (self.signatures, self.properties,
                               self.signature_len)

    def __repr__(self):
        return "%s" % self

def closed_domain_demo():
    lp = LogicParser()

    p1 = lp.parse(r'exists x.walk(x)')
--
    print('goal:', cdp.goal())
    print(cdp.prove())

def unique_names_demo():
    lp = LogicParser()

    p1 = lp.parse(r'man(Socrates)')
--
    print('goal:', unp.goal())
    print(unp.prove())

def closed_world_demo():
    lp = LogicParser()

    p1 = lp.parse(r'walk(Socrates)')
--
    print('goal:', cwp.goal())
    print(cwp.prove())

def combination_prover_demo():
    lp = LogicParser()

    p1 = lp.parse(r'see(Socrates, John)')
--
    for a in command.assumptions(): print(a)
    print(command.prove())

def default_reasoning_demo():
    lp = LogicParser()

    premises = []

    #define taxonomy
    premises.append(lp.parse(r'all x.(elephant(x)        -> animal(x))'))
    premises.append(lp.parse(r'all x.(bird(x)            -> animal(x))'))
    premises.append(lp.parse(r'all x.(dove(x)            -> bird(x))'))
    premises.append(lp.parse(r'all x.(ostrich(x)         -> bird(x))'))
    premises.append(lp.parse(r'all x.(flying_ostrich(x)  -> ostrich(x))'))

    #default properties
    premises.append(lp.parse(r'all x.((animal(x)  & -Ab1(x)) -> -fly(x))')) #normal animals don't fly
    premises.append(lp.parse(r'all x.((bird(x)    & -Ab2(x)) -> fly(x))')) #normal birds fly
    premises.append(lp.parse(r'all x.((ostrich(x) & -Ab3(x)) -> -fly(x))')) #normal ostriches don't fly
--
    premises.append(lp.parse(r'all x.(ostrich(x)        -> Ab2(x))')) #non-flying bird
    premises.append(lp.parse(r'all x.(flying_ostrich(x) -> Ab3(x))')) #flying ostrich

    #define entities
    premises.append(lp.parse(r'elephant(E)'))
    premises.append(lp.parse(r'dove(D)'))
    premises.append(lp.parse(r'ostrich(O)'))
--
    print_proof('fly(D)', premises)
    print_proof('-fly(O)', premises)

def print_proof(goal, premises):
    lp = LogicParser()
    prover = Prover9Command(lp.parse(goal), premises)
    command = UniqueNamesProver(ClosedWorldProver(prover))
    print(goal, prover.prove(), command.prove())

def demo():
    closed_domain_demo()
    unique_names_demo()
    closed_world_demo()
    combination_prover_demo()
    default_reasoning_demo()

if __name__ == '__main__':
    demo()
--
    which is responsible for maintaining a goal and a set of assumptions,
    and generating prover9-style input files from them.
    """
    def print_assumptions(self, output_format='nltk'):
        """
        Print the list of the current assumptions.
        """
--
    the a print_assumptions() method that is used to print the list
    of assumptions in multiple formats.
    """
    def __init__(self, goal=None, assumptions=None, timeout=60, prover=None):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--

        BaseProverCommand.__init__(self, prover, goal, assumptions)

    def decorate_proof(self, proof_string, simplify=True):
        """
        :see BaseProverCommand.decorate_proof()
        """
--

    _binary_location = None

    def config_prover9(self, binary_location, verbose=False):
        if binary_location is None:
            self._binary_location = None
            self._prover9_bin = None
--
                                  verbose=verbose)
            self._binary_location = self._prover9_bin.rsplit(os.path.sep, 1)

    def prover9_input(self, goal, assumptions):
        """
        :return: The input string that should be provided to the
        prover9 binary.  This string is formed based on the goal,
--

        return s

    def binary_locations(self):
        """
        A list of directories that should be searched for the prover9
        executables.  This list is used by ``config_prover9`` when searching
--
                '/usr/local/prover9',
                '/usr/local/share/prover9']

    def _find_binary(self, name, verbose=False):
        binary_locations = self.binary_locations()
        if self._binary_location is not None:
            binary_locations += [self._binary_location]
--
            binary_names=[name, name + '.exe'],
            verbose=verbose)

    def _call(self, input_str, binary, args=[], verbose=False):
        """
        Call the binary with the given input.

--
        return (stdout, p.returncode)


def convert_to_prover9(input):
    """
    Convert a ``logic.Expression`` to Prover9 format.
    """
--
            print('input %s cannot be converted to Prover9 input syntax' % input)
            raise

def _convert_to_prover9(expression):
    """
    Convert ``logic.Expression`` to Prover9 formatted string.
    """
--
    _prover9_bin = None
    _prooftrans_bin = None

    def __init__(self, timeout=60):
        self._timeout = timeout
        """The timeout value for prover9.  If a proof can not be found
           in this amount of time, then prover9 will return false.
           (Use 0 for no timeout.)"""

    def _prove(self, goal=None, assumptions=None, verbose=False):
        """
        Use Prover9 to prove a theorem.
        :return: A pair whose first element is a boolean indicating if the
--
                                                verbose=verbose)
        return (returncode == 0, stdout)

    def prover9_input(self, goal, assumptions):
        """
        :see: Prover9Parent.prover9_input
        """
        s = 'clear(auto_denials).\n' #only one proof required
        return s + Prover9Parent.prover9_input(self, goal, assumptions)

    def _call_prover9(self, input_str, args=[], verbose=False):
        """
        Call the ``prover9`` binary with the given input.

--

        return stdout, returncode

    def _call_prooftrans(self, input_str, args=[], verbose=False):
        """
        Call the ``prooftrans`` binary with the given input.

--


class Prover9Exception(Exception):
    def __init__(self, returncode, message):
        msg = p9_return_codes[returncode]
        if message:
            msg += '\n%s' % message
--
#{ Tests and Demos
######################################################################

def test_config():

    a = LogicParser().parse('(walk(j) & sing(j))')
    g = LogicParser().parse('walk(j)')
--
    print(p.prove())
    print(p.proof())

def test_convert_to_prover9(expr):
    """
    Test that parsing works OK.
    """
--
        e = LogicParser().parse(t)
        print(convert_to_prover9(e))

def test_prove(arguments):
    """
    Try some proofs and exhibit the results.
    """
--
               r'some x.(man(x) & (not walks(x)))',
               r'all x.(man(x) -> walks(x))']

def spacer(num=45):
    print('-' * num)

def demo():
    print("Testing configuration")
    spacer()
    test_config()
--


class ReadingCommand(object):
    def parse_to_readings(self, sentence):
        """
        :param sentence: the sentence to read
        :type sentence: str
        """
        raise NotImplementedError()

    def process_thread(self, sentence_readings):
        """
        This method should be used to handle dependencies between readings such
        as resolving anaphora.
--
        """
        return sentence_readings

    def combine_readings(self, readings):
        """
        :param readings: readings to combine
        :type readings: list(Expression)
--
        """
        raise NotImplementedError()
    
    def to_fol(self, expression):
        """
        Convert this expression into a First-Order Logic expression.
        
--


class CfgReadingCommand(ReadingCommand):
    def __init__(self, gramfile=None):
        """
        :param gramfile: name of file where grammar can be loaded
        :type gramfile: str
--
        self._gramfile = (gramfile if gramfile else 'grammars/book_grammars/discourse.fcfg')
        self._parser = load_parser(self._gramfile)

    def parse_to_readings(self, sentence):
        """:see: ReadingCommand.parse_to_readings()"""
        from nltk.sem import root_semrep
        tokens = sentence.split()
        trees = self._parser.nbest_parse(tokens)
        return [root_semrep(tree) for tree in trees]

    def combine_readings(self, readings):
        """:see: ReadingCommand.combine_readings()"""
        return reduce(and_, readings)
    
    def to_fol(self, expression):
        """:see: ReadingCommand.to_fol()"""
        return expression


class DrtGlueReadingCommand(ReadingCommand):
    def __init__(self, semtype_file=None, remove_duplicates=False,
                 depparser=None):
        """
        :param semtype_file: name of file where grammar can be loaded
--
                             remove_duplicates=remove_duplicates,
                             depparser=depparser)

    def parse_to_readings(self, sentence):
        """:see: ReadingCommand.parse_to_readings()"""
        return self._glue.parse_to_meaning(sentence)

    def process_thread(self, sentence_readings):
        """:see: ReadingCommand.process_thread()"""
        try:
            return [self.combine_readings(sentence_readings)]
        except AnaphoraResolutionException:
            return []

    def combine_readings(self, readings):
        """:see: ReadingCommand.combine_readings()"""
        thread_reading = reduce(add, readings)
        return resolve_anaphora(thread_reading.simplify())
    
    def to_fol(self, expression):
        """:see: ReadingCommand.to_fol()"""
        return expression.fol()

--
    """
    Check properties of an ongoing discourse.
    """
    def __init__(self, input, reading_command=None, background=None):
        """
        Initialize a ``DiscourseTester``.

--
    # Sentences
    ###############################

    def sentences(self):
        """
        Display the list of sentences in the current discourse.
        """
        for id in sorted(self._sentences):
            print("%s: %s" % (id, self._sentences[id]))

    def add_sentence(self, sentence, informchk=False, consistchk=False,):
        """
        Add a sentence to the current discourse.

--
            self.readings(verbose=False)
            self.models(show=False)

    def retract_sentence(self, sentence, verbose=True):
        """
        Remove a sentence from the current discourse.

--
            print("Current sentences are ")
            self.sentences()

    def grammar(self):
        """
        Print out the grammar in use for parsing input sentences
        """
--
    # Readings and Threads
    ###############################

    def _get_readings(self, sentence):
        """
        Build a list of semantic readings for a sentence.

--
        """
        return self._reading_command.parse_to_readings(sentence)

    def _construct_readings(self):
        """
        Use ``self._sentences`` to construct a value for ``self._readings``.
        """
--
            self._readings[sid] = dict([("%s-r%s" % (sid, rid), reading.simplify())
                                                        for rid, reading in enumerate(readings)])

    def _construct_threads(self):
        """
        Use ``self._readings`` to construct a value for ``self._threads``
        and use the model builder to construct a value for ``self._filtered_threads``
--
                self._filtered_threads[tid] = thread


    def _show_readings(self, sentence=None):
        """
        Print out the readings for  the discourse (or a single sentence).
        """
--
                    #TODO lf = lf.normalize('[xyz]\d*', 'z%d')
                    print("%s: %s" % (rid, lf))

    def _show_threads(self, filter=False, show_thread_readings=False):
        """
        Print out the value of ``self._threads`` or ``self._filtered_hreads``
        """
--
            print("%s:" % tid, self._threads[tid], thread_reading)


    def readings(self, sentence=None, threaded=False, verbose=True,
                 filter=False, show_thread_readings=False):
        """
        Construct and show the readings of the discourse (or of a single sentence).
--
                self._show_threads(filter=filter,
                                   show_thread_readings=show_thread_readings)

    def expand_threads(self, thread_id, threads=None):
        """
        Given a thread ID, find the list of ``logic.Expression`` objects corresponding to the reading IDs in that thread.

--
    # Models and Background
    ###############################

    def _check_consistency(self, threads, show=False, verbose=False):
        results = []
        for tid in sorted(threads):
            assumptions = [reading for (rid, reading) in self.expand_threads(tid, threads=threads)]
--
                    print("No model found!\n")
        return results

    def models(self, thread_id=None, show=True, verbose=False):
        """
        Call Mace4 to build a model for each current discourse thread.

--
                    print("    %s: %s" % (rid, reading))
                print()

    def add_background(self, background, verbose=False):
        """
        Add a list of background assumptions for reasoning about the discourse.

--
        self._construct_readings()
        self._construct_threads()

    def background(self):
        """
        Show the current background assumptions.
        """
--
    ###############################

    @staticmethod
    def multiply(discourse, readings):
        """
        Multiply every thread in ``discourse`` by every reading in ``readings``.

--
#L2 = ['a', 'b', 'c']
#print multiply(L1,L2)

def parse_fol(s):
    """
    Temporarily duplicated from ``nltk.sem.util``.
    Convert a  file of first order formulas into a list of ``Expression`` objects.
--
# Demo
###############################

def discourse_demo(reading_command=None):
    """
    Illustrate the various methods of ``DiscourseTester``
    """
--
    dt.models()


def drt_discourse_demo(reading_command=None):
    """
    Illustrate the various methods of ``DiscourseTester``
    """
--
    dt.readings(filter=True, show_thread_readings=True)


def spacer(num=30):
    print('-' * num)

def demo():
    discourse_demo()

    tagger = RegexpTagger(
--
    Interface for trying to prove a goal from assumptions.  Both the goal and
    the assumptions are constrained to be formulas of ``logic.Expression``.
    """
    def prove(self, goal=None, assumptions=None, verbose=False):
        """
        :return: Whether the proof was successful or not.
        :rtype: bool
        """
        return self._prove(goal, assumptions, verbose)[0]

    def _prove(self, goal=None, assumptions=None, verbose=False):
        """
        :return: Whether the proof was successful or not, along with the proof
        :rtype: tuple: (bool, str)
--
    Both the goal and the assumptions are constrained to be formulas
    of ``logic.Expression``.
    """
    def build_model(self, goal=None, assumptions=None, verbose=False):
        """
        Perform the actual model building.
        :return: Whether a model was generated
--
        """
        return self._build_model(goal, assumptions, verbose)[0]

    def _build_model(self, goal=None, assumptions=None, verbose=False):
        """
        Perform the actual model building.
        :return: Whether a model was generated, and the model itself
--
    This class holds a goal and a list of assumptions to be used in proving
    or model building.
    """
    def add_assumptions(self, new_assumptions):
        """
        Add new assumptions to the assumption list.

--
        """
        raise NotImplementedError()

    def retract_assumptions(self, retracted, debug=False):
        """
        Retract assumptions from the assumption list.

--
        """
        raise NotImplementedError()

    def assumptions(self):
        """
        List the current assumptions.

--
        """
        raise NotImplementedError()

    def goal(self):
        """
        Return the goal

--
        """
        raise NotImplementedError()

    def print_assumptions(self):
        """
        Print the list of the current assumptions.
        """
--
    This class holds a ``Prover``, a goal, and a list of assumptions.  When
    prove() is called, the ``Prover`` is executed with the goal and assumptions.
    """
    def prove(self, verbose=False):
        """
        Perform the actual proof.
        """
        raise NotImplementedError()

    def proof(self, simplify=True):
        """
        Return the proof string
        :param simplify: bool simplify the proof?
--
        """
        raise NotImplementedError()

    def get_prover(self):
        """
        Return the prover object
        :return: ``Prover``
--
    When build_model() is called, the ``ModelBuilder`` is executed with the goal
    and assumptions.
    """
    def build_model(self, verbose=False):
        """
        Perform the actual model building.
        :return: A model if one is generated; None otherwise.
--
        """
        raise NotImplementedError()

    def model(self, format=None):
        """
        Return a string representation of the model

--
        """
        raise NotImplementedError()

    def get_model_builder(self):
        """
        Return the model builder object
        :return: ``ModelBuilder``
--
    This class holds a goal and a list of assumptions to be used in proving
    or model building.
    """
    def __init__(self, goal=None, assumptions=None):
        """
        :param goal: Input expression to prove
        :type goal: sem.Expression
--
        self._result = None
        """A holder for the result, to prevent unnecessary re-proving"""

    def add_assumptions(self, new_assumptions):
        """
        Add new assumptions to the assumption list.

--
        self._assumptions.extend(new_assumptions)
        self._result = None

    def retract_assumptions(self, retracted, debug=False):
        """
        Retract assumptions from the assumption list.

--

        self._result = None

    def assumptions(self):
        """
        List the current assumptions.

--
        """
        return self._assumptions

    def goal(self):
        """
        Return the goal

--
        """
        return self._goal

    def print_assumptions(self):
        """
        Print the list of the current assumptions.
        """
--
    This class holds a ``Prover``, a goal, and a list of assumptions.  When
    prove() is called, the ``Prover`` is executed with the goal and assumptions.
    """
    def __init__(self, prover, goal=None, assumptions=None):
        """
        :param prover: The theorem tool to execute with the assumptions
        :type prover: Prover
--

        self._proof = None

    def prove(self, verbose=False):
        """
        Perform the actual proof.  Store the result to prevent unnecessary
        re-proving.
--
                                                            verbose)
        return self._result

    def proof(self, simplify=True):
        """
        Return the proof string
        :param simplify: bool simplify the proof?
--
        else:
            return self.decorate_proof(self._proof, simplify)

    def decorate_proof(self, proof_string, simplify=True):
        """
        Modify and return the proof string
        :param proof_string: str the proof to decorate
--
        """
        return proof_string

    def get_prover(self):
        return self._prover


--
    build_model() is called, the ``ModelBuilder`` is executed with the goal and
    assumptions.
    """
    def __init__(self, modelbuilder, goal=None, assumptions=None):
        """
        :param modelbuilder: The theorem tool to execute with the assumptions
        :type modelbuilder: ModelBuilder
--

        self._model = None

    def build_model(self, verbose=False):
        """
        Attempt to build a model.  Store the result to prevent unnecessary
        re-building.
--
                                                    verbose)
        return self._result

    def model(self, format=None):
        """
        Return a string representation of the model

--
        else:
            return self._decorate_model(self._model, format)

    def _decorate_model(self, valuation_str, format=None):
        """
        :param valuation_str: str with the model builder's output
        :param format: str indicating the format for displaying
--
        """
        return valuation_str

    def get_model_builder(self):
        return self._modelbuilder


--
    A base decorator for the ``ProverCommandDecorator`` and
    ``ModelBuilderCommandDecorator`` classes from which decorators can extend.
    """
    def __init__(self, command):
        """
        :param command: ``TheoremToolCommand`` to decorate
        """
--
        #underlying command
        self._result = None

    def assumptions(self):
        return self._command.assumptions()

    def goal(self):
        return self._command.goal()

    def add_assumptions(self, new_assumptions):
        self._command.add_assumptions(new_assumptions)
        self._result = None

    def retract_assumptions(self, retracted, debug=False):
        self._command.retract_assumptions(retracted, debug)
        self._result = None

    def print_assumptions(self):
        self._command.print_assumptions()


--
    A base decorator for the ``ProverCommand`` class from which other
    prover command decorators can extend.
    """
    def __init__(self, proverCommand):
        """
        :param proverCommand: ``ProverCommand`` to decorate
        """
--
        #because they may be different from the underlying command
        self._proof = None

    def prove(self, verbose=False):
        if self._result is None:
            prover = self.get_prover()
            self._result, self._proof = prover._prove(self.goal(),
--
                                                      verbose)
        return self._result

    def proof(self, simplify=True):
        """
        Return the proof string
        :param simplify: bool simplify the proof?
--
        else:
            return self.decorate_proof(self._proof, simplify)

    def decorate_proof(self, proof_string, simplify=True):
        """
        Modify and return the proof string
        :param proof_string: str the proof to decorate
--
        """
        return self._command.decorate_proof(proof_string, simplify)

    def get_prover(self):
        return self._command.get_prover()


--
    A base decorator for the ``ModelBuilderCommand`` class from which other
    prover command decorators can extend.
    """
    def __init__(self, modelBuilderCommand):
        """
        :param modelBuilderCommand: ``ModelBuilderCommand`` to decorate
        """
--
        #because they may be different from the underlying command
        self._model = None

    def build_model(self, verbose=False):
        """
        Attempt to build a model.  Store the result to prevent unnecessary
        re-building.
--
                                                      verbose)
        return self._result

    def model(self, format=None):
        """
        Return a string representation of the model

--
        else:
            return self._decorate_model(self._model, format)

    def _decorate_model(self, valuation_str, format=None):
        """
        Modify and return the proof string
        :param valuation_str: str with the model builder's output
--
        """
        return self._command._decorate_model(valuation_str, format)

    def get_model_builder(self):
        return self._command.get_prover()


--
    parallel.  Whichever finishes first, the prover or the model builder, is the
    result that will be used.
    """
    def __init__(self, prover, modelbuilder):
        self._prover = prover
        self._modelbuilder = modelbuilder

    def _prove(self, goal=None, assumptions=None, verbose=False):
        return self._run(goal, assumptions, verbose), ''

    def _build_model(self, goal=None, assumptions=None, verbose=False):
        return not self._run(goal, assumptions, verbose), ''

    def _run(self, goal, assumptions, verbose):
        # Set up two thread, Prover and ModelBuilder to run in parallel
        tp_thread = TheoremToolThread(lambda: self._prover.prove(goal, assumptions, verbose), verbose, 'TP')
        mb_thread = TheoremToolThread(lambda: self._modelbuilder.build_model(goal, assumptions, verbose), verbose, 'MB')
--
    Because the theorem prover result is the opposite of the model builder
    result, we will treat self._result as meaning "proof found/no model found".
    """
    def __init__(self, prover, modelbuilder, goal=None, assumptions=None):
        BaseProverCommand.__init__(self, prover, goal, assumptions)
        BaseModelBuilderCommand.__init__(self, modelbuilder, goal, assumptions)

    def prove(self, verbose=False):
        return self._run(verbose)

    def build_model(self, verbose=False):
        return not self._run(verbose)

    def _run(self, verbose):
        # Set up two thread, Prover and ModelBuilder to run in parallel
        tp_thread = TheoremToolThread(lambda: BaseProverCommand.prove(self, verbose), verbose, 'TP')
        mb_thread = TheoremToolThread(lambda: BaseModelBuilderCommand.build_model(self, verbose), verbose, 'MB')
--


class TheoremToolThread(threading.Thread):
    def __init__(self, command, verbose, name=None):
        threading.Thread.__init__(self)
        self._command = command
        self._result = None
        self._verbose = verbose
        self._name = name

    def run(self):
        try:
            self._result = self._command()
            if self._verbose:
--
            print('Thread %s completed abnormally' % (self._name))

    @property
    def result(self): return self._result
--

class MaltParser(ParserI):

    def __init__(self, tagger=None, mco=None, working_dir=None, additional_java_args=None):
        """
        An interface for parsing with the Malt Parser.

--
             (r'.*s$', 'NNS'),                  # plural nouns
             (r'.*ing$', 'VBG'),                # gerunds
             (r'.*ed$', 'VBD'),                 # past tense verbs
             (r'.*', 'NN')                      # nouns (default)
             ])

    def config_malt(self, bin=None, verbose=False):
        """
        Configure NLTK's interface to the ``malt`` package.  This
        searches for a directory containing the malt jar
--
            url='http://www.maltparser.org/',
            verbose=verbose)

    def parse(self, sentence, verbose=False):
        """
        Use MaltParser to parse a sentence. Takes a sentence as a list of
        words; it will be automatically tagged with this MaltParser instance's
--
        """
        return self.batch_parse([sentence], verbose)[0]

    def batch_parse(self, sentences, verbose=False):
        """
        Use MaltParser to parse multiple sentence. Takes multiple sentences as a
        list where each sentence is a list of words.
--
        tagged_sentences = [self.tagger.tag(sentence) for sentence in sentences]
        return self.tagged_batch_parse(tagged_sentences, verbose)

    def raw_parse(self, sentence, verbose=False):
        """
        Use MaltParser to parse a sentence. Takes a sentence as a string;
        before parsing, it will be automatically tokenized and tagged with this
--
        words = word_tokenize(sentence)
        return self.parse(words, verbose)

    def tagged_parse(self, sentence, verbose=False):
        """
        Use MaltParser to parse a sentence. Takes a sentence as a list of
        (word, tag) tuples; the sentence must have already been tokenized and
--
        """
        return self.tagged_batch_parse([sentence], verbose)[0]

    def tagged_batch_parse(self, sentences, verbose=False):
        """
        Use MaltParser to parse multiple sentences. Takes multiple sentences
        where each sentence is a list of (word, tag) tuples.
--
            output_file.close()
            os.remove(output_file.name)

    def train(self, depgraphs, verbose=False):
        """
        Train MaltParser from a list of ``DependencyGraph`` objects

--
            input_file.close()
            os.remove(input_file.name)

    def train_from_file(self, conll_file, verbose=False):
        """
        Train MaltParser from a file

--
        self._trained = True

    @staticmethod
    def _execute(cmd, verbose=False):
        output = None if verbose else subprocess.PIPE
        p = subprocess.Popen(cmd, stdout=output, stderr=output)
        return p.wait()


def demo():
    dg1 = DependencyGraph("""1    John    _    NNP   _    _    2    SUBJ    _    _
                             2    sees    _    VB    _    _    0    ROOT    _    _
                             3    a       _    DT    _    _    4    SPEC    _    _
--
However, the parser module does *not* distinguish these two types of
ambiguity.

The parser module defines ``ParserI``, a standard interface for parsing
texts; and two simple implementations of that interface,
``ShiftReduceParser`` and ``RecursiveDescentParser``.  It also contains
three sub-modules for specialized kinds of parsing:

  - ``nltk.parser.chart`` defines chart parsing, which uses dynamic
    programming to efficiently parse texts.
  - ``nltk.parser.probabilistic`` defines probabilistic parsing, which
    associates a probability with each parse.
"""

--
#
from __future__ import print_function, unicode_literals

from collections import defaultdict

from nltk.grammar import (DependencyProduction, DependencyGrammar,
                          StatisticalDependencyGrammar, parse_dependency_grammar)
--
    to the head word for the entire span.  This is the same as the root node if
    the dependency structure were depicted as a graph.
    """
    def __init__(self, start_index, end_index, head_index, arcs, tags):
        self._start_index = start_index
        self._end_index = end_index
        self._head_index = head_index
--
        self._comparison_key = (start_index, end_index, head_index, tuple(arcs))
        self._hash = hash(self._comparison_key)

    def head_index(self):
        """
        :return: An value indexing the head of the entire ``DependencySpan``.
        :rtype: int
        """
        return self._head_index

    def __repr__(self):
        """
        :return: A concise string representatino of the ``DependencySpan``.
        :rtype: str.
        """
        return 'Span %d-%d; Head Index: %d' % (self._start_index, self._end_index, self._head_index)

    def __str__(self):
        """
        :return: A verbose string representation of the ``DependencySpan``.
        :rtype: str
--
            str += '\n%d <- %d, %s' % (i, self._arcs[i], self._tags[i])
        return str

    def __eq__(self, other):
        return (type(self) == type(other) and
                self._comparison_key == other._comparison_key)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, DependencySpan):
            raise_unorderable_types("<", self, other)
        return self._comparison_key < other._comparison_key

    def __hash__(self):
        """
        :return: The hash value of this ``DependencySpan``.
        """
--
    Each cell keeps track of its x and y coordinates (though this will probably
    be discarded), and a list of spans serving as the cell's entries.
    """
    def __init__(self, x, y):
        """
        :param x: This cell's x coordinate.
        :type x: int.
--
        self._y = y
        self._entries = set([])

    def add(self, span):
        """
        Appends the given span to the list of spans
        representing the chart cell's entries.
--
        """
        self._entries.add(span)

    def __str__(self):
        """
        :return: A verbose string representation of this ``ChartCell``.
        :rtype: str.
        """
        return 'CC[%d,%d]: %s' % (self._x, self._y, self._entries)

    def __repr__(self):
        """
        :return: A concise string representation of this ``ChartCell``.
        :rtype: str.
--
    in the parse tree form a continuous substring of the input sequence.
    """

    def __init__(self, dependency_grammar):
        """
        Create a new ProjectiveDependencyParser, from a word-to-word
        dependency grammar ``DependencyGrammar``.
--
        """
        self._grammar = dependency_grammar

    def parse(self, tokens):
        """
        Performs a projective dependency parse on the list of tokens using
        a chart-based, span-concatenation algorithm similar to Eisner (1996).
--
        return trees


    def concatenate(self, span1, span2):
        """
        Concatenates the two spans in whichever way possible.  This
        includes rightward concatenation (from the leftmost word of the
--
    identical to the one utilized by the rule-based projective parser.
    """

    def __init__(self):
        """
        Create a new probabilistic dependency parser.  No additional
        operations are necessary.
        """
        print('')

    def parse(self, tokens):
        """
        Parses the list of tokens subject to the projectivity constraint
        and the productions in the parser's grammar.  This uses a method
        similar to the span-concatenation algorithm defined in Eisner (1996).
        It returns the most probable parse derived from the parser's
        probabilistic dependency grammar.
        """
--
        return [max_parse, max_score]


    def concatenate(self, span1, span2):
        """
        Concatenates the two spans in whichever way possible.  This
        includes rightward concatenation (from the leftmost word of the
--
            spans.append(DependencySpan(span1._start_index, span2._end_index, span2._head_index, new_arcs, new_tags))
        return spans

    def train(self, graphs):
        """
        Trains a StatisticalDependencyGrammar based on the list of input
        DependencyGraphs.  This model is an implementation of Eisner's (1996)
--
        :type: list(DependencyGraph)
        """
        productions = []
        events = defaultdict(int)
        tags = {}
        for dg in graphs:
            for node_index in range(1,len(dg.nodelist)):
--
        self._grammar = StatisticalDependencyGrammar(productions, events, tags)
#        print self._grammar

    def compute_prob(self, dg):
        """
        Computes the probability of a dependency graph based
        on the parser's probability model (defined by the parser's
        statistical dependency grammar).

        :param dg: A dependency graph to score.
--
# Demos
#################################################################

def demo():
    projective_rule_parse_demo()
#    arity_parse_demo()
    projective_prob_parse_demo()


def projective_rule_parse_demo():
    """
    A demonstration showing the creation and use of a
    ``DependencyGrammar`` to perform a projective dependency
--
    for tree in trees:
        print(tree)

def arity_parse_demo():
    """
    A demonstration showing the creation of a ``DependencyGrammar``
    in which a specific number of modifiers is listed for a given
--
    for tree in trees:
        print(tree)

def projective_prob_parse_demo():
    """
    A demo showing the training and use of a projective
    dependency parser.
--

    :see: ``nltk.grammar``
    """
    def __init__(self, grammar, trace=0):
        """
        Create a new ``ShiftReduceParser``, that uses ``grammar`` to
        parse texts.
--
        self._trace = trace
        self._check_grammar()

    def grammar(self):
        return self._grammar

    def parse(self, tokens):
        tokens = list(tokens)
        self._grammar.check_coverage(tokens)

--
        # We parsed successfully!
        return stack[0]

    def _shift(self, stack, remaining_text):
        """
        Move a token from the beginning of ``remaining_text`` to the
        end of ``stack``.
--
        remaining_text.remove(remaining_text[0])
        if self._trace: self._trace_shift(stack, remaining_text)

    def _match_rhs(self, rhs, rightmost_stack):
        """
        :rtype: bool
        :return: true if the right hand side of a CFG production
--
                if rightmost_stack[i] != rhs[i]: return 0
        return 1

    def _reduce(self, stack, remaining_text, production=None):
        """
        Find a CFG production whose right hand side matches the
        rightmost stack elements; and combine those stack elements
--
        # We didn't reduce anything
        return None

    def trace(self, trace=2):
        """
        Set the level of tracing output that should be generated when
        parsing a text.
--
        # 3: display which tokens & productions are shifed/reduced
        self._trace = trace

    def _trace_stack(self, stack, remaining_text, marker=' '):
        """
        Print trace output displaying the given stack and text.

--
        s += '* ' + ' '.join(remaining_text) + ']'
        print(s)

    def _trace_shift(self, stack, remaining_text):
        """
        Print trace output displaying that a token has been shifted.

--
        if self._trace == 2: self._trace_stack(stack, remaining_text, 'S')
        elif self._trace > 0: self._trace_stack(stack, remaining_text)

    def _trace_reduce(self, stack, production, remaining_text):
        """
        Print trace output displaying that ``production`` was used to
        reduce ``stack``.
--
        if self._trace == 2: self._trace_stack(stack, remaining_text, 'R')
        elif self._trace > 1: self._trace_stack(stack, remaining_text)

    def _check_grammar(self):
        """
        Check to make sure that all of the CFG productions are
        potentially useful.  If any productions can never be used,
--
        history is used to implement the ``undo`` operation.
    :see: ``nltk.grammar``
    """
    def __init__(self, grammar, trace=0):
        self._grammar = grammar
        self._trace = trace
        self._stack = None
        self._remaining_text = None
        self._history = []

    def nbest_parse(self, tokens, n=None):
        tokens = list(tokens)
        self.initialize(tokens)
        while self.step(): pass

        return self.parses()[:n]

    def stack(self):
        """
        :return: The parser's stack.
        :rtype: list(str and Tree)
        """
        return self._stack

    def remaining_text(self):
        """
        :return: The portion of the text that is not yet covered by the
            stack.
--
        """
        return self._remaining_text

    def initialize(self, tokens):
        """
        Start parsing a given text.  This sets the parser's stack to
        ``[]`` and sets its remaining text to ``tokens``.
--
        self._remaining_text = tokens
        self._history = []

    def step(self):
        """
        Perform a single parsing operation.  If a reduction is
        possible, then perform that reduction, and return the
--
        """
        return self.reduce() or self.shift()

    def shift(self):
        """
        Move a token from the beginning of the remaining text to the
        end of the stack.  If there are no more tokens in the
--
        self._shift(self._stack, self._remaining_text)
        return 1

    def reduce(self, production=None):
        """
        Use ``production`` to combine the rightmost stack elements into
        a single Tree.  If ``production`` does not match the
--
        if not return_val: self._history.pop()
        return return_val

    def undo(self):
        """
        Return the parser to its state before the most recent
        shift or reduce operation.  Calling ``undo`` repeatedly return
--
        (self._stack, self._remaining_text) = self._history.pop()
        return 1

    def reducible_productions(self):
        """
        :return: A list of the productions for which reductions are
            available for the current parser state.
--
                productions.append(production)
        return productions

    def parses(self):
        """
        :return: A list of the parses that have been found by this
            parser so far.
--

# copied from nltk.parser

    def set_grammar(self, grammar):
        """
        Change the grammar used to parse texts.

--
##  Demonstration Code
##//////////////////////////////////////////////////////

def demo():
    """
    A demonstration of the shift-reduce parser.
    """
--
import sys
from nltk.grammar import Nonterminal, parse_cfg

def generate(grammar, start=None, depth=None, n=None):
    """
    Generates a list of sentences from a CFG.

--
    sentences = generate_iter(grammar, start, depth)
    return list(itertools.islice(sentences, n))

def generate_iter(grammar, start=None, depth=None):
    """
    Generates an iterator of all sentences from a CFG.

--
        depth = sys.maxsize
    return _generate_all(grammar, [start], depth)

def _generate_all(grammar, items, depth):
    if items:
        for frag1 in _generate_one(grammar, items[0], depth):
            for frag2 in _generate_all(grammar, items[1:], depth):
--
    else:
        yield []

def _generate_one(grammar, item, depth):
    if depth > 0:
        if isinstance(item, Nonterminal):
            for prod in grammar.productions(lhs=item):
--
  P -> 'in' | 'with'
"""

def demo(N=23):
    print('Generating the first %d sentences for demo grammar:' % (N,))
    print(demo_grammar)
    grammar = parse_cfg(demo_grammar)
--
    """
    A container for the nodes and labelled edges of a dependency structure.
    """
    def __init__(self, tree_str=None):
        """
        We place a dummy 'top' node in the first position
        in the nodelist, since the root node is often assigned '0'
--
        if tree_str:
            self._parse(tree_str)

    def remove_by_address(self, address):
        """
        Removes the node with the given address.  References
        to this node in others will still exist.
--
                self.nodelist.pop(node_index)
            node_index -= 1

    def redirect_arcs(self, originals, redirect):
        """
        Redirects arcs to any of the nodes in the originals list
        to the redirect node address.
--
                    new_deps.append(dep)
            node['deps'] = new_deps

    def add_arc(self, head_address, mod_address):
        """
        Adds an arc from the node specified by head_address to the
        node specified by the mod address.
--
            if node['address'] == head_address and (mod_address not in node['deps']):
                node['deps'].append(mod_address)

    def connect_graph(self):
        """
        Fully connects all non-root nodes.  All nodes are set to be dependents
        of the root node.
--
                    node1['deps'].append(node2['address'])

    # fix error and return
    def get_by_address(self, node_address):
        """
        Returns the node with the given address.
        """
--
        print('THROW ERROR: address not found in -get_by_address-')
        return -1

    def contains_address(self, node_address):
        """
        Returns true if the graph contains a node with the given node
        address, false otherwise.
--
                return True
        return False

    def __str__(self):
        return pformat(self.nodelist)

    def __repr__(self):
        return "<DependencyGraph with %d nodes>" % len(self.nodelist)

    @staticmethod
    def load(file):
        """
        :param file: a file in Malt-TAB format
        :return: a list of DependencyGraphs
--
                                                  f.read().split('\n\n')]

    @staticmethod
    def _normalize(line):
        """
        Deal with lines in which spaces are used rather than tabs.
        """
        SPC = re.compile(' +')
        return re.sub(SPC, '\t', line).strip()

    def left_children(self, node_index):
        """
        Returns the number of left children under the node specified
        by the given address.
--
        index = self.nodelist[node_index]['address']
        return sum(1 for c in children if c < index)

    def right_children(self, node_index):
        """
        Returns the number of right children under the node specified
        by the given address.
--
        index = self.nodelist[node_index]['address']
        return sum(1 for c in children if c > index)

    def add_node(self, node):
        if not self.contains_address(node['address']):
            self.nodelist.append(node)

    def _parse(self, input):
        lines = [DependencyGraph._normalize(line) for line in input.split('\n') if line.strip()]
        temp = []
        for index, line in enumerate(lines):
--
        root_address = self.nodelist[0]['deps'][0]
        self.root = self.nodelist[root_address]

    def _word(self, node, filter=True):
        w = node['word']
        if filter:
            if w != ',': return w
        return w

    def _tree(self, i):
        """
        Recursive function for turning dependency graphs into
        NLTK trees.
--

        return (Tree(word, [self._tree(j) for j in deps]) if len(deps) != 0 else word)

    def tree(self):
        """
        Starting with the ``root`` node, build a dependency tree using the NLTK
        ``Tree`` constructor. Dependency labels are omitted.
--
        deps = node['deps']
        return Tree(word, [self._tree(i) for i in deps])

    def _hd(self, i):
        try:
            return self.nodelist[i]['head']
        except IndexError:
            return None

    def _rel(self, i):
        try:
            return self.nodelist[i]['rel']
        except IndexError:
            return None

    # what's the return type?  Boolean or list?
    def contains_cycle(self):
        distances = {}
        for node in self.nodelist:
            for dep in node['deps']:
--
        return False  # return []?


    def get_cycle_path(self, curr_node, goal_node_index):
        for dep in curr_node['deps']:
            if dep == goal_node_index:
                return [curr_node['address']]
--
                return path
        return []

    def to_conll(self, style):
        """
        The dependency graph in CoNLL format.

--
        return ''.join(lines)


def nx_graph(self):
    """
    Convert the data in a ``nodelist`` into a networkx
    labeled directed graph.
--

    return g

def demo():
    malt_demo()
    conll_demo()
    conll_file_demo()
    cycle_finding_demo()

def malt_demo(nx=False):
    """
    A demonstration of the result of reading a dependency
    version of the first sentence of the Penn Treebank.
--
        P.show()


def conll_demo():
    """
    A demonstration of how to read a string representation of
    a CoNLL format dependency tree.
--
    print(dg)
    print(dg.to_conll(4))

def conll_file_demo():
    print('Mass conll_read demo...')
    graphs = [DependencyGraph(entry)
              for entry in conll_data2.split('\n\n') if entry]
--
        tree = graph.tree()
        print('\n' + tree.pprint())

def cycle_finding_demo():
    dg = DependencyGraph(treebank_data)
    print(dg.contains_cycle())
    cyclic_dg = DependencyGraph()
--
    every nonterminal in the edge whose symbol implements the
    interface ``SubstituteBindingsI``.
    """
    def __init__(self, span, lhs, rhs, dot=0, bindings=None):
        """
        Construct a new edge.  If the edge is incomplete (i.e., if
        ``dot<len(rhs)``), then store the bindings as-is.  If the edge
--
        self._comparison_key = (self._comparison_key, tuple(sorted(bindings.items())))

    @staticmethod
    def from_production(production, index):
        """
        :return: A new ``TreeEdge`` formed from the given production.
            The new edge's left-hand side and right-hand side will
--
        return FeatureTreeEdge(span=(index, index), lhs=production.lhs(),
                               rhs=production.rhs(), dot=0)

    def move_dot_forward(self, new_end, bindings=None):
        """
        :return: A new ``FeatureTreeEdge`` formed from this edge.
            The new edge's dot position is increased by ``1``,
--
                               lhs=self._lhs, rhs=self._rhs,
                               dot=self._dot+1, bindings=bindings)

    def _bind(self, nt, bindings):
        if not isinstance(nt, FeatStructNonterminal): return nt
        return nt.substitute_bindings(bindings)

    def next_with_bindings(self):
        return self._bind(self.nextsym(), self._bindings)

    def bindings(self):
        """
        Return a copy of this edge's bindings dictionary.
        """
        return self._bindings.copy()

    def variables(self):
        """
        :return: The set of variables used by this edge.
        :rtype: set(Variable)
--
                              list(self._bindings.values()),
                              fs_class=FeatStruct)

    def __str__(self):
        if self.is_complete():
            return TreeEdge.__unicode__(self)
        else:
--
    :see: ``Chart`` for more information.
    """

    def select(self, **restrictions):
        """
        Returns an iterator over the edges in this chart.
        See ``Chart.select`` for more information about the
--
                     for key in restr_keys)
        return iter(self._indexes[restr_keys].get(vals, []))

    def _add_index(self, restr_keys):
        """
        A helper function for ``select``, which creates a new index for
        a given set of attributes (aka restriction keys).
--
        for edge in self._edges:
            vals = tuple(self._get_type_if_possible(getattr(edge, key)())
                         for key in restr_keys)
            index.setdefault(vals, []).append(edge)

    def _register_with_indexes(self, edge):
        """
        A helper function for ``insert``, which registers the new
        edge with all existing indexes.
--
        for (restr_keys, index) in self._indexes.items():
            vals = tuple(self._get_type_if_possible(getattr(edge, key)())
                         for key in restr_keys)
            index.setdefault(vals, []).append(edge)

    def _get_type_if_possible(self, item):
        """
        Helper function which returns the ``TYPE`` feature of the ``item``,
        if it exists, otherwise it returns the ``item`` itself
--
        else:
            return item

    def parses(self, start, tree_class=Tree):
        trees = []
        for edge in self.select(start=0, end=self._num_leaves):
            if ( (isinstance(edge, FeatureTreeEdge)) and
--

    assuming that B1 and B2 can be unified to generate B3.
    """
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.is_incomplete() and
--
    """
    _fundamental_rule = FeatureFundamentalRule()

    def _apply_complete(self, chart, grammar, right_edge):
        fr = self._fundamental_rule
        for left_edge in chart.select(end=right_edge.start(),
                                      is_complete=False,
--
            for new_edge in fr.apply_iter(chart, grammar, left_edge, right_edge):
                yield new_edge

    def _apply_incomplete(self, chart, grammar, left_edge):
        fr = self._fundamental_rule
        for right_edge in chart.select(start=left_edge.end(),
                                       is_complete=True,
--
#////////////////////////////////////////////////////////////

class FeatureTopDownInitRule(TopDownInitRule):
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(lhs=grammar.start()):
            new_edge = FeatureTreeEdge.from_production(prod, 0)
            if chart.insert(new_edge, ()):
--
    for each grammar production ``B2 -> gamma``, assuming that B1
    and B2 can be unified.
    """
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        nextsym, index = edge.nextsym(), edge.end()
        if not is_nonterminal(nextsym): return
--
#////////////////////////////////////////////////////////////

class FeatureBottomUpPredictRule(BottomUpPredictRule):
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions(rhs=edge.lhs()):
            if isinstance(edge, FeatureTreeEdge):
--
                yield new_edge

class FeatureBottomUpPredictCombineRule(BottomUpPredictCombineRule):
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        found = edge.lhs()
        for prod in grammar.productions(rhs=found):
--
                yield new_edge

class FeatureEmptyPredictRule(EmptyPredictRule):
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(empty=True):
            for index in xrange(chart.num_leaves() + 1):
                new_edge = FeatureTreeEdge.from_production(prod, index)
--
                          FeatureSingleEdgeFundamentalRule()]

class FeatureChartParser(ChartParser):
    def __init__(self, grammar,
                 strategy=BU_LC_FEATURE_STRATEGY,
                 trace_chart_width=20,
                 chart_class=FeatureChart,
--
                             **parser_args)

class FeatureTopDownChartParser(FeatureChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureChartParser.__init__(self, grammar, TD_FEATURE_STRATEGY, **parser_args)

class FeatureBottomUpChartParser(FeatureChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureChartParser.__init__(self, grammar, BU_FEATURE_STRATEGY, **parser_args)

class FeatureBottomUpLeftCornerChartParser(FeatureChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureChartParser.__init__(self, grammar, BU_LC_FEATURE_STRATEGY, **parser_args)


--
    variables in the edge's ``lhs`` whose names start with '@' will be
    replaced by unique new ``Variable``s.
    """
    def __init__(self, tokens):
        FeatureChart.__init__(self, tokens)

    def initialize(self):
        self._instantiated = set()
        FeatureChart.initialize(self)

    def insert(self, edge, child_pointer_list):
        if edge in self._instantiated: return False
        self.instantiate_edge(edge)
        return FeatureChart.insert(self, edge, child_pointer_list)

    def instantiate_edge(self, edge):
        """
        If the edge is a ``FeatureTreeEdge``, and it is complete,
        then instantiate all variables whose names start with '@',
--
        self._instantiated.add(edge)
        edge._lhs = edge.lhs().substitute_bindings(inst_vars)

    def inst_vars(self, edge):
        return dict((var, logic.unique_variable())
                    for var in edge.lhs().variables()
                    if var.name.startswith('@'))
--
# Demo
#////////////////////////////////////////////////////////////

def demo_grammar():
    from nltk.grammar import parse_fcfg
    return parse_fcfg("""
S  -> NP VP
--
Prep -> "under"
""")

def demo(should_print_times=True, should_print_grammar=True,
         should_print_trees=True, should_print_sentence=True,
         trace=1,
         parser=FeatureChartParser,
--
    else:
        print("Nr trees:", len(trees))

def run_profile():
    import profile
    profile.run('for i in range(1): demo()', '/tmp/profile.out')
    import pstats
--

    :see: ``nltk.grammar``
    """
    def __init__(self, grammar, trace=0):
        """
        Create a new ``RecursiveDescentParser``, that uses ``grammar``
        to parse texts.
--
        self._grammar = grammar
        self._trace = trace

    def grammar(self):
        return self._grammar

    def nbest_parse(self, tokens, n=None):
        # Inherit docs from ParserI

        tokens = list(tokens)
--
        # Return the parses.
        return parses[:n]

    def _parse(self, remaining_text, tree, frontier):
        """
        Recursively expand and match each elements of ``tree``
        specified by ``frontier``, to cover ``remaining_text``.  Return
--
        else:
            return self._match(remaining_text, tree, frontier)

    def _match(self, rtext, tree, frontier):
        """
        :rtype: list of Tree
        :return: a list of all parses that can be generated by
--
                self._trace_backtrack(tree, frontier, rtext[:1])
            return []

    def _expand(self, remaining_text, tree, frontier, production=None):
        """
        :rtype: list of Tree
        :return: A list of all parses that can be generated by
--
                                      new_frontier + frontier[1:])
        return parses

    def _production_to_tree(self, production):
        """
        :rtype: Tree
        :return: The Tree that is licensed by ``production``.
--
                children.append(elt)
        return Tree(production.lhs().symbol(), children)

    def trace(self, trace=2):
        """
        Set the level of tracing output that should be generated when
        parsing a text.
--
        """
        self._trace = trace

    def _trace_fringe(self, tree, treeloc=None):
        """
        Print trace output displaying the fringe of ``tree``.  The
        fringe of ``tree`` consists of all of its leaves and all of
--
        else:
            print(unicode_repr(tree), end=' ')

    def _trace_tree(self, tree, frontier, operation):
        """
        Print trace output displaying the parser's current state.

--
        else: self._trace_fringe(tree)
        print(']')

    def _trace_start(self, tree, frontier, text):
        print('Parsing %r' % " ".join(text))
        if self._trace > 2: print('Start:')
        if self._trace > 1: self._trace_tree(tree, frontier, ' ')

    def _trace_expand(self, tree, frontier, production):
        if self._trace > 2: print('Expand: %s' % production)
        if self._trace > 1: self._trace_tree(tree, frontier, 'E')

    def _trace_match(self, tree, frontier, tok):
        if self._trace > 2: print('Match: %r' % tok)
        if self._trace > 1: self._trace_tree(tree, frontier, 'M')

    def _trace_succeed(self, tree, frontier):
        if self._trace > 2: print('GOOD PARSE:')
        if self._trace == 1: print('Found a parse:\n%s' % tree)
        if self._trace > 1: self._trace_tree(tree, frontier, '+')

    def _trace_backtrack(self, tree, frontier, toks=None):
        if self._trace > 2:
            if toks: print('Backtrack: %r match failed' % toks[0])
            else: print('Backtrack')
--
        or not to match a token.
    :see: ``nltk.grammar``
    """
    def __init__(self, grammar, trace=0):
        self._grammar = grammar
        self._trace = trace
        self._rtext = None
--

    # [XX] TEMPORARY HACK WARNING!  This should be replaced with
    # something nicer when we get the chance.
    def _freeze(self, tree):
        c = tree.copy()
#        for pos in c.treepositions('leaves'):
#            c[pos] = c[pos].freeze()
        return ImmutableTree.convert(c)

    def nbest_parse(self, tokens, n=None):
        tokens = list(tokens)
        self.initialize(tokens)
        while self.step() is not None: pass

        return self.parses()[:n]

    def initialize(self, tokens):
        """
        Start parsing a given text.  This sets the parser's tree to
        the start symbol, its frontier to the root node, and its
--
        if self._trace:
            self._trace_start(self._tree, self._frontier, self._rtext)

    def remaining_text(self):
        """
        :return: The portion of the text that is not yet covered by the
            tree.
--
        """
        return self._rtext

    def frontier(self):
        """
        :return: A list of the tree locations of all subtrees that
            have not yet been expanded, and all leaves that have not
--
        """
        return self._frontier

    def tree(self):
        """
        :return: A partial structure for the text that is
            currently being parsed.  The elements specified by the
--
        """
        return self._tree

    def step(self):
        """
        Perform a single parsing operation.  If an untried match is
        possible, then perform the match, and return the matched
--
        # Nothing left to do.
        return None

    def expand(self, production=None):
        """
        Expand the first element of the frontier.  In particular, if
        the first element of the frontier is a subtree whose node type
--
        parses = []
        for prod in productions:
            # Record that we've tried this production now.
            self._tried_e.setdefault(self._freeze(self._tree), []).append(prod)

            # Try expanding.
            if self._expand(self._rtext, self._tree, self._frontier, prod):
--
        # We didn't expand anything.
        return None

    def match(self):
        """
        Match the first element of the frontier.  In particular, if
        the first element of the frontier has the same type as the
--

        # Record that we've tried matching this token.
        tok = self._rtext[0]
        self._tried_m.setdefault(self._freeze(self._tree), []).append(tok)

        # Make sure we *can* match.
        if len(self._frontier) == 0:
--
        else:
            return None

    def backtrack(self):
        """
        Return the parser to its state before the most recent
        match or expand operation.  Calling ``undo`` repeatedly return
--
        (self._rtext, self._tree, self._frontier) = self._history.pop()
        return 1

    def expandable_productions(self):
        """
        :return: A list of all the productions for which expansions
            are available for the current parser state.
--
        return [p for p in self._grammar.productions()
                if p.lhs().symbol() == frontier_child.node]

    def untried_expandable_productions(self):
        """
        :return: A list of all the untried productions for which
            expansions are available for the current parser state.
--
        return [p for p in self.expandable_productions()
                if p not in tried_expansions]

    def untried_match(self):
        """
        :return: Whether the first element of the frontier is a token
            that has not yet been matched.
--
        tried_matches = self._tried_m.get(self._freeze(self._tree), [])
        return (self._rtext[0] not in tried_matches)

    def currently_complete(self):
        """
        :return: Whether the parser's current state represents a
            complete parse.
--
        """
        return (len(self._frontier) == 0 and len(self._rtext) == 0)

    def _parse(self, remaining_text, tree, frontier):
        """
        A stub version of ``_parse`` that sets the parsers current
        state to the given arguments.  In ``RecursiveDescentParser``,
--

        return [1]

    def parses(self):
        """
        :return: A list of the parses that have been found by this
            parser so far.
--
        """
        return self._parses

    def set_grammar(self, grammar):
        """
        Change the grammar used to parse texts.

--
##  Demonstration Code
##//////////////////////////////////////////////////////

def demo():
    """
    A demonstration of the recursive descent parser.
    """
--
from nltk.parse.pchart import InsideChartParser
from nltk.parse.featurechart import FeatureChart, FeatureChartParser

def load_parser(grammar_url, trace=0,
                parser=None, chart_class=None,
                beam_size=0, **load_args):
    """
--

    :type grammar_url: str
    :param grammar_url: A URL specifying where the grammar is located.
        The default protocol is ``"nltk:"``, which searches for the file
        in the the NLTK data package.
    :type trace: int
    :param trace: The level of tracing that should be used when
--
    """
    Unit tests for  CFG.
    """
    def __init__(self, grammar, suite, accept=None, reject=None):
        self.test_grammar = grammar

        self.cp = load_parser(grammar, trace=0)
--
        self._reject = reject


    def run(self, show_trees=False):
        """
        Sentences in the test suite are divided into two classes:
         - grammatical (``accept``) and
--
            if accepted and rejected:
                print("All tests passed!")

def extract_test_sentences(string, comment_chars="#%;", encoding=None):
    """
    Parses a string with one test sentence per line.
    Lines can optionally begin with:
--
    :ivar _trace: The level of tracing output that should be generated
        when parsing a text.
    """
    def __init__(self, grammar, trace=0):
        """
        Create a new ``ViterbiParser`` parser, that uses ``grammar`` to
        parse texts.
--
        self._grammar = grammar
        self._trace = trace

    def grammar(self):
        return self._grammar

    def trace(self, trace=2):
        """
        Set the level of tracing output that should be generated when
        parsing a text.
--
        """
        self._trace = trace

    def parse(self, tokens):
        # Inherit docs from ParserI

        tokens = list(tokens)
--
        # Return the tree that spans the entire text & have the right cat
        return constituents.get((0, len(tokens), self._grammar.start()))

    def _add_constituents_spanning(self, span, constituents, tokens):
        """
        Find any constituents that might cover ``span``, and add them
        to the most likely constituents table.
--
                    constituents[span[0], span[1], production.lhs()] = tree
                    changed = True

    def _find_instantiations(self, span, constituents):
        """
        :return: a list of the production instantiations that cover a
            given span of the text.  A "production instantiation" is
--
                rv.append( (production, childlist) )
        return rv

    def _match_rhs(self, rhs, span, constituents):
        """
        :return: a set of all the lists of children that cover ``span``
            and that match ``rhs``.
--

        return childlists

    def _trace_production(self, production, p, span, width):
        """
        Print trace output indicating that a given production has been
        applied at a given location.
--

        print(str)

    def _trace_lexical_insertion(self, token, index, width):
        str = '   Insert: |' + '.' * index + '=' + '.' * (width-index-1) + '| '
        str += '%s' % (token,)
        print(str)

    def __repr__(self):
        return '<ViterbiParser for %r>' % self._grammar


--
##  Test Code
##//////////////////////////////////////////////////////

def demo():
    """
    A demonstration of the probabilistic parsers.  The user is
    prompted to select which demo to run, and how many parses should
--
    from nltk import tokenize
    from nltk.parse import ViterbiParser

    # Define two demos.  Each demo has a sentence and a grammar.
    demos = [('I saw the man with my telescope', nltk.toy_pcfg1),
             ('the boy saw Jack with Bob under the table with a telescope', nltk.toy_pcfg2)]

--
    fields.
    """

    def __init__(self):
        if self.__class__ == DependencyScorerI:
            raise TypeError('DependencyScorerI is an abstract interface')

    def train(self, graphs):
        """
        :type graphs: list(DependencyGraph)
        :param graphs: A list of dependency graphs to train the scorer.
--
        """
        raise NotImplementedError()

    def score(self, graph):
        """
        :type graph: DependencyGraph
        :param graph: A dependency graph whose set of edges need to be
--
    for classification.
    """

    def __init__(self):
        pass # Do nothing without throwing error

    def train(self, graphs):
        """
        Trains a ``NaiveBayesClassifier`` using the edges present in
        graphs list as positive examples, the edges not present as
--
        nltk.usage(nltk.ClassifierI)
        self.classifier = nltk.classify.NaiveBayesClassifier.train(labeled_examples)

    def score(self, graph):
        """
        Converts the graph into a feature-based representation of
        each edge, and then assigns a score to each based on the
--
# A short class necessary to show parsing example from paper
class DemoScorer:

    def train(self, graphs):
        print('Training...')

    def score(self, graph):
        # scores for Keith Hall 'K-best Spanning Tree Parsing' paper
        return [[[], [5],  [1],  [1]],
                [[], [],   [11], [4]],
--
    which likens the search for the best non-projective parse to
    finding the maximum spanning tree in a weighted directed graph.
    """
    def __init__(self):
        """
        Creates a new non-projective parser.
        """
        print('initializing prob. nonprojective...')

    def train(self, graphs, dependency_scorer):
        """
        Trains a ``DependencyScorerI`` from a set of ``DependencyGraph`` objects,
        and establishes this as the parser's scorer.  This is used to
--
        self._scorer = dependency_scorer
        self._scorer.train(graphs)

    def initialize_edge_scores(self, graph):
        """
        Assigns a score to every edge in the ``DependencyGraph`` graph.
        These scores are generated via the parser's scorer which
--
        """
        self.scores = self._scorer.score(graph)

    def collapse_nodes(self, new_node, cycle_path, g_graph, b_graph, c_graph):
        """
        Takes a list of nodes that have been identified to belong to a cycle,
        and collapses them into on larger node.  The arcs of all nodes in
--
        g_graph.nodelist.append(new_node)
        g_graph.redirect_arcs(cycle_path, new_node['address'])

    def update_edge_scores(self, new_node, cycle_path):
        """
        Updates the edge scores to reflect a collapse operation into
        new_node.
--
                    self.scores[i][j] = []
        print('After update:\n', self.scores)

    def compute_original_indexes(self, new_indexes):
        """
        As nodes are collapsed into others, they are replaced
        by the new node in the graph, but it's still necessary
--
            new_indexes = originals
        return new_indexes

    def compute_max_subtract_score(self, column_index, cycle_indexes):
        """
        When updating scores the score of the highest-weighted incoming
        arc is subtracted upon collapse.  This returns the correct
--
        return max_score


    def best_incoming_arc(self, node_index):
        """
        Returns the source of the best incoming arc to the
        node with address: node_index
--
                return key
        return max_arc

    def original_best_arc(self, node_index):
        """
        ???
        """
--
        return [max_arc, max_orig]


    def parse(self, tokens, tags):
        """
        Parses a list of tokens in accordance to the MST parsing algorithm
        for non-projective dependency parses.  Assumes that the tokens to
--
    """
    A non-projective, rule-based, dependency parser.  This parser
    will return the set of all possible non-projective parses based on
    the word-to-word relations defined in the parser's dependency
    grammar, and will allow the branches of the parse tree to cross
    in order to capture a variety of linguistic phenomena that a
    projective parser will not.
    """

    def __init__(self, dependency_grammar):
        """
        Creates a new ``NonprojectiveDependencyParser``.

--
	    """
        self._grammar = dependency_grammar

    def parse(self, tokens):
        """
        Parses the input tokens with respect to the parser's grammar.  Parsing
        is accomplished by representing the search-space of possible parses as
--
        """
        # Create graph representation of tokens
        self._graph = DependencyGraph()
        self._graph.nodelist = []  # Remove the default root
        for index, token in enumerate(tokens):
            self._graph.nodelist.append({'word':token, 'deps':[], 'rel':'NTOP', 'address':index})
        for head_node in self._graph.nodelist:
--
# Demos
#################################################################

def demo():
#   hall_demo()
    nonprojective_conll_parse_demo()
    rule_based_demo()


def hall_demo():
    npp = ProbabilisticNonprojectiveParser()
    npp.train([], DemoScorer())
    parse_graph = npp.parse(['v1', 'v2', 'v3'], [None, None, None])
    print(parse_graph)

def nonprojective_conll_parse_demo():
    graphs = [DependencyGraph(entry)
              for entry in conll_data2.split('\n\n') if entry]
    npp = ProbabilisticNonprojectiveParser()
--
    parse_graph = npp.parse(['Cathy', 'zag', 'hen', 'zwaaien', '.'], ['N', 'V', 'Pron', 'Adj', 'N', 'Punc'])
    print(parse_graph)

def rule_based_demo():
    grammar = parse_dependency_grammar("""
    'taught' -> 'play' | 'man'
    'man' -> 'the' | 'in'
--
"""
Classes and interfaces for associating probabilities with tree
structures that represent the internal organization of a text.  The
probabilistic parser module defines ``BottomUpProbabilisticChartParser``.

``BottomUpProbabilisticChartParser`` is an abstract class that implements
a bottom-up chart parser for ``PCFG`` grammars.  It maintains a queue of edges,
--
is based on the probabilities associated with the edges, allowing the
parser to expand more likely edges before less likely ones.  Each
subclass implements a different queue ordering, producing different
search strategies.  Currently the following subclasses are defined:

  - ``InsideChartParser`` searches edges in decreasing order of
    their trees' inside probabilities.
--

# Probabilistic edges
class ProbabilisticLeafEdge(LeafEdge):
    def prob(self): return 1.0

class ProbabilisticTreeEdge(TreeEdge):
    def __init__(self, prob, *args, **kwargs):
        TreeEdge.__init__(self, *args, **kwargs)
        self._prob = prob
        # two edges with different probabilities are not equal.
        self._comparison_key = (self._comparison_key, prob)

    def prob(self): return self._prob

    @staticmethod
    def from_production(production, index, p):
        return ProbabilisticTreeEdge(p, (index, index), production.lhs(),
                                     production.rhs(), 0)

# Rules using probabilistic edges
class ProbabilisticBottomUpInitRule(AbstractChartRule):
    NUM_EDGES=0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = ProbabilisticLeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
--

class ProbabilisticBottomUpPredictRule(AbstractChartRule):
    NUM_EDGES=1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions():
            if edge.lhs() == prod.rhs()[0]:
--

class ProbabilisticFundamentalRule(AbstractChartRule):
    NUM_EDGES=2
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.end() == right_edge.start() and
                left_edge.nextsym() == right_edge.lhs() and
--

    _fundamental_rule = ProbabilisticFundamentalRule()

    def apply_iter(self, chart, grammar, edge1):
        fr = self._fundamental_rule
        if edge1.is_incomplete():
            # edge1 = left_edge; edge2 = right_edge
--
                for new_edge in fr.apply_iter(chart, grammar, edge2, edge1):
                    yield new_edge

    def __str__(self):
        return 'Fundamental Rule'

class BottomUpProbabilisticChartParser(ParserI):
--
    The sorting order for the queue is not specified by
    ``BottomUpProbabilisticChartParser``.  Different sorting orders will
    result in different search strategies.  The sorting order for the
    queue is defined by the method ``sort_queue``; subclasses are required
    to provide a definition for this method.

    :type _grammar: PCFG
    :ivar _grammar: The grammar used to parse sentences.
--
    :ivar _trace: The level of tracing output that should be generated
        when parsing a text.
    """
    def __init__(self, grammar, beam_size=0, trace=0):
        """
        Create a new ``BottomUpProbabilisticChartParser``, that uses
        ``grammar`` to parse texts.
--
        self.beam_size = beam_size
        self._trace = trace

    def grammar(self):
        return self._grammar

    def trace(self, trace=2):
        """
        Set the level of tracing output that should be generated when
        parsing a text.
--
        self._trace = trace

    # TODO: change this to conform more with the standard ChartParser
    def nbest_parse(self, tokens, n=None):
        self._grammar.check_coverage(tokens)
        chart = Chart(list(tokens))
        grammar = self._grammar
--
            # Re-sort the queue.
            self.sort_queue(queue, chart)

            # Prune the queue to the correct size if a beam was defined
            if self.beam_size:
                self._prune(queue, chart)

--

        return parses[:n]

    def _setprob(self, tree, prod_probs):
        if tree.prob() is not None: return

        # Get the prob of the CFG production.
--

        tree.set_prob(prob)

    def sort_queue(self, queue, chart):
        """
        Sort the given queue of ``Edge`` objects, placing the edge that should
        be tried first at the beginning of the queue.  This method
--
        """
        raise NotImplementedError()

    def _prune(self, queue, chart):
        """ Discard items in the queue if the queue is longer than the beam."""
        if len(queue) > self.beam_size:
            split = len(queue)-self.beam_size
--
    strategy.
    """
    # Inherit constructor.
    def sort_queue(self, queue, chart):
        """
        Sort the given queue of edges, in descending order of the
        inside probabilities of the edges' trees.
--

# Eventually, this will become some sort of inside-outside parser:
# class InsideOutsideParser(BottomUpProbabilisticChartParser):
#     def __init__(self, grammar, trace=0):
#         # Inherit docs.
#         BottomUpProbabilisticChartParser.__init__(self, grammar, trace)
#
--
#         self._bestp = bestp
#         for (k,v) in self._bestp.items(): print k,v
#
#     def _sortkey(self, edge):
#         return edge.structure()[PROB] * self._bestp[edge.lhs()]
#
#     def sort_queue(self, queue, chart):
#         queue.sort(key=self._sortkey)

import random
--
    This sorting order results in a random search strategy.
    """
    # Inherit constructor
    def sort_queue(self, queue, chart):
        i = random.randint(0, len(queue)-1)
        (queue[-1], queue[i]) = (queue[i], queue[-1])

--
    A bottom-up parser for ``PCFG`` grammars that tries edges in whatever order.
    """
    # Inherit constructor
    def sort_queue(self, queue, chart): return

class LongestChartParser(BottomUpProbabilisticChartParser):
    """
--
    search strategy.
    """
    # Inherit constructor
    def sort_queue(self, queue, chart):
        queue.sort(key=lambda edge: edge.length())

##//////////////////////////////////////////////////////
##  Test Code
##//////////////////////////////////////////////////////

def demo(choice=None, draw_parses=None, print_parses=None):
    """
    A demonstration of the probabilistic parsers.  The user is
    prompted to select which demo to run, and how many parses should
--
    from nltk import tokenize, toy_pcfg1, toy_pcfg2
    from nltk.parse import pchart

    # Define two demos.  Each demo has a sentence and a grammar.
    demos = [('I saw John with my telescope', toy_pcfg1),
             ('the boy saw Jack with Bob under the table with a telescope',
              toy_pcfg2)]
--
    # Tokenize the sentence.
    tokens = sent.split()

    # Define a list of parsers.  We'll use all parsers.
    parsers = [
        pchart.InsideChartParser(grammar),
        pchart.RandomChartParser(grammar),
--
#////////////////////////////////////////////////////////////

class IncrementalChart(Chart):
    def initialize(self):
        # A sequence of edge lists contained in this chart.
        self._edgelists = tuple([] for x in self._positions())

--
        # (used by select()).
        self._indexes = {}

    def edges(self):
        return list(self.iteredges())

    def iteredges(self):
        return (edge for edgelist in self._edgelists for edge in edgelist)

    def select(self, end, **restrictions):
        edgelist = self._edgelists[end]

        # If there are no restrictions, then return all edges.
--
        vals = tuple(restrictions[key] for key in restr_keys)
        return iter(self._indexes[restr_keys][end].get(vals, []))

    def _add_index(self, restr_keys):
        # Make sure it's a valid index.
        for key in restr_keys:
            if not hasattr(EdgeI, key):
--
            this_index = index[end]
            for edge in edgelist:
                vals = tuple(getattr(edge, key)() for key in restr_keys)
                this_index.setdefault(vals, []).append(edge)

    def _register_with_indexes(self, edge):
        end = edge.end()
        for (restr_keys, index) in self._indexes.items():
            vals = tuple(getattr(edge, key)() for key in restr_keys)
            index[end].setdefault(vals, []).append(edge)

    def _append_edge(self, edge):
        self._edgelists[edge.end()].append(edge)

    def _positions(self):
        return xrange(self.num_leaves() + 1)


class FeatureIncrementalChart(IncrementalChart, FeatureChart):
    def select(self, end, **restrictions):
        edgelist = self._edgelists[end]

        # If there are no restrictions, then return all edges.
--
                     for key in restr_keys)
        return iter(self._indexes[restr_keys][end].get(vals, []))

    def _add_index(self, restr_keys):
        # Make sure it's a valid index.
        for key in restr_keys:
            if not hasattr(EdgeI, key):
--
            for edge in edgelist:
                vals = tuple(self._get_type_if_possible(getattr(edge, key)())
                             for key in restr_keys)
                this_index.setdefault(vals, []).append(edge)

    def _register_with_indexes(self, edge):
        end = edge.end()
        for (restr_keys, index) in self._indexes.items():
            vals = tuple(self._get_type_if_possible(getattr(edge, key)())
                         for key in restr_keys)
            index[end].setdefault(vals, []).append(edge)

#////////////////////////////////////////////////////////////
# Incremental CFG Rules
#////////////////////////////////////////////////////////////

class CompleteFundamentalRule(SingleEdgeFundamentalRule):
    def _apply_incomplete(self, chart, grammar, left_edge):
        end = left_edge.end()
        # When the chart is incremental, we only have to look for
        # empty complete edges here.
--

class CompleterRule(CompleteFundamentalRule):
    _fundamental_rule = CompleteFundamentalRule()
    def apply_iter(self, chart, grammar, edge):
        if not isinstance(edge, LeafEdge):
            for new_edge in self._fundamental_rule.apply_iter(chart, grammar, edge):
                yield new_edge

class ScannerRule(CompleteFundamentalRule):
    _fundamental_rule = CompleteFundamentalRule()
    def apply_iter(self, chart, grammar, edge):
        if isinstance(edge, LeafEdge):
            for new_edge in self._fundamental_rule.apply_iter(chart, grammar, edge):
                yield new_edge
--
    pass

class FilteredCompleteFundamentalRule(FilteredSingleEdgeFundamentalRule):
    def apply_iter(self, chart, grammar, edge):
        # Since the Filtered rule only works for grammars without empty productions,
        # we only have to bother with complete edges here.
        if edge.is_complete():
--
#////////////////////////////////////////////////////////////

class FeatureCompleteFundamentalRule(FeatureSingleEdgeFundamentalRule):
    def _apply_incomplete(self, chart, grammar, left_edge):
        fr = self._fundamental_rule
        end = left_edge.end()
        # When the chart is incremental, we only have to look for
--
    |       Apply CompleterRule to edge
    | Return any complete parses in the chart
    """
    def __init__(self, grammar, strategy=BU_LC_INCREMENTAL_STRATEGY,
                 trace=0, trace_chart_width=50,
                 chart_class=IncrementalChart):
        """
--
            and higher numbers will produce more verbose tracing
            output.
        :type trace_chart_width: int
        :param trace_chart_width: The default total width reserved for
            the chart in trace output.  The remainder of each line will
            be used to display edges.
        :param chart_class: The class that should be used to create
--
                raise ValueError("Incremental inference rules must have "
                                 "NUM_EDGES == 0 or 1")

    def chart_parse(self, tokens, trace=None):
        if trace is None: trace = self._trace
        trace_new_edges = self._trace_new_edges

--
        return chart

class EarleyChartParser(IncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        IncrementalChartParser.__init__(self, grammar, EARLEY_STRATEGY, **parser_args)
    pass

class IncrementalTopDownChartParser(IncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        IncrementalChartParser.__init__(self, grammar, TD_INCREMENTAL_STRATEGY, **parser_args)

class IncrementalBottomUpChartParser(IncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        IncrementalChartParser.__init__(self, grammar, BU_INCREMENTAL_STRATEGY, **parser_args)

class IncrementalBottomUpLeftCornerChartParser(IncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        IncrementalChartParser.__init__(self, grammar, BU_LC_INCREMENTAL_STRATEGY, **parser_args)

class IncrementalLeftCornerChartParser(IncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        if not grammar.is_nonempty():
            raise ValueError("IncrementalLeftCornerParser only works for grammars "
                             "without empty productions.")
--
                                      FeatureCompleteFundamentalRule()]

class FeatureIncrementalChartParser(IncrementalChartParser, FeatureChartParser):
    def __init__(self, grammar,
                 strategy=BU_LC_INCREMENTAL_FEATURE_STRATEGY,
                 trace_chart_width=20,
                 chart_class=FeatureIncrementalChart,
--
                                        **parser_args)

class FeatureEarleyChartParser(FeatureIncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureIncrementalChartParser.__init__(self, grammar, EARLEY_FEATURE_STRATEGY, **parser_args)

class FeatureIncrementalTopDownChartParser(FeatureIncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureIncrementalChartParser.__init__(self, grammar, TD_INCREMENTAL_FEATURE_STRATEGY, **parser_args)

class FeatureIncrementalBottomUpChartParser(FeatureIncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureIncrementalChartParser.__init__(self, grammar, BU_INCREMENTAL_FEATURE_STRATEGY, **parser_args)

class FeatureIncrementalBottomUpLeftCornerChartParser(FeatureIncrementalChartParser):
    def __init__(self, grammar, **parser_args):
        FeatureIncrementalChartParser.__init__(self, grammar, BU_LC_INCREMENTAL_FEATURE_STRATEGY, **parser_args)


--
# Demonstration
#////////////////////////////////////////////////////////////

def demo(should_print_times=True, should_print_grammar=False,
         should_print_trees=True, trace=2,
         sent='I saw John with a dog with my cookie', numparses=5):
    """
--

Charts are encoded with the ``Chart`` class, and edges are encoded with
the ``TreeEdge`` and ``LeafEdge`` classes.  The chart parser module
defines three chart parsers:

  - ``ChartParser`` is a simple and flexible chart parser.  Given a
    set of chart rules, it will apply those rules to the chart until
--
    The ``EdgeI`` interface provides a common interface to both types
    of edge, allowing chart parsers to treat them in a uniform manner.
    """
    def __init__(self):
        if self.__class__ == EdgeI:
            raise TypeError('Edge is an abstract interface')

--
    # Span
    #////////////////////////////////////////////////////////////

    def span(self):
        """
        Return a tuple ``(s, e)``, where ``tokens[s:e]`` is the
        portion of the sentence that is consistent with this
--
        """
        raise NotImplementedError()

    def start(self):
        """
        Return the start index of this edge's span.

--
        """
        raise NotImplementedError()

    def end(self):
        """
        Return the end index of this edge's span.

--
        """
        raise NotImplementedError()

    def length(self):
        """
        Return the length of this edge's span.

--
    # Left Hand Side
    #////////////////////////////////////////////////////////////

    def lhs(self):
        """
        Return this edge's left-hand side, which specifies what kind
        of structure is hypothesized by this edge.
--
    # Right Hand Side
    #////////////////////////////////////////////////////////////

    def rhs(self):
        """
        Return this edge's right-hand side, which specifies
        the content of the structure hypothesized by this edge.
--
        """
        raise NotImplementedError()

    def dot(self):
        """
        Return this edge's dot position, which indicates how much of
        the hypothesized structure is consistent with the
--
        """
        raise NotImplementedError()

    def nextsym(self):
        """
        Return the element of this edge's right-hand side that
        immediately follows its dot.
--
        """
        raise NotImplementedError()

    def is_complete(self):
        """
        Return True if this edge's structure is fully consistent
        with the text.
--
        """
        raise NotImplementedError()

    def is_incomplete(self):
        """
        Return True if this edge's structure is partially consistent
        with the text.
--
    # Comparisons & hashing
    #////////////////////////////////////////////////////////////

    def __eq__(self, other):
        return (self.__class__ is other.__class__ and
                self._comparison_key == other._comparison_key)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, EdgeI):
            raise_unorderable_types("<", self, other)
        if self.__class__ is other.__class__:
--
        else:
            return self.__class__.__name__ < other.__class__.__name__

    def __hash__(self):
        try:
            return self._hash
        except AttributeError:
--

    For more information about edges, see the ``EdgeI`` interface.
    """
    def __init__(self, span, lhs, rhs, dot=0):
        """
        Construct a new ``TreeEdge``.

--
        self._comparison_key = (span, lhs, rhs, dot)

    @staticmethod
    def from_production(production, index):
        """
        Return a new ``TreeEdge`` formed from the given production.
        The new edge's left-hand side and right-hand side will
--
        return TreeEdge(span=(index, index), lhs=production.lhs(),
                        rhs=production.rhs(), dot=0)

    def move_dot_forward(self, new_end):
        """
        Return a new ``TreeEdge`` formed from this edge.
        The new edge's dot position is increased by ``1``,
--
                        dot=self._dot+1)

    # Accessors
    def lhs(self): return self._lhs
    def span(self): return self._span
    def start(self): return self._span[0]
    def end(self): return self._span[1]
    def length(self): return self._span[1] - self._span[0]
    def rhs(self): return self._rhs
    def dot(self): return self._dot
    def is_complete(self): return self._dot == len(self._rhs)
    def is_incomplete(self): return self._dot != len(self._rhs)
    def nextsym(self):
        if self._dot >= len(self._rhs): return None
        else: return self._rhs[self._dot]

    # String representation
    def __str__(self):
        str = '[%s:%s] ' % (self._span[0], self._span[1])
        str += '%-2r ->' % (self._lhs,)

--
        if len(self._rhs) == self._dot: str += ' *'
        return str

    def __repr__(self):
        return '[Edge: %s]' % self


--
    side is ``()``.  Its span is ``[index, index+1]``, and its dot
    position is ``0``.
    """
    def __init__(self, leaf, index):
        """
        Construct a new ``LeafEdge``.

--
        self._comparison_key = (leaf, index)

    # Accessors
    def lhs(self): return self._leaf
    def span(self): return (self._index, self._index+1)
    def start(self): return self._index
    def end(self): return self._index+1
    def length(self): return 1
    def rhs(self): return ()
    def dot(self): return 0
    def is_complete(self): return True
    def is_incomplete(self): return False
    def nextsym(self): return None

    # String representations
    def __str__(self):
        return '[%s:%s] %s' % (self._index, self._index+1, unicode_repr(self._leaf))
    def __repr__(self):
        return '[Edge: %s]' % (self)

########################################################################
--
        to indices, where each index maps the corresponding edge
        attribute values to lists of edges.
    """
    def __init__(self, tokens):
        """
        Construct a new chart. The chart is initialized with the
        leaf edges corresponding to the terminal leaves.
--
        # Initialise the chart.
        self.initialize()

    def initialize(self):
        """
        Clear the chart.
        """
--
    # Sentence Access
    #////////////////////////////////////////////////////////////

    def num_leaves(self):
        """
        Return the number of words in this chart's sentence.

--
        """
        return self._num_leaves

    def leaf(self, index):
        """
        Return the leaf value of the word at the given index.

--
        """
        return self._tokens[index]

    def leaves(self):
        """
        Return a list of the leaf values of each word in the
        chart's sentence.
--
    # Edge access
    #////////////////////////////////////////////////////////////

    def edges(self):
        """
        Return a list of all edges in this chart.  New edges
        that are added to the chart after the call to edges()
--
        """
        return self._edges[:]

    def iteredges(self):
        """
        Return an iterator over the edges in this chart.  It is
        not guaranteed that new edges which are added to the
--
    # Iterating over the chart yields its edges.
    __iter__ = iteredges

    def num_edges(self):
        """
        Return the number of edges contained in this chart.

--
        """
        return len(self._edge_to_cpls)

    def select(self, **restrictions):
        """
        Return an iterator over the edges in this chart.  Any
        new edges that are added to the chart before the iterator
--
        vals = tuple(restrictions[key] for key in restr_keys)
        return iter(self._indexes[restr_keys].get(vals, []))

    def _add_index(self, restr_keys):
        """
        A helper function for ``select``, which creates a new index for
        a given set of attributes (aka restriction keys).
--
        # Add all existing edges to the index.
        for edge in self._edges:
            vals = tuple(getattr(edge, key)() for key in restr_keys)
            index.setdefault(vals, []).append(edge)

    def _register_with_indexes(self, edge):
        """
        A helper function for ``insert``, which registers the new
        edge with all existing indexes.
        """
        for (restr_keys, index) in self._indexes.items():
            vals = tuple(getattr(edge, key)() for key in restr_keys)
            index.setdefault(vals, []).append(edge)

    #////////////////////////////////////////////////////////////
    # Edge Insertion
    #////////////////////////////////////////////////////////////

    def insert_with_backpointer(self, new_edge, previous_edge, child_edge):
        """
        Add a new edge to the chart, using a pointer to the previous edge.
        """
--
        new_cpls = [cpl+(child_edge,) for cpl in cpls]
        return self.insert(new_edge, *new_cpls)

    def insert(self, edge, *child_pointer_lists):
        """
        Add a new edge to the chart, and return True if this operation
        modified the chart.  In particular, return true iff the chart
--
            self._register_with_indexes(edge)

        # Get the set of child pointer lists for this edge.
        cpls = self._edge_to_cpls.setdefault(edge, OrderedDict())
        chart_was_modified = False
        for child_pointer_list in child_pointer_lists:
            child_pointer_list = tuple(child_pointer_list)
--
                chart_was_modified = True
        return chart_was_modified

    def _append_edge(self, edge):
        self._edges.append(edge)

    #////////////////////////////////////////////////////////////
    # Tree extraction & child pointer lists
    #////////////////////////////////////////////////////////////

    def parses(self, root, tree_class=Tree):
        """
        Return a list of the complete tree structures that span
        the entire chart, and whose root node is ``root``.
--
            trees += self.trees(edge, tree_class=tree_class, complete=True)
        return trees

    def trees(self, edge, tree_class=Tree, complete=False):
        """
        Return a list of the tree structures that are associated
        with ``edge``.
--
        """
        return self._trees(edge, complete, memo={}, tree_class=tree_class)

    def _trees(self, edge, complete, memo, tree_class):
        """
        A helper function for ``trees``.

--
        # Return the list of trees.
        return trees

    def _choose_children(self, child_choices):
        """
        A helper function for ``_trees`` that finds the possible sets
        of subtrees for a new tree.
--
                                  for child_list in children_lists]
        return children_lists

    def child_pointer_lists(self, edge):
        """
        Return the set of child pointer lists for the given edge.
        Each child pointer list is a list of edges that have
--
    #////////////////////////////////////////////////////////////
    # Display
    #////////////////////////////////////////////////////////////
    def pp_edge(self, edge, width=None):
        """
        Return a pretty-printed string representation of a given edge
        in this chart.
--
        str += (' '*(width-1)+'.')*(self._num_leaves-end)
        return str + '| %s' % edge

    def pp_leaves(self, width=None):
        """
        Return a pretty-printed string representation of this
        chart's leaves.  This string can be used as a header
--

        return header

    def pp(self, width=None):
        """
        Return a pretty-printed string representation of this chart.

--
    # Display: Dot (AT&T Graphviz)
    #////////////////////////////////////////////////////////////

    def dot_digraph(self):
        # Header
        s = 'digraph nltk_chart {\n'
        #s += '  size="5,5";\n'
--
        to license new edges.  Typically, this number ranges from zero
        to two.
    """
    def apply(self, chart, grammar, *edges):
        """
        Add the edges licensed by this rule and the given edges to the
        chart.  Return a list of the edges that were added.
--
        """
        raise NotImplementedError()

    def apply_iter(self, chart, grammar, *edges):
        """
        Return a generator that will add edges licensed by this rule
        and the given edges to the chart, one at a time.  Each
--
        """
        raise NotImplementedError()

    def apply_everywhere(self, chart, grammar):
        """
        Add all the edges licensed by this rule and the edges in the
        chart to the chart.  Return a list of the edges that were added.
--
        """
        raise NotImplementedError()

    def apply_everywhere_iter(self, chart, grammar):
        """
        Return a generator that will add all edges licensed by
        this rule, given the edges that are currently in the
--
    An abstract base class for chart rules.  ``AbstractChartRule``
    provides:

    - A default implementation for ``apply``, based on ``apply_iter``.
    - A default implementation for ``apply_everywhere_iter``,
      based on ``apply_iter``.
    - A default implementation for ``apply_everywhere``, based on
      ``apply_everywhere_iter``.  Currently, this implementation
      assumes that ``NUM_EDGES``<=3.
    - A default implementation for ``__str__``, which returns a
      name basd on the rule's class name.
    """

    # Subclasses must define apply_iter.
    def apply_iter(self, chart, grammar, *edges):
        raise NotImplementedError()

    # Default: loop through the given number of edges, and call
    # self.apply() for each set of edges.
    def apply_everywhere_iter(self, chart, grammar):
        if self.NUM_EDGES == 0:
            for new_edge in self.apply_iter(chart, grammar):
                yield new_edge
--
        else:
            raise AssertionError('NUM_EDGES>3 is not currently supported')

    # Default: delegate to apply_iter.
    def apply(self, chart, grammar, *edges):
        return list(self.apply_iter(chart, grammar, *edges))

    # Default: delegate to apply_everywhere_iter.
    def apply_everywhere(self, chart, grammar):
        return list(self.apply_everywhere_iter(chart, grammar))

    # Default: return a name based on the class name.
    def __str__(self):
        # Add spaces between InitialCapsWords.
        return re.sub('([a-z])([A-Z])', r'\1 \2', self.__class__.__name__)

--
    - ``[A -> alpha B * beta][i:j]``
    """
    NUM_EDGES = 2
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # Make sure the rule is applicable.
        if not (left_edge.is_incomplete() and
                right_edge.is_complete() and
--
    """
    NUM_EDGES = 1

    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete():
            for new_edge in self._apply_incomplete(chart, grammar, edge):
                yield new_edge
--
            for new_edge in self._apply_complete(chart, grammar, edge):
                yield new_edge

    def _apply_complete(self, chart, grammar, right_edge):
        for left_edge in chart.select(end=right_edge.start(),
                                      is_complete=False,
                                      nextsym=right_edge.lhs()):
--
            if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
                yield new_edge

    def _apply_incomplete(self, chart, grammar, left_edge):
        for right_edge in chart.select(start=left_edge.end(),
                                       is_complete=True,
                                       lhs=left_edge.nextsym()):
--

class LeafInitRule(AbstractChartRule):
    NUM_EDGES=0
    def apply_iter(self, chart, grammar):
        for index in range(chart.num_leaves()):
            new_edge = LeafEdge(chart.leaf(index), index)
            if chart.insert(new_edge, ()):
--
    ``S -> alpha``, where ``S`` is the grammar's start symbol.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(lhs=grammar.start()):
            new_edge = TreeEdge.from_production(prod, 0)
            if chart.insert(new_edge, ()):
--
    :note: This rule corresponds to the Predictor Rule in Earley parsing.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        for prod in grammar.productions(lhs=edge.nextsym()):
            new_edge = TreeEdge.from_production(prod, edge.end())
--

    If ``chart`` or ``grammar`` are changed, then the cache is flushed.
    """
    def __init__(self):
        TopDownPredictRule.__init__(self)
        self._done = {}

    def apply_iter(self, chart, grammar, edge):
        if edge.is_complete(): return
        nextsym, index = edge.nextsym(), edge.end()
        if not is_nonterminal(nextsym): return
--
    the edge ``[B -> \* A beta]`` for each grammar production ``B -> A beta``.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions(rhs=edge.lhs()):
            new_edge = TreeEdge.from_production(prod, edge.start())
--
        the ``FundamentalRule`` to the resulting edge.
    """
    NUM_EDGES = 1
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete(): return
        for prod in grammar.productions(rhs=edge.lhs()):
            new_edge = TreeEdge(edge.span(), prod.lhs(), prod.rhs(), 1)
--
    in every position in the chart.
    """
    NUM_EDGES = 0
    def apply_iter(self, chart, grammar):
        for prod in grammar.productions(empty=True):
            for index in compat.xrange(chart.num_leaves() + 1):
                new_edge = TreeEdge.from_production(prod, index)
--
########################################################################

class FilteredSingleEdgeFundamentalRule(SingleEdgeFundamentalRule):
    def _apply_complete(self, chart, grammar, right_edge):
        end = right_edge.end()
        nexttoken = end < chart.num_leaves() and chart.leaf(end)
        for left_edge in chart.select(end=right_edge.start(),
--
                if chart.insert_with_backpointer(new_edge, left_edge, right_edge):
                    yield new_edge

    def _apply_incomplete(self, chart, grammar, left_edge):
        for right_edge in chart.select(start=left_edge.end(),
                                       is_complete=True,
                                       lhs=left_edge.nextsym()):
--
                    yield new_edge

class FilteredBottomUpPredictCombineRule(BottomUpPredictCombineRule):
    def apply_iter(self, chart, grammar, edge):
        if edge.is_incomplete():
            return

--
                if chart.insert(new_edge, (edge,)):
                    yield new_edge

def _bottomup_filter(grammar, nexttoken, rhs, dot=0):
    if len(rhs) <= dot + 1:
        return True
    _next = rhs[dot + 1]
--
    |     Apply *rule* to any applicable edges in the chart.
    | Return any complete parses in the chart
    """
    def __init__(self, grammar, strategy=BU_LC_STRATEGY, trace=0,
                 trace_chart_width=50, use_agenda=True, chart_class=Chart):
        """
        Create a new chart parser, that uses ``grammar`` to parse
--
        :param grammar: The grammar used to parse texts.
        :type strategy: list(ChartRuleI)
        :param strategy: A list of rules that should be used to decide
            what edges to add to the chart (top-down strategy by default).
        :type trace: int
        :param trace: The level of tracing that should be used when
            parsing a text.  ``0`` will generate no tracing output;
            and higher numbers will produce more verbose tracing
            output.
        :type trace_chart_width: int
        :param trace_chart_width: The default total width reserved for
            the chart in trace output.  The remainder of each line will
            be used to display edges.
        :type use_agenda: bool
--
            else:
                self._use_agenda = False

    def grammar(self):
        return self._grammar

    def _trace_new_edges(self, chart, rule, new_edges, trace, edge_width):
        if not trace: return
        should_print_rule_header = trace > 1
        for edge in new_edges:
--
                should_print_rule_header = False
            print(chart.pp_edge(edge, edge_width))

    def chart_parse(self, tokens, trace=None):
        """
        Return the final parse ``Chart`` from which all possible
        parse trees can be extracted.
--
        # Return the final chart.
        return chart

    def nbest_parse(self, tokens, n=None, tree_class=Tree):
        chart = self.chart_parse(tokens)
        # Return a list of complete parses.
        return chart.parses(self._grammar.start(), tree_class=tree_class)[:n]
--
    A ``ChartParser`` using a top-down parsing strategy.
    See ``ChartParser`` for more information.
    """
    def __init__(self, grammar, **parser_args):
        ChartParser.__init__(self, grammar, TD_STRATEGY, **parser_args)

class BottomUpChartParser(ChartParser):
--
    A ``ChartParser`` using a bottom-up parsing strategy.
    See ``ChartParser`` for more information.
    """
    def __init__(self, grammar, **parser_args):
        if isinstance(grammar, WeightedGrammar):
            warnings.warn("BottomUpChartParser only works for ContextFreeGrammar, "
                          "use BottomUpProbabilisticChartParser instead",
--
    This strategy is often more efficient than standard bottom-up.
    See ``ChartParser`` for more information.
    """
    def __init__(self, grammar, **parser_args):
        ChartParser.__init__(self, grammar, BU_LC_STRATEGY, **parser_args)

class LeftCornerChartParser(ChartParser):
    def __init__(self, grammar, **parser_args):
        if not grammar.is_nonempty():
            raise ValueError("LeftCornerParser only works for grammars "
                             "without empty productions.")
--
        or chart has been changed.  If so, then ``step`` must restart
        the parsing algorithm.
    """
    def __init__(self, grammar, strategy=[], trace=0):
        self._chart = None
        self._current_chartrule = None
        self._restart = False
--
    # Initialization
    #////////////////////////////////////////////////////////////

    def initialize(self, tokens):
        "Begin parsing the given tokens."
        self._chart = Chart(list(tokens))
        self._restart = True
--
    # Stepping
    #////////////////////////////////////////////////////////////

    def step(self):
        """
        Return a generator that adds edges to the chart, one at a
        time.  Each time the generator is resumed, it adds a single
--
            else:
                yield None # No more edges.

    def _parse(self):
        """
        A generator that implements the actual parsing algorithm.
        ``step`` iterates through this generator, and restarts it
--
    # Accessors
    #////////////////////////////////////////////////////////////

    def strategy(self):
        "Return the strategy used by this parser."
        return self._strategy

    def grammar(self):
        "Return the grammar used by this parser."
        return self._grammar

    def chart(self):
        "Return the chart that is used by this parser."
        return self._chart

    def current_chartrule(self):
        "Return the chart rule used to generate the most recent edge."
        return self._current_chartrule

    def parses(self, tree_class=Tree):
        "Return the parse trees currently contained in the chart."
        return self._chart.parses(self._grammar.start(), tree_class)

--
    # Parser modification
    #////////////////////////////////////////////////////////////

    def set_strategy(self, strategy):
        """
        Change the strategy that the parser uses to decide which edges
        to add to the chart.
--
        self._strategy = strategy[:] # Make a copy.
        self._restart = True

    def set_grammar(self, grammar):
        "Change the grammar used by the parser."
        if grammar is self._grammar: return
        self._grammar = grammar
        self._restart = True

    def set_chart(self, chart):
        "Load a given chart into the chart parser."
        if chart is self._chart: return
        self._chart = chart
--
    # Standard parser methods
    #////////////////////////////////////////////////////////////

    def nbest_parse(self, tokens, n=None, tree_class=Tree):
        tokens = list(tokens)
        self._grammar.check_coverage(tokens)

--
##  Demo Code
########################################################################

def demo_grammar():
    from nltk.grammar import parse_cfg
    return parse_cfg("""
S  -> NP VP
--
Prep -> "under"
""")

def demo(choice=None,
         should_print_times=True, should_print_grammar=False,
         should_print_trees=True, trace=2,
         sent='I saw John with a dog with my cookie', numparses=5):
--
    kinds of tree structure, such as morphological trees and discourse
    structures.

    Subclasses must define:
      - at least one of: ``parse()``, ``nbest_parse()``, ``iter_parse()``,
        ``batch_parse()``, ``batch_nbest_parse()``, ``batch_iter_parse()``.

    Subclasses may define:
      - ``grammar()``
      - either ``prob_parse()`` or ``batch_prob_parse()`` (or both)
    """
    def grammar(self):
        """
        :return: The grammar used by this parser.
        """
        raise NotImplementedError()

    def parse(self, sent):
        """
        :return: A parse tree that represents the structure of the
        given sentence, or None if no parse tree is found.  If
--
            if trees: return trees[0]
            else: return None

    def nbest_parse(self, sent, n=None):
        """
        :return: A list of parse trees that represent possible
        structures for the given sentence.  When possible, this list is
--
        else:
            return list(itertools.islice(self.iter_parse(sent), n))

    def iter_parse(self, sent):
        """
        :return: An iterator that generates parse trees that represent
        possible structures for the given sentence.  When possible,
--
        else:
            raise NotImplementedError()

    def prob_parse(self, sent):
        """
        :return: A probability distribution over the possible parse
        trees for the given sentence.  If there are no possible parse
--
        else:
            raise NotImplementedError

    def batch_parse(self, sents):
        """
        Apply ``self.parse()`` to each element of ``sents``.  I.e.:

--
        """
        return [self.parse(sent) for sent in sents]

    def batch_nbest_parse(self, sents, n=None):
        """
        Apply ``self.nbest_parse()`` to each element of ``sents``.  I.e.:

--
        """
        return [self.nbest_parse(sent,n ) for sent in sents]

    def batch_iter_parse(self, sents):
        """
        Apply ``self.iter_parse()`` to each element of ``sents``.  I.e.:

--
        """
        return [self.iter_parse(sent) for sent in sents]

    def batch_prob_parse(self, sents):
        """
        Apply ``self.prob_parse()`` to each element of ``sents``.  I.e.:

--
# For license information, see LICENSE.TXT

"""
The NLTK corpus and module downloader.  This module defines several
interfaces which can be used to download corpora, models, and other
data packages that can be used with NLTK.

--

Download Directory
==================
By default, packages are installed in either a system-wide directory
(if Python has sufficient access to write to it); or in the current
user's home directory.  However, the ``download_dir`` argument may be
used to specify a different installation target, if desired.

See ``Downloader.default_download_dir()`` for more a detailed
description of how the default download directory is chosen.

NLTK Download Server
====================
Before downloading any packages, the corpus and module downloader
contacts the NLTK download server, to retrieve an index file
describing the available packages.  By default, this index file is
loaded from ``http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml``.
If necessary, it is possible to create a new ``Downloader`` object,
specifying a different URL for the package index file.
--
  - in index: add "unzipped-size"
  - when checking status: check both compressed & uncompressed size.
    uncompressed size is important to make sure we detect a problem
    if something got partially unzipped.  define new status values
    to differentiate stale vs corrupt vs corruptly-uncompressed??
    (we shouldn't need to re-download the file if the zip file is ok
    but it didn't get uncompressed fully.)
--
n.b.: there will have to be a fallback to the punkt tokenizer, in case
they didn't download that model.

default: unzip or not?

"""
import time, os, zipfile, sys, textwrap, threading, itertools
--
    that file is a zip file, then it can be automatically decompressed
    when the package is installed.
    """
    def __init__(self, id, url, name=None, subdir='',
                 size=None, unzipped_size=None,
                 checksum=None, svn_revision=None,
                 copyright='Unknown', contact='Unknown',
--

        self.unzip = bool(int(unzip)) # '0' or '1'
        """A flag indicating whether this corpus should be unzipped by
           default."""

        # Include any other attributes provided by the XML file.
        self.__dict__.update(kw)

    @staticmethod
    def fromxml(xml):
        if isinstance(xml, compat.string_types):
            xml = ElementTree.parse(xml)
        for key in xml.attrib:
            xml.attrib[key] = compat.text_type(xml.attrib[key])
        return Package(**xml.attrib)

    def __lt__(self, other):
        return self.id < other.id

    def __repr__(self):
        return '<Package %s>' % self.id

@compat.python_2_unicode_compatible
--
    These entries are extracted from the XML index file that is
    downloaded by ``Downloader``.
    """
    def __init__(self, id, children, name=None, **kw):
        self.id = id
        """A unique identifier for this collection."""

--
        self.__dict__.update(kw)

    @staticmethod
    def fromxml(xml):
        if isinstance(xml, compat.string_types):
            xml = ElementTree.parse(xml)
        for key in xml.attrib:
--
        children = [child.get('ref') for child in xml.findall('item')]
        return Collection(children=children, **xml.attrib)

    def __lt__(self, other):
        return self.id < other.id

    def __repr__(self):
        return '<Collection %s>' % self.id

######################################################################
--
       communicate its progress."""
class StartCollectionMessage(DownloaderMessage):
    """Data server has started working on a collection of packages."""
    def __init__(self, collection): self.collection = collection
class FinishCollectionMessage(DownloaderMessage):
    """Data server has finished working on a collection of packages."""
    def __init__(self, collection): self.collection = collection
class StartPackageMessage(DownloaderMessage):
    """Data server has started working on a package."""
    def __init__(self, package): self.package = package
class FinishPackageMessage(DownloaderMessage):
    """Data server has finished working on a package."""
    def __init__(self, package): self.package = package
class StartDownloadMessage(DownloaderMessage):
    """Data server has started downloading a package."""
    def __init__(self, package): self.package = package
class FinishDownloadMessage(DownloaderMessage):
    """Data server has finished downloading a package."""
    def __init__(self, package): self.package = package
class StartUnzipMessage(DownloaderMessage):
    """Data server has started unzipping a package."""
    def __init__(self, package): self.package = package
class FinishUnzipMessage(DownloaderMessage):
    """Data server has finished unzipping a package."""
    def __init__(self, package): self.package = package
class UpToDateMessage(DownloaderMessage):
    """The package download file is already up-to-date"""
    def __init__(self, package): self.package = package
class StaleMessage(DownloaderMessage):
    """The package download file is out-of-date or corrupt"""
    def __init__(self, package): self.package = package
class ErrorMessage(DownloaderMessage):
    """Data server encountered an error"""
    def __init__(self, package, message):
        self.package = package
        if isinstance(message, Exception):
            self.message = str(message)
--

class ProgressMessage(DownloaderMessage):
    """Indicates how much progress the data server has made"""
    def __init__(self, progress): self.progress = progress
class SelectDownloadDirMessage(DownloaderMessage):
    """Indicates what download directory the data server is using"""
    def __init__(self, download_dir): self.download_dir = download_dir

######################################################################
# NLTK Data Server
--
       server index will be considered 'stale,' and will be
       re-downloaded."""

    # DEFAULT_URL = 'http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml'
    DEFAULT_URL = 'http://nltk.github.com/nltk_data/'
    """The default URL for the NLTK data server's index.  An
       alternative URL can be specified when creating a new
       ``Downloader`` object."""

--
    # Cosntructor
    #/////////////////////////////////////////////////////////////////

    def __init__(self, server_index_url=None, download_dir=None):
        self._url = server_index_url or self.DEFAULT_URL
        """The URL for the data server's index file."""

        self._collections = {}
--
        """Dictionary from package identifier to ``Package``"""

        self._download_dir = download_dir
        """The default directory to which packages will be downloaded."""

        self._index = None
        """The XML index file downloaded from the data server"""
--

        # decide where we're going to save things to.
        if self._download_dir is None:
            self._download_dir = self.default_download_dir()

    #/////////////////////////////////////////////////////////////////
    # Information
    #/////////////////////////////////////////////////////////////////

    def list(self, download_dir=None, show_packages=True,
             show_collections=True, header=True, more_prompt=False,
             skip_installed=False):
        lines = 0 # for more_prompt
        if download_dir is None:
            download_dir = self._download_dir
            print('Using default data directory (%s)' % download_dir)
        if header:
            print('='*(26+len(self._url)))
            print(' Data server index for <%s>' % self._url)
--
        if partial: msg += '; [P] marks partially installed collections'
        print(textwrap.fill(msg+')', subsequent_indent=' ', width=76))

    def packages(self):
        self._update_index()
        return self._packages.values()

    def corpora(self):
        self._update_index()
        return [pkg for (id,pkg) in self._packages.items()
                if pkg.subdir == 'corpora']

    def models(self):
        self._update_index()
        return [pkg for (id,pkg) in self._packages.items()
                if pkg.subdir != 'corpora']

    def collections(self):
        self._update_index()
        return self._collections.values()

--
    # Downloading
    #/////////////////////////////////////////////////////////////////

    def _info_or_id(self, info_or_id):
        if isinstance(info_or_id, compat.string_types):
            return self.info(info_or_id)
        else:
--
    # downloader in the gui can just kill the download thread anytime
    # it wants.

    def incr_download(self, info_or_id, download_dir=None, force=False):
        # If they didn't specify a download_dir, then use the default one.
        if download_dir is None:
            download_dir = self._download_dir
            yield SelectDownloadDirMessage(download_dir)
--
            for msg in self._download_package(info, download_dir, force):
                yield msg

    def _num_packages(self, item):
        if isinstance(item, Package): return 1
        else: return len(item.packages)

    def _download_list(self, items, download_dir, force):
        # Look up the requested items.
        for i in range(len(items)):
            try: items[i] = self._info_or_id(items[i])
--

            progress += 100*delta

    def _download_package(self, info, download_dir, force):
        yield StartPackageMessage(info)
        yield ProgressMessage(0)

--
        # If it's a zipfile, uncompress it.
        if info.filename.endswith('.zip'):
            zipdir = os.path.join(download_dir, info.subdir)
            # Unzip if we're unzipping by default; *or* if it's already
            # been unzipped (presumably a previous version).
            if info.unzip or os.path.exists(os.path.join(zipdir, info.id)):
                yield StartUnzipMessage(info)
--

        yield FinishPackageMessage(info)

    def download(self, info_or_id=None, download_dir=None, quiet=False,
                 force=False, prefix='[nltk_data] ', halt_on_error=True,
                 raise_on_error=False):
        # If no info or id is given, then use the interactive shell.
--
            return True

        else:
            # Define a helper function for displaying output:
            def show(s, prefix2=''):
                print(textwrap.fill(s, initial_indent=prefix+prefix2,
                                    subsequent_indent=prefix+prefix2+' '*4))

--
                        download_dir = msg.download_dir
        return True

    def is_stale(self, info_or_id, download_dir=None):
        return self.status(info_or_id, download_dir) == self.STALE

    def is_installed(self, info_or_id, download_dir=None):
        return self.status(info_or_id, download_dir) == self.INSTALLED

    def clear_status_cache(self, id=None):
        if id is None:
            self._status_cache.clear()
        else:
            self._status_cache.pop(id, None)

    def status(self, info_or_id, download_dir=None):
        """
        Return a constant describing the status of the given package
        or collection.  Status can be one of ``INSTALLED``,
--
                                                                   filepath)
                return self._status_cache[info.id]

    def _pkg_status(self, info, filepath):
        if not os.path.exists(filepath):
            return self.NOT_INSTALLED

--
        # Otherwise, everything looks good.
        return self.INSTALLED

    def update(self, quiet=False, prefix='[nltk_data] '):
        """
        Re-download any packages whose status is STALE.
        """
--
    # Index
    #/////////////////////////////////////////////////////////////////

    def _update_index(self, url=None):
        """A helper function that ensures that self._index is
        up-to-date.  If the index is older than self.INDEX_TIMEOUT,
        then download it again."""
--
        # Flush the status cache
        self._status_cache.clear()

    def index(self):
        """
        Return the XML index describing the packages available from
        the data server.  If necessary, this index will be downloaded
--
        self._update_index()
        return self._index

    def info(self, id):
        """Return the ``Package`` or ``Collection`` record for the
           given item."""
        self._update_index()
--
        if id in self._collections: return self._collections[id]
        raise ValueError('Package %r not found in index' % id)

    def xmlinfo(self, id):
        """Return the XML info record for the given item"""
        self._update_index()
        for package in self._index.findall('packages/package'):
--
    # URL & Data Directory
    #/////////////////////////////////////////////////////////////////

    def _get_url(self):
        """The URL for the data server's index file."""
        return self._url
    def _set_url(self, url):
        """
        Set a new URL for the data server. If we're unable to contact
        the given url, then the original url is kept.
--
            raise
    url = property(_get_url, _set_url)

    def default_download_dir(self):
        """
        Return the directory to which packages will be downloaded by
        default.  This value can be overridden using the constructor,
        or on a case-by-case basis using the ``download_dir`` argument when
        calling ``download()``.

        On Windows, the default download directory is
        ``PYTHONHOME/lib/nltk``, where *PYTHONHOME* is the
        directory containing Python, e.g. ``C:\\Python25``.

        On all other platforms, the default directory is the first of
        the following which exists or which can be created with write
        permission: ``/usr/share/nltk_data``, ``/usr/local/share/nltk_data``,
        ``/usr/lib/nltk_data``, ``/usr/local/lib/nltk_data``, ``~/nltk_data``.
--
        else:
            homedir = os.path.expanduser('~/')
            if homedir == '~/':
                raise ValueError("Could not find a default download directory")

        # append "nltk_data" to the home directory
        return os.path.join(homedir, 'nltk_data')

    def _get_download_dir(self):
        """
        The default directory to which packages will be downloaded.
        This defaults to the value returned by ``default_download_dir()``.
        To override this default on a case-by-case basis, use the
        ``download_dir`` argument when calling ``download()``.
        """
        return self._download_dir
    def _set_download_dir(self, download_dir):
        self._download_dir = download_dir
        # Clear the status cache.
        self._status_cache.clear()
--
    # Interactive Shell
    #/////////////////////////////////////////////////////////////////

    def _interactive_download(self):
        # Try the GUI first; if that doesn't work, try the simple
        # interactive shell.
        if TKINTER:
--
            DownloaderShell(self).run()

class DownloaderShell(object):
    def __init__(self, dataserver):
        self._ds = dataserver

    def _simple_interactive_menu(self, *options):
        print('-'*75)
        spc = (68 - sum(len(o) for o in options))//(len(options)-1)*' '
        print('    ' + spc.join(options))
--
        #print fmt % options
        print('-'*75)

    def run(self):
        print('NLTK Downloader')
        while True:
            self._simple_interactive_menu(
--
            # downloading it?
            print()

    def _simple_interactive_download(self, args):
        if args:
            for arg in args:
                try: self._ds.download(arg, prefix='    ')
--
                        except (IOError, ValueError) as e: print(e)
                    break

    def _simple_interactive_update(self):
        while True:
            stale_packages = []
            stale = partial = False
--
                print('Nothing to update.')
                return

    def _simple_interactive_help(self):
        print()
        print('Commands:')
        print('  d) Download a package or collection     u) Update out of date packages')
        print('  l) List packages & collections          h) Help')
        print('  c) View & Modify Configuration          q) Quit')

    def _show_config(self):
        print()
        print('Data Server:')
        print('  - URL: <%s>' % self._ds.url)
--
        print('Local Machine:')
        print('  - Data directory: %s' % self._ds.download_dir)

    def _simple_interactive_config(self):
        self._show_config()
        while True:
            print()
--
    """A dictionary specifying how columns should be resized when the
       table is resized.  Columns with weight 0 will not be resized at
       all; and columns with high weight will be resized more.
       Default weight (for columns not explicitly listed) is 1."""

    COLUMN_WIDTHS = {'':1, 'Identifier':20, 'Name':45,
                     'Size': 10, 'Unzipped Size': 10,
                     'Status': 12}
    """A dictionary specifying how wide each column should be, in
       characters.  The default width (for columns not explicitly
       listed) is specified by ``DEFAULT_COLUMN_WIDTH``."""

    DEFAULT_COLUMN_WIDTH = 30
    """The default width for columns that are not explicitly listed
       in ``COLUMN_WIDTHS``."""

    INITIAL_COLUMNS = ['', 'Identifier', 'Name', 'Size', 'Status']
    """The set of columns that should be displayed by default."""

    # Perform a few import-time sanity checks to make sure that the
    # column configuration variables are defined consistently:
    for c in COLUMN_WEIGHTS: assert c in COLUMNS
    for c in COLUMN_WIDTHS: assert c in COLUMNS
    for c in INITIAL_COLUMNS: assert c in COLUMNS
--
    # Constructor
    #/////////////////////////////////////////////////////////////////

    def __init__(self, dataserver, use_threads=True):
        self._ds = dataserver
        self._use_threads = use_threads

--
        # cancel any download in progress.
        self._table.bind('<Destroy>', self._destroy)

    def _log(self, msg):
        self._log_messages.append('%s %s%s' % (time.ctime(),
                                     ' | '*self._log_indent, msg))

--
    # Internals
    #/////////////////////////////////////////////////////////////////

    def _init_widgets(self):
        # Create the top-level frame structures
        f1 = Frame(self.top, relief='raised', border=2, padx=8, pady=0)
        f1.pack(sid='top', expand=True, fill='both')
--
                            reprfunc=self._table_reprfunc)
        self._table.columnconfig(0, foreground=self._MARK_COLOR[0]) # marked
        for i, column in enumerate(self.COLUMNS):
            width = self.COLUMN_WIDTHS.get(column, self.DEFAULT_COLUMN_WIDTH)
            self._table.columnconfig(i, width=width)
        self._table.pack(expand=True, fill='both')
        self._table.focus()
--
        self._progressbar.pack(side='right')
        self._progresslabel.pack(side='left')

    def _init_menu(self):
        menubar = Menu(self.top)

        filemenu = Menu(menubar, tearoff=0)
--

        self.top.config(menu=menubar)

    def _select_columns(self):
        for (column, var) in self._column_vars.items():
            if var.get():
                self._table.show_column(column)
            else:
                self._table.hide_column(column)

    def _refresh(self):
        self._ds.clear_status_cache()
        try:
            self._fill_table()
--
            showerror('Error connecting to server', e.reason)
        self._table.select(0)

    def _info_edit(self, info_key):
        self._info_save() # just in case.
        (entry, callback) = self._info[info_key]
        entry['state'] = 'normal'
        entry['relief'] = 'sunken'
        entry.focus()

    def _info_save(self, e=None):
        focus = self._table
        for entry, callback in self._info.values():
            if entry['state'] == 'disabled': continue
--
                callback(entry.get())
        focus.focus()

    def _table_reprfunc(self, row, col, val):
        if self._table.column_names[col].endswith('Size'):
            if isinstance(val, compat.string_types): return '  %s' % val
            elif val < 1024**2: return '  %.1f KB' % (val/1024.**1)
--
        if col in (0, ''): return str(val)
        else: return '  %s' % val

    def _set_url(self, url):
        if url == self._ds.url: return
        try:
            self._ds.url = url
--
        self._show_info()


    def _set_download_dir(self, download_dir):
        if self._ds.download_dir == download_dir: return
        # check if the dir exists, and if not, ask if we should create it?

--
            showerror('Error connecting to server', e.reason)
        self._show_info()

    def _show_info(self):
        print('showing info', self._ds.url)
        for entry,cb in self._info.values():
            entry['state'] = 'normal'
--
        for entry,cb in self._info.values():
            entry['state'] = 'disabled'

    def _prev_tab(self, *e):
        for i, tab in enumerate(self._tab_names):
            if tab.lower() == self._tab and i > 0:
                self._tab = self._tab_names[i-1].lower()
--
                except compat.URLError as e:
                    showerror('Error connecting to server', e.reason)

    def _next_tab(self, *e):
        for i, tab in enumerate(self._tab_names):
            if tab.lower() == self._tab and i < (len(self._tabs)-1):
                self._tab = self._tab_names[i+1].lower()
--
                except compat.URLError as e:
                    showerror('Error connecting to server', e.reason)

    def _select_tab(self, event):
        self._tab = event.widget['text'].lower()
        try:
            self._fill_table()
--
    _tab = 'collections'
    #_tab = 'corpora'
    _rows = None
    def _fill_table(self):
        selected_row = self._table.selected_row()
        self._table.clear()
        if self._tab == 'all packages':
--
        self.top.after(300, self._table._scrollbar.set,
                       *self._table._mlb.yview())

    def _update_table_status(self):
        for row_num in range(len(self._table)):
            status = self._ds.status(self._table[row_num, 'Identifier'])
            self._table[row_num, 'Status'] = status
        self._color_table()

    def _download(self, *e):
        # If we're using threads, then delegate to the threaded
        # downloader instead.
        if self._use_threads:
--
        self._download_cb(download_iter, marked)

    _DL_DELAY=10
    def _download_cb(self, download_iter, ids):
        try: msg = next(download_iter)
        except StopIteration:
            #self._fill_table(sort=False)
--
            self._afterid['_download_cb'] = afterid
            return

        def show(s):
            self._progresslabel['text'] = s
            self._log(s)
        if isinstance(msg, ProgressMessage):
--
                                 download_iter, ids)
        self._afterid['_download_cb'] = afterid

    def _select(self, id):
        for row in range(len(self._table)):
            if self._table[row, 'Identifier'] == id:
                self._table.select(row)
                return

    def _color_table(self):
        # Color rows according to status.
        for row in range(len(self._table)):
            bg, sbg = self._ROW_COLOR[self._table[row, 'Status']]
--
                                      background=self._MARK_COLOR[1])


    def _clear_mark(self, id):
        for row in range(len(self._table)):
            if self._table[row, 'Identifier'] == id:
                self._table[row, 0] = ''

    def _mark_all(self, *e):
        for row in range(len(self._table)):
            self._table[row,0] = 'X'

    def _table_mark(self, *e):
        selection = self._table.selected_row()
        if selection >= 0:
            if self._table[selection][0] != '':
--
                self._table[selection,0] = 'X'
        self._table.select(delta=1)

    def _show_log(self):
        text = '\n'.join(self._log_messages)
        ShowText(self.top, 'NLTK Downloader Log', text)

    def _package_to_columns(self, pkg):
        """
        Given a package, return a list of values describing that
        package, one for each column in ``self.COLUMNS``.
--
    # External Interface
    #/////////////////////////////////////////////////////////////////

    def destroy(self, *e):
        if self._destroyed: return
        self.top.destroy()
        self._destroyed = True

    def _destroy(self, *e):
        if self.top is not None:
            for afterid in self._afterid.values():
                self.top.after_cancel(afterid)
--
        # thread, which would make Tkinter unhappy.
        self._column_vars.clear()

    def mainloop(self, *args, **kwargs):
        self.top.mainloop(*args, **kwargs)

    #/////////////////////////////////////////////////////////////////
--
    This tool can be used to download a variety of corpora and models
    that can be used with NLTK.  Each corpus or model is distributed
    in a single zip file, known as a \"package file.\"  You can
    download packages individually, or you can download pre-defined
    collections of packages.

    When you download a package, it will be saved to the \"download
    directory.\"  A default download directory is chosen when you run

    the downloader; but you may also select a different download
    directory.  On Windows, the default download directory is


    \"package.\"
--
      [right]\t Select next tab
    """)

    def help(self, *e):
        # The default font's not very legible; try using 'fixed' instead.
        try:
            ShowText(self.top, 'Help: NLTK Dowloader',
                     self.HELP.strip(), width=75, font='fixed')
--
            ShowText(self.top, 'Help: NLTK Downloader',
                     self.HELP.strip(), width=75)

    def about(self, *e):
        ABOUT = ("NLTK Downloader\n"+
                 "Written by Edward Loper")
        TITLE = 'About: NLTK Downloader'
--
    #/////////////////////////////////////////////////////////////////

    _gradient_width = 5
    def _init_progressbar(self):
        c = self._progressbar
        width, height = int(c['width']), int(c['height'])
        for i in range(0, (int(c['width'])*2)//self._gradient_width):
--
        c.addtag_withtag('redbox', c.create_rectangle(
            0, 0, 0, 0, fill=self._PROGRESS_COLOR[0]))

    def _show_progress(self, percent):
        c = self._progressbar
        if percent is None:
            c.coords('redbox', 0, 0, 0, 0)
--
            x = percent * int(width) // 100 + 1
            c.coords('redbox', 0, 0, x, height+1)

    def _progress_alive(self):
        c = self._progressbar
        if not self._downloading:
            c.itemconfig('gradient', state='hidden')
--
    # Threaded downloader
    #/////////////////////////////////////////////////////////////////

    def _download_threaded(self, *e):
        # If the user tries to start a new download while we're already
        # downloading something, then abort the current download instead.
        if self._downloading:
--
        # cycling the progress bar.
        self._progress_alive()

    def _abort_download(self):
        if self._downloading:
            self._download_lock.acquire()
            self._download_abort_queue.append('abort')
            self._download_lock.release()

    class _DownloadThread(threading.Thread):
        def __init__(self, data_server, items, lock, message_queue, abort):
            self.data_server = data_server
            self.items = items
            self.lock = lock
--
            self.abort = abort
            threading.Thread.__init__(self)

        def run (self):
            for msg in self.data_server.incr_download(self.items):
                self.lock.acquire()
                self.message_queue.append(msg)
--
            self.lock.release()

    _MONITOR_QUEUE_DELAY=100
    def _monitor_message_queue(self):
        def show(s):
            self._progresslabel['text'] = s
            self._log(s)

--
######################################################################
# [xx] It may make sense to move these to nltk.internals.

def md5_hexdigest(file):
    """
    Calculate and return the MD5 checksum for a given file.
    ``file`` may either be a filename or an open stream.
--
            return _md5_hexdigest(fp)
    return _md5_hexdigest(file)

def _md5_hexdigest(fp):
    md5_digest = md5()
    while True:
        block = fp.read(1024*16)  # 16k blocks
--
# change this to periodically yield progress messages?
# [xx] get rid of topdir parameter -- we should be checking
# this when we build the index, anyway.
def unzip(filename, root, verbose=True):
    """
    Extract the contents of the zip file ``filename`` into the
    directory ``root``.
--
        if isinstance(message, ErrorMessage):
            raise Exception(message)

def _unzip_iter(filename, root, verbose=True):
    if verbose:
        sys.stdout.write('Unzipping %s' % os.path.split(filename)[1])
        sys.stdout.flush()
--
# This may move to a different file sometime.
import subprocess, zipfile

def build_index(root, base_url):
    """
    Create a new data.xml index file, by combining the xml description
    files for various packages and collections.  ``root`` should be the
--
    _indent_xml(top_elt)
    return top_elt

def _indent_xml(xml, prefix=''):
    """
    Helper for ``build_index()``: Given an XML ``ElementTree``, modify it
    (and its descendents) ``text`` and ``tail`` attributes to generate
--
            child.tail = (child.tail or '').strip() + '\n' + prefix + '  '
        xml[-1].tail = (xml[-1].tail or '').strip() + '\n' + prefix

def _check_package(pkg_xml, zipfilename, zf):
    """
    Helper for ``build_index()``: Perform some checks to make sure that
    the given package is consistent.
--
                         'subdirectory %s/' % (uid, uid))

# update for git?
def _svn_revision(filename):
    """
    Helper for ``build_index()``: Calculate the subversion revision
    number for a given file (by using ``subprocess`` to run ``svn``).
--
                         (os.path.split(filename)[1], textwrap.fill(stderr)))
    return stdout.split()[2]

def _find_collections(root):
    """
    Helper for ``build_index()``: Yield a list of ElementTree.Element
    objects, each holding the xml for a single package collection.
--
                xmlfile = os.path.join(dirname, filename)
                yield ElementTree.parse(xmlfile).getroot()

def _find_packages(root):
    """
    Helper for ``build_index()``: Yield a list of tuples
    ``(pkg_xml, zf, subdir)``, where:
--
_downloader = Downloader()
download = _downloader.download

def download_shell():
    DownloaderShell(_downloader).run()

def download_gui():
    DownloaderGUI(_downloader).mainloop()

def update():
    _downloader.update()

if __name__ == '__main__':
--
    parser.add_option("-d", "--dir", dest="dir",
        help="download package to directory DIR", metavar="DIR")
    parser.add_option("-q", "--quiet", dest="quiet", action="store_true",
        default=False, help="work quietly")
    parser.add_option("-f", "--force", dest="force", action="store_true",
        default=False, help="download even if already installed")
    parser.add_option("-e", "--exit-on-error", dest="halt_on_error", action="store_true",
        default=False, help="exit if an error occurs")
    parser.add_option("-u", "--url", dest="server_index_url",
        default=None, help="download server index url")

    (options, args) = parser.parse_args()

--

""" Helper to enable simple lazy module import.

    'Lazy' means the actual import is deferred until an attribute is
    requested from the module's namespace. This has the advantage of
    allowing all imports to be done at the top of a script (in a
    prominent and visible place) without having a great impact
--
    # Globals dictionary to use for the module import
    __lazymodule_globals = None

    def __init__(self, name, locals, globals=None):

        """ Create a LazyModule instance wrapping module name.

            The module will later on be registered in locals under the
            given module name.

            globals is optional and defaults to locals.

        """
        self.__lazymodule_locals = locals
--
            self.__name__ = self.__lazymodule_name = name
        self.__lazymodule_init = 1

    def __lazymodule_import(self):

        """ Import the module now.
        """
--
            print('LazyModule: Module %r loaded' % name)
        return module

    def __getattr__(self, name):

        """ Import the module on demand and get the attribute.
        """
--
        module = self.__lazymodule_import()
        return getattr(module, name)

    def __setattr__(self, name, value):

        """ Import the module on demand and set the attribute.
        """
--
        module = self.__lazymodule_import()
        setattr(module, name, value)

    def __repr__(self):
        return "<LazyModule '%s'>" % self.__name__
--

from itertools import islice, chain
from pprint import pprint
from collections import defaultdict, deque
from sys import version_info

from nltk.internals import slice_bounds, raise_unorderable_types
--
# Short usage message
######################################################################

def usage(obj, selfname='self'):
    import inspect
    str(obj) # In case it's lazy, this will load it.

--
        if name.startswith('_'): continue
        if getattr(method, '__deprecated__', False): continue

        args, varargs, varkw, defaults = inspect.getargspec(method)
        if (args and args[0]=='self' and
            (defaults is None or len(args)>len(defaults))):
            args = args[1:]
            name = '%s.%s' % (selfname, name)
        argspec = inspect.formatargspec(
            args, varargs, varkw, defaults)
        print(textwrap.fill('%s%s' % (name, argspec),
                            initial_indent='  - ',
                            subsequent_indent=' '*(len(name)+5)))
--
# IDLE
##########################################################################

def in_idle():
    """
    Return True if this function is run within idle.  Tkinter
    programs that are run in idle should never call ``Tk.mainloop``; so
--
# PRETTY PRINTING
##########################################################################

def pr(data, start=0, end=None):
    """
    Pretty print a sequence of data items

--
    """
    pprint(list(islice(data, start, end)))

def print_string(s, width=70):
    """
    Pretty print a string, breaking lines on whitespace

--
    """
    print('\n'.join(textwrap.wrap(s, width=width)))

def tokenwrap(tokens, separator=" ", width=70):
    """
    Pretty print a list of text tokens, breaking lines on whitespace

--
    :type tokens: list
    :param separator: the string to use to separate tokens
    :type separator: str
    :param width: the display width (default=70)
    :type width: int
    """
    return '\n'.join(textwrap.wrap(separator.join(tokens), width=width))
--
# Python version
##########################################################################

def py25():
    return version_info[0] == 2 and version_info[1] == 5
def py26():
    return version_info[0] == 2 and version_info[1] == 6
def py27():
    return version_info[0] == 2 and version_info[1] == 7


--
# Indexing
##########################################################################

class Index(defaultdict):

    def __init__(self, pairs):
        defaultdict.__init__(self, list)
        for key, value in pairs:
            self[key].append(value)

--
## Regexp display (thanks to David Mertz)
######################################################################

def re_show(regexp, string, left="{", right="}"):
    """
    Return a string with markers surrounding the matched substrings.
    Search str for substrings matching ``regexp`` and wrap the matches
--
##########################################################################

# recipe from David Mertz
def filestring(f):
    if hasattr(f, 'read'):
        return f.read()
    elif isinstance(f, string_types):
--
# Breadth-First Search
##########################################################################

def breadth_first(tree, children=iter, maxdepth=-1):
    """Traverse the nodes of a tree in breadth-first order.
    (No need to check for cycles.)
    The first argument should be the tree root;
--
# adapted from io.py in the docutils extension module (http://docutils.sourceforge.net)
# http://www.pyzine.com/Issue008/Section_Articles/article_Encodings.html

def guess_encoding(data):
    """
    Given a byte string, attempt to decode it.
    Tries the standard 'UTF8' and 'latin-1' encodings,
--
    except (AttributeError, IndexError):
        pass
    try:
        encodings.append(locale.getdefaultlocale()[1])
    except (AttributeError, IndexError):
        pass
    #
--
# Invert a dictionary
##########################################################################

def invert_dict(d):
    inverted_dict = defaultdict(list)
    for key in d:
        if hasattr(d[key], '__iter__'):
            for term in d[key]:
--
# The graph is represented as a dictionary of sets
##########################################################################

def transitive_closure(graph, reflexive=False):
    """
    Calculate the transitive closure of a directed graph,
    optionally the reflexive transitive closure.
--
        while agenda:
            j = agenda.pop()
            closure.add(j)
            closure |= closure_graph.setdefault(j, base_set(j))
            agenda |= agenda_graph.get(j, base_set(j))
            agenda -= closure
    return closure_graph


def invert_graph(graph):
    """
    Inverts a directed graph.

--
    inverted = {}
    for key in graph:
        for value in graph[key]:
            inverted.setdefault(value, set()).add(key)
    return inverted


--
# HTML Cleaning
##########################################################################

def clean_html(html):
    raise NotImplementedError ("To remove HTML markup, use BeautifulSoup's get_text() function")

def clean_url(url):
    raise NotImplementedError ("To remove HTML markup, use BeautifulSoup's get_text() function")

##########################################################################
# FLATTEN LISTS
##########################################################################

def flatten(*args):
    """
    Flatten a list.

--

# add a flag to pad the sequence so we get peripheral ngrams?

def ngrams(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):
    """
    Return a sequence of ngrams from a sequence of items.  For example:

--
    :type pad_left: bool
    :param pad_right: whether the ngrams should be right-padded
    :type pad_right: bool
    :param pad_symbol: the symbol to use for padding (default is None)
    :type pad_symbol: any
    :rtype: list(tuple)
    """
--
    count = max(0, len(sequence) - n + 1)
    return [tuple(sequence[i:i+n]) for i in range(count)]

def bigrams(sequence, **kwargs):
    """
    Return a sequence of bigrams from a sequence of items.  For example:

--
    """
    return ngrams(sequence, 2, **kwargs)

def trigrams(sequence, **kwargs):
    """
    Return a sequence of trigrams from a sequence of items.  For example:

--
    """
    return ngrams(sequence, 3, **kwargs)

def ingrams(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):
    """
    Return the ngrams generated from a sequence of items, as an iterator.
    For example:
--
    :type pad_left: bool
    :param pad_right: whether the ngrams should be right-padded
    :type pad_right: bool
    :param pad_symbol: the symbol to use for padding (default is None)
    :type pad_symbol: any
    :rtype: iter(tuple)
    """
--
        yield tuple(history)
        del history[0]

def ibigrams(sequence, **kwargs):
    """
    Return the bigrams generated from a sequence of items, as an iterator.
    For example:
--
    for item in ingrams(sequence, 2, **kwargs):
        yield item

def itrigrams(sequence, **kwargs):
    """
    Return the trigrams generated from a sequence of items, as an iterator.
    For example:
--
##########################################################################

class OrderedDict(dict):
    def __init__(self, data=None, **kwargs):
        self._keys = self.keys(data, kwargs.get('keys'))
        self._default_factory = kwargs.get('default_factory')
        if data is None:
            dict.__init__(self)
        else:
            dict.__init__(self, data)

    def __delitem__(self, key):
        dict.__delitem__(self, key)
        self._keys.remove(key)

    def __getitem__(self, key):
        try:
            return dict.__getitem__(self, key)
        except KeyError:
            return self.__missing__(key)

    def __iter__(self):
        return (key for key in self.keys())

    def __missing__(self, key):
        if not self._default_factory and key not in self._keys:
            raise KeyError()
        return self._default_factory()

    def __setitem__(self, key, item):
        dict.__setitem__(self, key, item)
        if key not in self._keys:
            self._keys.append(key)

    def clear(self):
        dict.clear(self)
        self._keys.clear()

    def copy(self):
        d = dict.copy(self)
        d._keys = self._keys
        return d

    def items(self):
        # returns iterator under python 3 and list under python 2
        return zip(self.keys(), self.values())

    def keys(self, data=None, keys=None):
        if data:
            if keys:
                assert isinstance(keys, list)
--
        else:
            return []

    def popitem(self):
        if not self._keys:
            raise KeyError()

--
        del self[key]
        return (key, value)

    def setdefault(self, key, failobj=None):
        dict.setdefault(self, key, failobj)
        if key not in self._keys:
            self._keys.append(key)

    def update(self, data):
        dict.update(self, data)
        for key in self.keys(data):
            if key not in self._keys:
                self._keys.append(key)

    def values(self):
        # returns iterator under python 3
        return map(self.get, self._keys)

--
    pieces of the corpus from disk as needed.

    The result of modifying a mutable element of a lazy sequence is
    undefined.  In particular, the modifications made to the element
    may or may not persist, depending on whether and when the lazy
    sequence caches that element's value or reconstructs it from
    scratch.

    Subclasses are required to define two methods: ``__len__()``
    and ``iterate_from()``.
    """
    def __len__(self):
        """
        Return the number of tokens in the corpus file underlying this
        corpus view.
        """
        raise NotImplementedError('should be implemented by subclass')

    def iterate_from(self, start):
        """
        Return an iterator that generates the tokens in the corpus
        file underlying this corpus view, starting at the token number
--
        """
        raise NotImplementedError('should be implemented by subclass')

    def __getitem__(self, i):
        """
        Return the *i* th token in the corpus file underlying this
        corpus view.  Negative indices and spans are both supported.
--
            except StopIteration:
                raise IndexError('index out of range')

    def __iter__(self):
        """Return an iterator that generates the tokens in the corpus
        file underlying this corpus view."""
        return self.iterate_from(0)

    def count(self, value):
        """Return the number of times this list contains ``value``."""
        return sum(1 for elt in self if elt==value)

    def index(self, value, start=None, stop=None):
        """Return the index of the first occurrence of ``value`` in this
        list that is greater than or equal to ``start`` and less than
        ``stop``.  Negative start and stop values are treated like negative
--
            if elt == value: return i+start
        raise ValueError('index(x): x not in list')

    def __contains__(self, value):
        """Return true if this list contains ``value``."""
        return bool(self.count(value))

    def __add__(self, other):
        """Return a list concatenating self with other."""
        return LazyConcatenation([self, other])

    def __radd__(self, other):
        """Return a list concatenating other with self."""
        return LazyConcatenation([other, self])

    def __mul__(self, count):
        """Return a list concatenating self with itself ``count`` times."""
        return LazyConcatenation([self] * count)

    def __rmul__(self, count):
        """Return a list concatenating self with itself ``count`` times."""
        return LazyConcatenation([self] * count)

    _MAX_REPR_SIZE = 60
    def __repr__(self):
        """
        Return a string representation for this corpus view that is
        similar to a list's representation; but if it would be more
--
        else:
            return '[%s]' % text_type(', ').join(pieces)

    def __eq__(self, other):
        return (type(self) == type(other) and list(self) == list(other))

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if type(other) != type(self):
            raise_unorderable_types("<", self, other)
        return list(self) < list(other)

    def __hash__(self):
        """
        :raise ValueError: Corpus view objects are unhashable.
        """
--
    shorter than ``MIN_SIZE``, then a tuple will be returned instead.
    """

    def __new__(cls, source, start, stop):
        """
        Construct a new slice from a given underlying sequence.  The
        ``start`` and ``stop`` indices should be absolute indices --
--
        else:
            return object.__new__(cls)

    def __init__(self, source, start, stop):
        self._source = source
        self._start = start
        self._stop = stop

    def __len__(self):
        return self._stop - self._start

    def iterate_from(self, start):
        return islice(self._source.iterate_from(start+self._start),
                      max(0, len(self)-start))

--
    between offsets in the concatenated lists and offsets in the
    sublists.
    """
    def __init__(self, list_of_lists):
        self._list = list_of_lists
        self._offsets = [0]

    def __len__(self):
        if len(self._offsets) <= len(self._list):
            for tok in self.iterate_from(self._offsets[-1]): pass
        return self._offsets[-1]

    def iterate_from(self, start_index):
        if start_index < self._offsets[-1]:
            sublist_index = bisect.bisect_right(self._offsets, start_index)-1
        else:
--
    using a ``LazyMap`` can significantly reduce memory usage when
    training and running classifiers.
    """
    def __init__(self, function, *lists, **config):
        """
        :param function: The function that should be applied to
            elements of ``lists``.  It should take as many arguments
            as there are ``lists``.
        :param lists: The underlying lists.
        :param cache_size: Determines the size of the cache used
            by this lazy map.  (default=5)
        """
        if not lists:
            raise TypeError('LazyMap requires at least two args')
--
        self._all_lazy = sum(isinstance(lst, AbstractLazySequence)
                             for lst in lists) == len(lists)

    def iterate_from(self, index):
        # Special case: one lazy sublist
        if len(self._lists) == 1 and self._all_lazy:
            for value in self._lists[0].iterate_from(index):
--
                yield self._func(*elements)
                index += 1

    def __getitem__(self, index):
        if isinstance(index, slice):
            sliced_lists = [lst[index] for lst in self._lists]
            return LazyMap(self._func, *sliced_lists)
--
            # Return the value
            return val

    def __len__(self):
        return max(len(lst) for lst in self._lists)


--
    avoiding the creation of an additional long sequence, memory usage can be
    significantly reduced.
    """
    def __init__(self, *lists):
        """
        :param lists: the underlying lists
        :type lists: list(list)
        """
        LazyMap.__init__(self, lambda *elts: elts, *lists)

    def iterate_from(self, index):
        iterator = LazyMap.iterate_from(self, index)
        while index < len(self):
            yield next(iterator)
            index += 1
        return

    def __len__(self):
        return min(len(lst) for lst in self._lists)


--
    significantly reduced.
    """

    def __init__(self, lst):
        """
        :param lst: the underlying list
        :type lst: list
--
######################################################################

# inherited from pywordnet, by Oliver Steele
def binary_search_file(file, key, cache={}, cacheDepth=-1):
    """
    Return the line from the file with first word key.
    Searches through a sorted file using the binary search algorithm.
--
# Proxy configuration
######################################################################

def set_proxy(proxy, user=None, password=''):
    """
    Set the HTTP proxy for Python to download through.

--
        try:
            proxy = compat.getproxies()['http']
        except KeyError:
            raise ValueError('Could not detect default proxy settings')

    # Set up the proxy handler
    proxy_handler = compat.ProxyHandler({'http': proxy})
--

    if user is not None:
        # Set up basic proxy authentication if provided
        password_manager = compat.HTTPPasswordMgrWithDefaultRealm()
        password_manager.add_password(realm=None, uri=proxy, user=user,
                passwd=password)
        opener.add_handler(compat.ProxyBasicAuthHandler(password_manager))
--
    (suntsu_chat, 'Suntsu (Chinese sayings)'),
    (zen_chat,    'Zen (gems of wisdom)')]

def chatbots():
    import sys
    print('Which chatbot would you like to talk to?')
    botcount = len(bots)
--
  (r'[G-Ig-i](.*)',
  ("Heaven signifies night and day, cold and heat, times and seasons.",
   "It is the rule in war, if our forces are ten to the enemy's one, to surround him; if five to one, to attack him; if twice as numerous, to divide our army into two.",
   "The good fighters of old first put themselves beyond the possibility of defeat, and then waited for an opportunity of defeating the enemy.",
   "One may know how to conquer without being able to do it.")),

  (r'[J-Lj-l](.*)',
--

  (r'[M-Om-o](.*)',
  ("If you know the enemy and know yourself, you need not fear the result of a hundred battles.",
   "If you know yourself but not the enemy, for every victory gained you will also suffer a defeat.",
   "If you know neither the enemy nor yourself, you will succumb in every battle.",
   "The control of a large force is the same principle as the control of a few men: it is merely a question of dividing up their numbers.")),

  (r'[P-Rp-r](.*)',
  ("Security against defeat implies defensive tactics; ability to defeat the enemy means taking the offensive.",
   "Standing on the defensive indicates insufficient strength; attacking, a superabundance of strength.",
   "He wins his battles by making no mistakes. Making no mistakes is what establishes the certainty of victory, for it means conquering an enemy that is already defeated.",
   "A victorious army opposed to a routed one, is as a pound's weight placed in the scale against a single grain.",
   "The onrush of a conquering force is like the bursting of pent-up waters into a chasm a thousand fathoms deep.")),

  (r'[S-Us-u](.*)',
  ("What the ancients called a clever fighter is one who not only wins, but excels in winning with ease.",
   "Hence his victories bring him neither reputation for wisdom nor credit for courage.",
   "Hence the skillful fighter puts himself into a position which makes defeat impossible, and does not miss the moment for defeating the enemy.",
   "In war the victorious strategist only seeks battle after the victory has been won, whereas he who is destined to defeat first fights and afterwards looks for victory.",
   "There are not more than five musical notes, yet the combinations of these five give rise to more melodies than can ever be heard.",
   "Appear at points which the enemy must hasten to defend; march swiftly to places where you are not expected.")),

  (r'[V-Zv-z](.*)',
  ("It is a matter of life and death, a road either to safety or to ruin.",
--

suntsu_chatbot = Chat(pairs, reflections)

def suntsu_chat():
    print("Talk to the program by typing in plain English, using normal upper-")
    print('and lower-case letters and punctuation.  Enter "quit" when done.')
    print('='*72)
--

    suntsu_chatbot.converse()

def demo():
    suntsu_chat()

if __name__ == "__main__":
--

iesha_chatbot = Chat(pairs, reflections)

def iesha_chat():
    print("Iesha the TeenBoT\n---------")
    print("Talk to the program by typing in plain English, using normal upper-")
    print('and lower-case letters and punctuation.  Enter "quit" when done.')
--

    iesha_chatbot.converse()

def demo():
    iesha_chat()

if __name__ == "__main__":
--

rude_chatbot = Chat(pairs, reflections)

def rude_chat():
    print("Talk to the program by typing in plain English, using normal upper-")
    print('and lower-case letters and punctuation.  Enter "quit" when done.')
    print('='*72)
--

    rude_chatbot.converse()

def demo():
    rude_chat()

if __name__ == "__main__":
--

eliza_chatbot = Chat(pairs, reflections)

def eliza_chat():
    print("Therapist\n---------")
    print("Talk to the program by typing in plain English, using normal upper-")
    print('and lower-case letters and punctuation.  Enter "quit" when done.')
--

    eliza_chatbot.converse()

def demo():
    eliza_chat()

if __name__ == "__main__":
--
}

class Chat(object):
    def __init__(self, pairs, reflections={}):
        """
        Initialize the chatbot.  Pairs is a list of patterns and responses.  Each
        pattern is a regular expression matching the user's statement or question,
--
        self._regex = self._compile_reflections()


    def _compile_reflections(self):
        sorted_refl = sorted(self._reflections.keys(), key=len,
                reverse=True)
        return  re.compile(r"\b({0})\b".format("|".join(map(re.escape,
            sorted_refl))), re.IGNORECASE)

    def _substitute(self, str):
        """
        Substitute words in the string, according to the specified reflections,
        e.g. "I'm" -> "you are"
--
                self._reflections[mo.string[mo.start():mo.end()]],
                    str.lower())

    def _wildcards(self, response, match):
        pos = response.find('%')
        while pos >= 0:
            num = int(response[pos+1:pos+2])
--
            pos = response.find('%')
        return response

    def respond(self, str):
        """
        Generate a response to the user input.

--
                return resp

    # Hold a conversation with a chatbot
    def converse(self, quit="quit"):
        input = ""
        while input != quit:
            input = quit
--

zen_chatbot = Chat(responses, reflections)

def zen_chat():
    print('*'*75)
    print("Zen Chatbot!".center(75))
    print('*'*75)
--

    zen_chatbot.converse()

def demo():
    zen_chat()

if __name__ == "__main__":
--

Lightweight Feature Structures
==============================
Many of the functions defined by ``nltk.featstruct`` can be applied
directly to simple Python dictionaries and lists, rather than to
full-fledged ``FeatDict`` and ``FeatList`` objects.  In other words,
Python ``dicts`` and ``lists`` can be used as "light-weight" feature
--
    Two feature structures are considered equal if they assign the
    same values to all features, and have the same reentrancies.

    By default, feature structures are mutable.  They may be made
    immutable with the ``freeze()`` method.  Once they have been
    frozen, they may be hashed, and thus used as dictionary keys.
    """
--
    #{ Constructor
    ##////////////////////////////////////////////////////////////

    def __new__(cls, features=None, **morefeatures):
        """
        Construct and return a new feature structure.  If this
        constructor is called directly, then the returned feature
--
    ##////////////////////////////////////////////////////////////
    #{ Uniform Accessor Methods
    ##////////////////////////////////////////////////////////////
    # These helper functions allow the methods defined by FeatStruct
    # to treat all feature structures as mappings, even if they're
    # really lists.  (Lists are treated as mappings from ints to vals)

    def _keys(self):
        """Return an iterable of the feature identifiers used by this
        FeatStruct."""
        raise NotImplementedError() # Implemented by subclasses.

    def _values(self):
        """Return an iterable of the feature values directly defined
        by this FeatStruct."""
        raise NotImplementedError() # Implemented by subclasses.

    def _items(self):
        """Return an iterable of (fid,fval) pairs, where fid is a
        feature identifier and fval is the corresponding feature
        value, for all features defined by this FeatStruct."""
        raise NotImplementedError() # Implemented by subclasses.

    ##////////////////////////////////////////////////////////////
    #{ Equality & Hashing
    ##////////////////////////////////////////////////////////////

    def equal_values(self, other, check_reentrance=False):
        """
        Return True if ``self`` and ``other`` assign the same value to
        to every feature.  In particular, return true if
--
        """
        return self._equal(other, check_reentrance, set(), set(), set())

    def __eq__(self, other):
        """
        Return true if ``self`` and ``other`` are both feature structures,
        assign the same values to all features, and contain the same
--
        """
        return self._equal(other, True, set(), set(), set())

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, FeatStruct):
            # raise_unorderable_types("<", self, other)
            # Sometimes feature values can be pure strings,
--
        else:
            return len(self) < len(other)

    def __hash__(self):
        """
        If this feature structure is frozen, return its hash value;
        otherwise, raise ``TypeError``.
--
            self._hash = self._calculate_hashvalue(set())
            return self._hash

    def _equal(self, other, check_reentrance, visited_self,
               visited_other, visited_pairs):
        """
        Return True iff self and other have equal values.
--
        # If we're the same object, then we're equal.
        if self is other: return True

        # If we have different classes, we're definitely not equal.
        if self.__class__ != other.__class__: return False

        # If we define different features, we're definitely not equal.
        # (Perform len test first because it's faster -- we should
        # do profiling to see if this actually helps)
        if len(self) != len(other): return False
--
        # Everything matched up; return true.
        return True

    def _calculate_hashvalue(self, visited):
        """
        Return a hash value for this feature structure.

--
    #: feature structure.
    _FROZEN_ERROR = "Frozen FeatStructs may not be modified."

    def freeze(self):
        """
        Make this feature structure, and any feature structures it
        contains, immutable.  Note: this method does not attempt to
--
        if self._frozen: return
        self._freeze(set())

    def frozen(self):
        """
        Return True if this feature structure is immutable.  Feature
        structures can be made immutable with the ``freeze()`` method.
--
        """
        return self._frozen

    def _freeze(self, visited):
        """
        Make this feature structure, and any feature structure it
        contains, immutable.
--
    #{ Copying
    ##////////////////////////////////////////////////////////////

    def copy(self, deep=True):
        """
        Return a new copy of ``self``.  The new copy will not be frozen.

--
        else:
            return self.__class__(self)

    # Subclasses should define __deepcopy__ to ensure that the new
    # copy will not be frozen.
    def __deepcopy__(self, memo):
        raise NotImplementedError() # Implemented by subclasses.

    ##////////////////////////////////////////////////////////////
    #{ Structural Information
    ##////////////////////////////////////////////////////////////

    def cyclic(self):
        """
        Return True if this feature structure contains itself.
        """
        return self._find_reentrances({})[id(self)]

    def walk(self):
        """
        Return an iterator that generates this feature structure, and
        each feature structure it contains.  Each feature structure will
--
        """
        return self._walk(set())

    def _walk(self, visited):
        """
        Return an iterator that generates this feature structure, and
        each feature structure it contains.
--
        """
        raise NotImplementedError() # Implemented by subclasses.

    def _walk(self, visited):
        if id(self) in visited: return
        visited.add(id(self))
        yield self
--
    # Walk through the feature tree.  The first time we see a feature
    # value, map it to False (not reentrant).  If we see a feature
    # value more than once, then map it to True (reentrant).
    def _find_reentrances(self, reentrances):
        """
        Return a dictionary that maps from the ``id`` of each feature
        structure contained in ``self`` (including ``self``) to a
--
    #{ Variables & Bindings
    ##////////////////////////////////////////////////////////////

    def substitute_bindings(self, bindings):
        """:see: ``nltk.featstruct.substitute_bindings()``"""
        return substitute_bindings(self, bindings)

    def retract_bindings(self, bindings):
        """:see: ``nltk.featstruct.retract_bindings()``"""
        return retract_bindings(self, bindings)

    def variables(self):
        """:see: ``nltk.featstruct.find_variables()``"""
        return find_variables(self)

    def rename_variables(self, vars=None, used_vars=(), new_vars=None):
        """:see: ``nltk.featstruct.rename_variables()``"""
        return rename_variables(self, vars, used_vars, new_vars)

    def remove_variables(self):
        """
        Return the feature structure that is obtained by deleting
        any feature whose value is a ``Variable``.
--
    #{ Unification
    ##////////////////////////////////////////////////////////////

    def unify(self, other, bindings=None, trace=False,
              fail=None, rename_vars=True):
        return unify(self, other, bindings, trace, fail, rename_vars)

    def subsumes(self, other):
        """
        Return True if ``self`` subsumes ``other``.  I.e., return true
        If unifying ``self`` with ``other`` would result in a feature
--
    #{ String Representations
    ##////////////////////////////////////////////////////////////

    def __repr__(self):
        """
        Display a single-line representation of this feature structure,
        suitable for embedding in other representations.
        """
        return self._repr(self._find_reentrances({}), {})

    def _repr(self, reentrances, reentrance_ids):
        """
        Return a string representation of this feature structure.

--
# Mutation: disable if frozen.
_FROZEN_ERROR = "Frozen FeatStructs may not be modified."
_FROZEN_NOTICE = "\n%sIf self is frozen, raise ValueError."
def _check_frozen(method, indent=''):
    """
    Given a method function, return a new method function that first
    checks if ``self._frozen`` is true; and if so, raises ``ValueError``
    with an appropriate message.  Otherwise, call the method and return
    its result.
    """
    def wrapped(self, *args, **kwargs):
        if self._frozen: raise ValueError(_FROZEN_ERROR)
        else: return method(self, *args, **kwargs)
    wrapped.__name__ = method.__name__
--
    :see: ``FeatStruct`` for information about feature paths, reentrance,
        cyclic feature structures, mutability, freezing, and hashing.
    """
    def __init__(self, features=None, **morefeatures):
        """
        Create a new feature dictionary, with the specified features.

--
    #////////////////////////////////////////////////////////////
    _INDEX_ERROR = str("Expected feature name or path.  Got %r.")

    def __getitem__(self, name_or_path):
        """If the feature with the given name or path exists, return
        its value; otherwise, raise ``KeyError``."""
        if isinstance(name_or_path, (string_types, Feature)):
--
        else:
            raise TypeError(self._INDEX_ERROR % name_or_path)

    def get(self, name_or_path, default=None):
        """If the feature with the given name or path exists, return its
        value; otherwise, return ``default``."""
        try: return self[name_or_path]
        except KeyError: return default

    def __contains__(self, name_or_path):
        """Return true if a feature with the given name or path exists."""
        try: self[name_or_path]; return True
        except KeyError: return False

    def has_key(self, name_or_path):
        """Return true if a feature with the given name or path exists."""
        return name_or_path in self

    def __delitem__(self, name_or_path):
        """If the feature with the given name or path exists, delete
        its value; otherwise, raise ``KeyError``."""
        if self._frozen: raise ValueError(_FROZEN_ERROR)
--
        else:
            raise TypeError(self._INDEX_ERROR % name_or_path)

    def __setitem__(self, name_or_path, value):
        """Set the value for the feature with the given name or path
        to ``value``.  If ``name_or_path`` is an invalid path, raise
        ``KeyError``."""
--
    clear = _check_frozen(dict.clear)
    pop = _check_frozen(dict.pop)
    popitem = _check_frozen(dict.popitem)
    setdefault = _check_frozen(dict.setdefault)

    def update(self, features=None, **morefeatures):
        if self._frozen: raise ValueError(_FROZEN_ERROR)
        if features is None:
            items = ()
--
    #{ Copying
    ##////////////////////////////////////////////////////////////

    def __deepcopy__(self, memo):
        memo[id(self)] = selfcopy = self.__class__()
        for (key, val) in self._items():
            selfcopy[copy.deepcopy(key,memo)] = copy.deepcopy(val,memo)
--
    #{ Uniform Accessor Methods
    ##////////////////////////////////////////////////////////////

    def _keys(self): return self.keys()
    def _values(self): return self.values()
    def _items(self): return self.items()

    ##////////////////////////////////////////////////////////////
    #{ String Representations
    ##////////////////////////////////////////////////////////////

    def __str__(self):
        """
        Display a multi-line representation of this feature dictionary
        as an FVM (feature value matrix).
        """
        return '\n'.join(self._str(self._find_reentrances({}), {}))

    def _repr(self, reentrances, reentrance_ids):
        segments = []
        prefix = ''
        suffix = ''
--
            prefix = '(%s)%s' % (reentrance_ids[id(self)], prefix)
        return '%s[%s]%s' % (prefix, ', '.join(segments), suffix)

    def _str(self, reentrances, reentrance_ids):
        """
        :return: A list of lines composing a string representation of
            this feature dictionary.
--
    :see: ``FeatStruct`` for information about feature paths, reentrance,
        cyclic feature structures, mutability, freezing, and hashing.
    """
    def __init__(self, features=()):
        """
        Create a new feature list, with the specified features.

--
    #////////////////////////////////////////////////////////////
    _INDEX_ERROR = "Expected int or feature path.  Got %r."

    def __getitem__(self, name_or_path):
        if isinstance(name_or_path, integer_types):
            return list.__getitem__(self, name_or_path)
        elif isinstance(name_or_path, tuple):
--
        else:
            raise TypeError(self._INDEX_ERROR % name_or_path)

    def __delitem__(self, name_or_path):
        """If the feature with the given name or path exists, delete
        its value; otherwise, raise ``KeyError``."""
        if self._frozen: raise ValueError(_FROZEN_ERROR)
--
        else:
            raise TypeError(self._INDEX_ERROR % name_or_path)

    def __setitem__(self, name_or_path, value):
        """Set the value for the feature with the given name or path
        to ``value``.  If ``name_or_path`` is an invalid path, raise
        ``KeyError``."""
--
    #{ Copying
    ##////////////////////////////////////////////////////////////

    def __deepcopy__(self, memo):
        memo[id(self)] = selfcopy = self.__class__()
        selfcopy.extend(copy.deepcopy(fval,memo) for fval in self)
        return selfcopy
--
    #{ Uniform Accessor Methods
    ##////////////////////////////////////////////////////////////

    def _keys(self): return list(range(len(self)))
    def _values(self): return self
    def _items(self): return enumerate(self)

    ##////////////////////////////////////////////////////////////
    #{ String Representations
    ##////////////////////////////////////////////////////////////

    # Special handling for: reentrances, variables, expressions.
    def _repr(self, reentrances, reentrance_ids):
        # If this is the first time we've seen a reentrant structure,
        # then assign it a unique identifier.
        if reentrances[id(self)]:
--
# Variables & Bindings
######################################################################

def substitute_bindings(fstruct, bindings, fs_class='default'):
    """
    Return the feature structure that is obtained by replacing each
    variable bound by ``bindings`` with its binding.  If a variable is
--
    :type bindings: dict(Variable -> any)
    :param bindings: A dictionary mapping from variables to values.
    """
    if fs_class == 'default': fs_class = _default_fs_class(fstruct)
    fstruct = copy.deepcopy(fstruct)
    _substitute_bindings(fstruct, bindings, fs_class, set())
    return fstruct

def _substitute_bindings(fstruct, bindings, fs_class, visited):
    # Visit each node only once:
    if id(fstruct) in visited: return
    visited.add(id(fstruct))
--
        elif isinstance(fval, SubstituteBindingsI):
            fstruct[fname] = fval.substitute_bindings(bindings)

def retract_bindings(fstruct, bindings, fs_class='default'):
    """
    Return the feature structure that is obtained by replacing each
    feature structure value that is bound by ``bindings`` with the
--
    values in ``bindings`` may be modified if they are contained in
    ``fstruct``.
    """
    if fs_class == 'default': fs_class = _default_fs_class(fstruct)
    (fstruct, new_bindings) = copy.deepcopy((fstruct, bindings))
    bindings.update(new_bindings)
    inv_bindings = dict((id(val),var) for (var,val) in bindings.items())
    _retract_bindings(fstruct, inv_bindings, fs_class, set())
    return fstruct

def _retract_bindings(fstruct, inv_bindings, fs_class, visited):
    # Visit each node only once:
    if id(fstruct) in visited: return
    visited.add(id(fstruct))
--
            _retract_bindings(fval, inv_bindings, fs_class, visited)


def find_variables(fstruct, fs_class='default'):
    """
    :return: The set of variables used by this feature structure.
    :rtype: set(Variable)
    """
    if fs_class == 'default': fs_class = _default_fs_class(fstruct)
    return _variables(fstruct, set(), fs_class, set())

def _variables(fstruct, vars, fs_class, visited):
    # Visit each node only once:
    if id(fstruct) in visited: return
    visited.add(id(fstruct))
--
            vars.update(fval.variables())
    return vars

def rename_variables(fstruct, vars=None, used_vars=(), new_vars=None,
                     fs_class='default'):
    """
    Return the feature structure that is obtained by replacing
    any of this feature structure's variables that are in ``vars``
--

    If new_vars is not specified, then an empty dictionary is used.
    """
    if fs_class == 'default': fs_class = _default_fs_class(fstruct)

    # Default values:
    if new_vars is None: new_vars = {}
    if vars is None: vars = find_variables(fstruct, fs_class)
    else: vars = set(vars)
--
    return _rename_variables(copy.deepcopy(fstruct), vars, used_vars,
                             new_vars, fs_class, set())

def _rename_variables(fstruct, vars, used_vars, new_vars, fs_class, visited):
    if id(fstruct) in visited: return
    visited.add(id(fstruct))
    if _is_mapping(fstruct): items = fstruct.items()
--
            fstruct[fname] = fval.substitute_bindings(new_vars)
    return fstruct

def _rename_variable(var, used_vars):
    name, n = re.sub('\d+$', '', var.name), 2
    if not name: name = '?'
    while Variable('%s%s' % (name, n)) in used_vars: n += 1
    return Variable('%s%s' % (name, n))

def remove_variables(fstruct, fs_class='default'):
    """
    :rtype: FeatStruct
    :return: The feature structure that is obtained by deleting
        all features whose values are ``Variables``.
    """
    if fs_class == 'default': fs_class = _default_fs_class(fstruct)
    return _remove_variables(copy.deepcopy(fstruct), fs_class, set())

def _remove_variables(fstruct, fs_class, visited):
    if id(fstruct) in visited:
        return
    visited.add(id(fstruct))
--

@python_2_unicode_compatible
class _UnificationFailure(object):
    def __repr__(self):
        return 'nltk.featstruct.UnificationFailure'

UnificationFailure = _UnificationFailure()
--
#   2. Destructively unify self and other
#   3. Apply forward pointers, to preserve reentrance.
#   4. Replace bound variables with their values.
def unify(fstruct1, fstruct2, bindings=None, trace=False,
          fail=None, rename_vars=True, fs_class='default'):
    """
    Unify ``fstruct1`` with ``fstruct2``, and return the resulting feature
    structure.  This unified feature structure is the minimal
--
    ``bindings[v]`` is set to *x*.

    If ``bindings`` is unspecified, then all variables are
    assumed to be unbound.  I.e., ``bindings`` defaults to an
    empty dict.

        >>> from nltk.featstruct import FeatStruct
--
    """
    # Decide which class(es) will be treated as feature structures,
    # for the purposes of unification.
    if fs_class == 'default':
        fs_class = _default_fs_class(fstruct1)
        if _default_fs_class(fstruct2) != fs_class:
            raise ValueError("Mixing FeatStruct objects with Python "
                             "dicts and lists is not supported.")
    assert isinstance(fstruct1, fs_class)
--
    """An exception that is used by ``_destructively_unify`` to abort
    unification when a failure is encountered."""

def _destructively_unify(fstruct1, fstruct2, bindings, forward,
                         trace, fail, fs_class, path):
    """
    Attempt to unify ``fstruct1`` and ``fstruct2`` by modifying them
    in-place.  If the unification succeeds, then ``fstruct1`` will
    contain the unified value, the value of ``fstruct2`` is undefined,
    and forward[id(fstruct2)] is set to fstruct1.  If the unification
    fails, then a _UnificationFailureError is raised, and the
    values of ``fstruct1`` and ``fstruct2`` are undefined.

    :param bindings: A dictionary mapping variables to values.
    :param forward: A dictionary mapping feature structures ids
--
    # Unifying two mappings:
    if _is_mapping(fstruct1) and _is_mapping(fstruct2):
        for fname in fstruct1:
            if getattr(fname, 'default', None) is not None:
                fstruct2.setdefault(fname, fname.default)
        for fname in fstruct2:
            if getattr(fname, 'default', None) is not None:
                fstruct1.setdefault(fname, fname.default)

        # Unify any values that are defined in both fstruct1 and
        # fstruct2.  Copy any values that are defined in fstruct2 but
        # not in fstruct1 to fstruct1.  Note: sorting fstruct2's
        # features isn't actually necessary; but we do it to give
        # deterministic behavior, e.g. for tracing.
--
    # Unifying anything else: not allowed!
    raise TypeError('Expected mappings or sequences')

def _unify_feature_values(fname, fval1, fval2, bindings, forward,
                          trace, fail, fs_class, fpath):
    """
    Attempt to unify ``fval1`` and and ``fval2``, and return the
--
         then bind the unbound variable to the value.
      4. If one is a feature structure, and the other is a base value,
         then fail.
      5. If they're both base values, then unify them.  By default,
         this will succeed if they are equal, and fail otherwise.
    """
    if trace: _trace_unify_start(fpath, fval1, fval2)
--

    # Case 5: Two base values
    else:
        # Case 5a: Feature defines a custom unification method for base values
        if isinstance(fname, Feature):
            result = fname.unify_base_values(fval1, fval2, bindings)
        # Case 5b: Feature value defines custom unification method
        elif isinstance(fval1, CustomFeatureValue):
            result = fval1.unify(fval2)
            # Sanity check: unify value should be symmetric
--

    return result

def _apply_forwards_to_bindings(forward, bindings):
    """
    Replace any feature structure that has a forward pointer with
    the target of its forward pointer (to preserve reentrancy).
--
            value = forward[id(value)]
        bindings[var] = value

def _apply_forwards(fstruct, forward, fs_class, visited):
    """
    Replace any feature structure that has a forward pointer with
    the target of its forward pointer (to preserve reentrancy).
--

    return fstruct

def _resolve_aliases(bindings):
    """
    Replace any bound aliased vars with their binding; and replace
    any unbound aliased vars with their representative var.
--
        while isinstance(value, Variable) and value in bindings:
            value = bindings[var] = bindings[value]

def _trace_unify_start(path, fval1, fval2):
    if path == ():
        print('\nUnification trace:')
    else:
--
        print('  '+'|   '*(len(path)-1)+'| Unify feature: %s' % fullname)
    print('  '+'|   '*len(path)+' / '+_trace_valrepr(fval1))
    print('  '+'|   '*len(path)+'|\\ '+_trace_valrepr(fval2))
def _trace_unify_identity(path, fval1):
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'| (identical objects)')
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'+-->'+unicode_repr(fval1))
def _trace_unify_fail(path, result):
    if result is UnificationFailure: resume = ''
    else: resume = ' (nonfatal)'
    print('  '+'|   '*len(path)+'|   |')
    print('  '+'X   '*len(path)+'X   X <-- FAIL'+resume)
def _trace_unify_succeed(path, fval1):
    # Print the result.
    print('  '+'|   '*len(path)+'|')
    print('  '+'|   '*len(path)+'+-->'+unicode_repr(fval1))
def _trace_bindings(path, bindings):
    # Print the bindings (if any).
    if len(bindings) > 0:
        binditems = sorted(bindings.items(), key=lambda v:v[0].name)
--
            '%s: %s' % (var, _trace_valrepr(val))
            for (var, val) in binditems)
        print('  '+'|   '*len(path)+'    Bindings: '+bindstr)
def _trace_valrepr(val):
    if isinstance(val, Variable):
        return '%s' % val
    else:
        return '%s' % unicode_repr(val)

def subsumes(fstruct1, fstruct2):
    """
    Return True if ``fstruct1`` subsumes ``fstruct2``.  I.e., return
    true if unifying ``fstruct1`` with ``fstruct2`` would result in a
--
    """
    return fstruct2 == unify(fstruct1, fstruct2)

def conflicts(fstruct1, fstruct2, trace=0):
    """
    Return a list of the feature paths of all features which are
    assigned incompatible values by ``fstruct1`` and ``fstruct2``.
--
    :rtype: list(tuple)
    """
    conflict_list = []
    def add_conflict(fval1, fval2, path):
        conflict_list.append(path)
        return fval1
    unify(fstruct1, fstruct2, fail=add_conflict, trace=trace)
--
# Helper Functions
######################################################################

def _is_mapping(v):
    return hasattr(v, '__contains__') and hasattr(v, 'keys')

def _is_sequence(v):
    return (hasattr(v, '__iter__') and hasattr(v, '__len__') and
            not isinstance(v, string_types))

def _default_fs_class(obj):
    if isinstance(obj, FeatStruct): return FeatStruct
    if isinstance(obj, (dict, list)): return (dict, list)
    else:
--
    A mixin class for sequence clases that distributes variables() and
    substitute_bindings() over the object's elements.
    """
    def variables(self):
        return ([elt for elt in self if isinstance(elt, Variable)] +
                sum([list(elt.variables()) for elt in self
                     if isinstance(elt, SubstituteBindingsI)], []))

    def substitute_bindings(self, bindings):
        return self.__class__([self.subst(v, bindings) for v in self])

    def subst(self, v, bindings):
        if isinstance(v, SubstituteBindingsI):
            return v.substitute_bindings(bindings)
        else:
--
    variable substitutions will be propagated to the elements
    contained by the set.  A ``FeatureValueTuple`` is immutable.
    """
    def __repr__(self): # [xx] really use %s here?
        if len(self) == 0: return '()'
        return '(%s)' % ', '.join('%s' % (b,) for b in self)

--
    variable substitutions will be propagated to the elements
    contained by the set.  A ``FeatureValueSet`` is immutable.
    """
    def __repr__(self): # [xx] really use %s here?
        if len(self) == 0: return '{/}' # distinguish from dict.
        # n.b., we sort the string reprs of our elements, to ensure
        # that our own repr is deterministic.
--
    A base feature value that represents the union of two or more
    ``FeatureValueSet`` or ``Variable``.
    """
    def __new__(cls, values):
        # If values contains FeatureValueUnions, then collapse them.
        values = _flatten(values, FeatureValueUnion)

--
        # Otherwise, build the FeatureValueUnion.
        return frozenset.__new__(cls, values)

    def __repr__(self):
        # n.b., we sort the string reprs of our elements, to ensure
        # that our own repr is deterministic.  also, note that len(self)
        # is guaranteed to be 2 or more.
--
    A base feature value that represents the concatenation of two or
    more ``FeatureValueTuple`` or ``Variable``.
    """
    def __new__(cls, values):
        # If values contains FeatureValueConcats, then collapse them.
        values = _flatten(values, FeatureValueConcat)

--
        # Otherwise, build the FeatureValueConcat.
        return tuple.__new__(cls, values)

    def __repr__(self):
        # n.b.: len(self) is guaranteed to be 2 or more.
        return '(%s)' % '+'.join('%s' % (b,) for b in self)


def _flatten(lst, cls):
    """
    Helper function -- return a copy of list, with all elements of
    type ``cls`` spliced in rather than appended in.
--
class Feature(object):
    """
    A feature identifier that's specialized to put additional
    constraints, default values, etc.
    """
    def __init__(self, name, default=None, display=None):
        assert display in (None, 'prefix', 'slash')

        self._name = name # [xx] rename to .identifier?
        self._default = default # [xx] not implemented yet.
        self._display = display

        if self._display == 'prefix':
--
            self._sortkey = (0, self._name)

    @property
    def name(self):
        """The name of this feature."""
        return self._name

    @property
    def default(self):
        """Default value for this feature."""
        return self._default

    @property
    def display(self):
        """Custom display location: can be prefix, or slash."""
        return self._display

    def __repr__(self):
        return '*%s*' % self.name

    def __lt__(self, other):
        if isinstance(other, string_types):
            return True
        if not isinstance(other, Feature):
            raise_unorderable_types("<", self, other)
        return self._sortkey < other._sortkey

    def __eq__(self, other):
        return type(self) == type(other) and self._name == other._name

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash(self._name)

    #////////////////////////////////////////////////////////////
    # These can be overridden by subclasses:
    #////////////////////////////////////////////////////////////

    def parse_value(self, s, position, reentrances, parser):
        return parser.parse_value(s, position, reentrances)

    def unify_base_values(self, fval1, fval2, bindings):
        """
        If possible, return a single value..  If not, return
        the value ``UnificationFailure``.
--


class SlashFeature(Feature):
    def parse_value(self, s, position, reentrances, parser):
        return parser.partial_parse(s, position, reentrances)

class RangeFeature(Feature):
    RANGE_RE = re.compile('(-?\d+):(-?\d+)')
    def parse_value(self, s, position, reentrances, parser):
        m = self.RANGE_RE.match(s, position)
        if not m: raise ValueError('range', position)
        return (int(m.group(1)), int(m.group(2))), m.end()

    def unify_base_values(self, fval1, fval2, bindings):
        if fval1 is None: return fval2
        if fval2 is None: return fval1
        rng = max(fval1[0], fval2[0]), min(fval1[1], fval2[1])
        if rng[1] < rng[0]: return UnificationFailure
        return rng

SLASH = SlashFeature('slash', default=False, display='slash')
TYPE = Feature('type', display='prefix')

######################################################################
--
@total_ordering
class CustomFeatureValue(object):
    """
    An abstract base class for base values that define a custom
    unification method.  The custom unification method of
    ``CustomFeatureValue`` will be used during unification if:

      - The ``CustomFeatureValue`` is unified with another base value.
      - The ``CustomFeatureValue`` is not the value of a customized
        ``Feature`` (which defines its own unification method).

    If two ``CustomFeatureValue`` objects are unified with one another
    during feature structure unification, then the unified base values
    they return *must* be equal; otherwise, an ``AssertionError`` will
    be raised.

    Subclasses must define ``unify()``, ``__eq__()`` and ``__lt__()``.
    Subclasses may also wish to define ``__hash__()``.
    """
    def unify(self, other):
        """
        If this base value unifies with ``other``, then return the
        unified value.  Otherwise, return ``UnificationFailure``.
        """
        raise NotImplementedError('abstract base class')

    def __eq__(self, other):
        raise NotImplementedError('abstract base class')

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        raise NotImplementedError('abstract base class')

    def __hash__(self):
        raise TypeError('%s objects or unhashable' % self.__class__.__name__)

######################################################################
--
######################################################################

class FeatStructParser(object):
    def __init__(self, features=(SLASH, TYPE), fdict_class=FeatStruct,
                 flist_class=FeatList, logic_parser=None):
        self._features = dict((f.name,f) for f in features)
        self._fdict_class = fdict_class
--
                if self._prefix_feature:
                    raise ValueError('Multiple features w/ display=prefix')
                self._prefix_feature = feature
        self._features_with_defaults = [feature for feature in features
                                        if feature.default is not None]
        if logic_parser is None:
            logic_parser = LogicParser()
        self._logic_parser = logic_parser

    def parse(self, s, fstruct=None):
        """
        Convert a string representation of a feature structure (as
        displayed by repr) into a ``FeatStruct``.  This parse
--
        _BARE_PREFIX_RE.pattern, _START_FSTRUCT_RE.pattern,
        _FEATURE_NAME_RE.pattern, _FEATURE_NAME_RE.pattern))

    def partial_parse(self, s, position=0, reentrances=None, fstruct=None):
        """
        Helper function that parses a feature structure.

        :param s: The string to parse.
        :param position: The position in the string to start parsing.
        :param reentrances: A dictionary from reentrance ids to values.
            Defaults to an empty dictionary.
        :return: A tuple (val, pos) of the feature structure created by
            parsing and the position where the parsed feature structure ends.
        :rtype: bool
--
            if len(e.args) != 2: raise
            self._error(s, *e.args)

    def _partial_parse(self, s, position, reentrances, fstruct=None):
        # Create the new feature structure
        if fstruct is None:
            if self._START_FDICT_RE.match(s, position):
--
            return self._partial_parse_featlist(s, position, match,
                                                reentrances, fstruct)

    def _partial_parse_featlist(self, s, position, match,
                                reentrances, fstruct):
        # Prefix features are not allowed:
        if match.group(2): raise ValueError('open bracket')
        # Bare prefixes are not allowed:
        if not match.group(3): raise ValueError('open bracket')

        # Build a list of the features defined by the structure.
        while position < len(s):
            # Check for the close bracket.
            match = self._END_FSTRUCT_RE.match(s, position)
--
        # We never saw a close bracket.
        raise ValueError('close bracket', position)

    def _partial_parse_featdict(self, s, position, match,
                                reentrances, fstruct):
        # If there was a prefix feature, record it.
        if match.group(2):
--
        if not match.group(3):
            return self._finalize(s, match.end(), reentrances, fstruct)

        # Build a list of the features defined by the structure.
        # Each feature has one of the three following forms:
        #     name = value
        #     name -> (target)
--
        # We never saw a close bracket.
        raise ValueError('close bracket', position)

    def _finalize(self, s, pos, reentrances, fstruct):
        """
        Called when we see the close brace -- checks for a slash feature,
        and adds in default values.
        """
        # Add the slash feature (if any)
        match = self._SLASH_RE.match(s, pos)
--
            name = self._slash_feature
            v, pos = self._parse_value(name, s, match.end(), reentrances)
            fstruct[name] = v
        ## Add any default features.  -- handle in unficiation instead?
        #for feature in self._features_with_defaults:
        #    fstruct.setdefault(feature, feature.default)
        # Return the value.
        return fstruct, pos

    def _parse_value(self, name, s, position, reentrances):
        if isinstance(name, Feature):
            return name.parse_value(s, position, reentrances, self)
        else:
            return self.parse_value(s, position, reentrances)

    def parse_value(self, s, position, reentrances):
        for (handler, regexp) in self.VALUE_HANDLERS:
            match = regexp.match(s, position)
            if match:
--
                return handler_func(s, position, reentrances, match)
        raise ValueError('value', position)

    def _error(self, s, expected, position):
        lines = s.split('\n')
        while position > len(lines[0]):
            position -= len(lines.pop(0))+1 # +1 for the newline.
--
    #: with a matching regexp will have its handler called.  Handlers
    #: should have the following signature::
    #:
    #:    def handler(s, position, reentrances, match): ...
    #:
    #: and should return a tuple (value, position), where position is
    #: the string position where the value ended.  (n.b.: order is
--
        ('parse_tuple_value', re.compile(r'\(')),
        ]

    def parse_fstruct_value(self, s, position, reentrances, match):
        return self.partial_parse(s, position, reentrances)

    def parse_str_value(self, s, position, reentrances, match):
        return parse_str(s, position)

    def parse_int_value(self, s, position, reentrances, match):
        return int(match.group()), match.end()

    # Note: the '?' is included in the variable name.
    def parse_var_value(self, s, position, reentrances, match):
        return Variable(match.group()), match.end()

    _SYM_CONSTS = {'None':None, 'True':True, 'False':False}
    def parse_sym_value(self, s, position, reentrances, match):
        val, end = match.group(), match.end()
        return self._SYM_CONSTS.get(val, val), end

    def parse_app_value(self, s, position, reentrances, match):
        """Mainly included for backwards compat."""
        return self._logic_parser.parse('%s(%s)' % match.group(2,3)), match.end()

    def parse_logic_value(self, s, position, reentrances, match):
        try:
            try:
                expr = self._logic_parser.parse(match.group(1))
--
        except ValueError:
            raise ValueError('logic expression', match.start(1))

    def parse_tuple_value(self, s, position, reentrances, match):
        return self._parse_seq_value(s, position, reentrances, match, ')',
                                     FeatureValueTuple, FeatureValueConcat)

    def parse_set_value(self, s, position, reentrances, match):
        return self._parse_seq_value(s, position, reentrances, match, '}',
                                     FeatureValueSet, FeatureValueUnion)

    def _parse_seq_value(self, s, position, reentrances, match,
                         close_paren, seq_class, plus_class):
        """
        Helper function used by parse_tuple_value and parse_set_value.
--
#{ Demo
######################################################################

def display_unification(fs1, fs2, indent='  '):
    # Print the two input feature structures, side by side.
    fs1_lines = ("%s" % fs1).split('\n')
    fs2_lines = ("%s" % fs2).split('\n')
--
            print(repr(bindings).center(linelen))
    return result

def interactive_demo(trace=False):
    import random, sys

    HELP = '''
--
    all_fstructs = [(i, FeatStruct(fstruct_strings[i]))
                    for i in range(len(fstruct_strings))]

    def list_fstructs(fstructs):
        for i, fstruct in fstructs:
            print()
            lines = ("%s" % fstruct).split('\n')
--
        input = sys.stdin.readline().strip()
        if input in ('q', 'Q', 'x', 'X'): return

def demo(trace=False):
    """
    Just for testing
    """
--
To convert a chunk structure back to a list of tokens, simply use the
chunk structure's ``leaves()`` method.

This module defines ``ChunkParserI``, a standard interface for
chunking texts; and ``RegexpChunkParser``, a regular-expression based
implementation of that interface. It also defines ``ChunkScore``, a
utility class for scoring chunk parsers.

RegexpChunkParser
--

A ``RegexpChunkRule`` is a transformational rule that updates the
chunking of a text by modifying its ``ChunkString``.  Each
``RegexpChunkRule`` defines the ``apply()`` method, which modifies
the chunking encoded by a ``ChunkString``.  The
``RegexpChunkRule`` class itself can be used to implement any
transformational rule based on regular expressions.  There are
--
with emacs or xemacs ("C-c !")::

    (let ()
      (defconst comint-mode-font-lock-keywords
        '(("<[^>]+>" 0 'font-lock-reference-face)
          ("[{}]" 0 'font-lock-function-name-face)))
      (add-hook 'comint-mode-hook (lambda () (turn-on-font-lock))))
--
_BINARY_NE_CHUNKER = 'chunkers/maxent_ne_chunker/english_ace_binary.pickle'
_MULTICLASS_NE_CHUNKER = 'chunkers/maxent_ne_chunker/english_ace_multiclass.pickle'

def ne_chunk(tagged_tokens, binary=False):
    """
    Use NLTK's currently recommended named entity chunker to
    chunk the given list of tagged tokens.
--
    chunker = load(chunker_pickle)
    return chunker.parse(tagged_tokens)

def batch_ne_chunk(tagged_sentences, binary=False):
    """
    Use NLTK's currently recommended named entity chunker to chunk the
    given list of tagged sentences, each consisting of a list of tagged tokens.
--
    """
    The IOB tagger used by the chunk parser.
    """
    def __init__(self, train):
        ClassifierBasedTagger.__init__(
            self, train=train,
            classifier_builder=self._classifier_builder)

    def _classifier_builder(self, train):
        return MaxentClassifier.train(train, algorithm='megam',
                                           gaussian_prior_sigma=1,
                                           trace=2)

    def _english_wordlist(self):
        try:
            wl = self._en_wordlist
        except AttributeError:
--
            wl = self._en_wordlist
        return wl

    def _feature_detector(self, tokens, index, history):
        word = tokens[index][0]
        pos = simplify_pos(tokens[index][1])
        if index == 0:
--
    """
    Expected input: list of pos-tagged words
    """
    def __init__(self, train):
        self._train(train)

    def parse(self, tokens):
        """
        Each token should be a pos-tagged word
        """
--
        tree = self._tagged_to_parse(tagged)
        return tree

    def _train(self, corpus):
        # Convert to tagged sequence
        corpus = [self._parse_to_tagged(s) for s in corpus]

        self._tagger = NEChunkParserTagger(train=corpus)

    def _tagged_to_parse(self, tagged_tokens):
        """
        Convert a list of tagged tokens to a chunk-parse tree.
        """
--
        return sent

    @staticmethod
    def _parse_to_tagged(sent):
        """
        Convert a chunk-parse tree to a list of tagged tokens.
        """
--
                toks.append((child, 'O'))
        return toks

def shape(word):
    if re.match('[0-9]+(\.[0-9]*)?|[0-9]*\.[0-9]+$', word):
        return 'number'
    elif re.match('\W+$', word):
--
    else:
        return 'other'

def simplify_pos(s):
    if s.startswith('V'): return "V"
    else: return s.split('-')[0]

def postag_tree(tree):
    # Part-of-speech tagging.
    words = tree.leaves()
    tag_iter = (pos for (word, pos) in pos_tag(words))
--
            newtree.append( (child, next(tag_iter)) )
    return newtree

def load_ace_data(roots, fmt='binary', skip_bnews=True):
    for root in roots:
        for root, dirs, files in os.walk(root):
            if root.endswith('bnews') and skip_bnews:
--
                    for sent in load_ace_file(os.path.join(root, f), fmt):
                        yield sent

def load_ace_file(textfile, fmt):
    print('  - %s' % os.path.split(textfile)[1])
    annfile = textfile+'.tmx.rdc.xml'

--
    text = re.sub('<(?!/?TEXT)[^>]+>', '', text)

    # Blank out anything before/after <TEXT>
    def subfunc(m): return ' '*(m.end()-m.start()-6)
    text = re.sub('[\s\S]*<TEXT>', subfunc, text)
    text = re.sub('</TEXT>[\s\S]*', '', text)

--

# This probably belongs in a more general-purpose location (as does
# the parse_to_tagged function).
def cmp_chunks(correct, guessed):
    correct = NEChunkParser._parse_to_tagged(correct)
    guessed = NEChunkParser._parse_to_tagged(guessed)
    ellipsis = False
--
            ellipsis = False
            print("  %-15s %-15s %s" % (ct, gt, w))

def build_model(fmt='binary'):
    print('Loading training data...')
    train_paths = [find('corpora/ace_data/ace.dev'),
                   find('corpora/ace_data/ace.heldout'),
--
##//////////////////////////////////////////////////////

from nltk.metrics import accuracy as _accuracy
def accuracy(chunker, gold):
    """
    Score the accuracy of the chunker against the gold standard.
    Strip the chunk information from the gold standard and rechunk it using
--
          numerical metrics (precision, recall, or f-measure)

        - chunk_node: A regular expression indicating which chunks
          should be compared.  Defaults to ``'.*'`` (i.e., all chunks).

    :type _tp: list(Token)
    :ivar _tp: List of true positives
--
    :type _fn_num: int
    :ivar _fn_num: Number of false negatives.
    """
    def __init__(self, **kwargs):
        self._correct = set()
        self._guessed = set()
        self._tp = set()
--

        self._measuresNeedUpdate = False

    def _updateMeasures(self):
        if (self._measuresNeedUpdate):
           self._tp = self._guessed & self._correct
           self._fn = self._correct - self._guessed
--
           self._fn_num = len(self._fn)
           self._measuresNeedUpdate = False

    def score(self, correct, guessed):
        """
        Given a correctly chunked sentence, score another chunked
        version of the same sentence.
--
                                                     correct_tags)
                                  if t==g)

    def accuracy(self):
        """
        Return the overall tag-based accuracy for all text that have
        been scored by this ``ChunkScore``, using the IOB (conll2000)
--
        if self._tags_total == 0: return 1
        return self._tags_correct/self._tags_total

    def precision(self):
        """
        Return the overall precision for all texts that have been
        scored by this ``ChunkScore``.
--
        if div == 0: return 0
        else: return float(self._tp_num) / div

    def recall(self):
        """
        Return the overall recall for all texts that have been
        scored by this ``ChunkScore``.
--
        if div == 0: return 0
        else: return float(self._tp_num) / div

    def f_measure(self, alpha=0.5):
        """
        Return the overall F measure for all texts that have been
        scored by this ``ChunkScore``.
--
            return 0
        return 1/(alpha/p + (1-alpha)/r)

    def missed(self):
        """
        Return the chunks which were included in the
        correct chunk structures, but not in the guessed chunk
--
        chunks = list(self._fn)
        return [c[1] for c in chunks]  # discard position information

    def incorrect(self):
        """
        Return the chunks which were included in the guessed chunk structures,
        but not in the correct chunk structures, listed in input order.
--
        chunks = list(self._fp)
        return [c[1] for c in chunks]  # discard position information

    def correct(self):
        """
        Return the chunks which were included in the correct
        chunk structures, listed in input order.
--
        chunks = list(self._correct)
        return [c[1] for c in chunks]  # discard position information

    def guessed(self):
        """
        Return the chunks which were included in the guessed
        chunk structures, listed in input order.
--
        chunks = list(self._guessed)
        return [c[1] for c in chunks]  # discard position information

    def __len__(self):
        self._updateMeasures()
        return self._tp_num + self._fn_num

    def __repr__(self):
        """
        Return a concise representation of this ``ChunkScoring``.

--
        """
        return '<ChunkScoring of '+repr(len(self))+' chunks>'

    def __str__(self):
        """
        Return a verbose representation of this ``ChunkScoring``.
        This representation includes the precision, recall, and
--

# extract chunks, and assign unique id, the absolute position of
# the first word of the chunk
def _chunksets(t, count, chunk_node):
    pos = 0
    chunks = []
    for child in t:
--
    return set(chunks)


def tagstr2tree(s, chunk_node="NP", top_node="S", sep='/'):
    """
    Divide a string of bracketted tagged text into
    chunks and unchunked tokens, and produce a Tree.
--
### CONLL

_LINE_RE = re.compile('(\S+)\s+(\S+)\s+([IOB])-?(\S+)?')
def conllstr2tree(s, chunk_types=('NP', 'PP', 'VP'), top_node="S"):
    """
    Return a chunk structure for a single sentence
    encoded in the given CONLL 2000 style string.
    This function converts a CoNLL IOB string into a tree.
    It uses the specified chunk types
    (defaults to NP, PP and VP), and creates a tree rooted at a node
    labeled S (by default).

    :param s: The CoNLL string to be converted.
    :type s: str
--

    return stack[0]

def tree2conlltags(t):
    """
    Return a list of 3-tuples containing ``(word, tag, IOB-tag)``.
    Convert a tree to the CoNLL IOB tag format.
--
            tags.append((child[0], child[1], "O"))
    return tags

def conlltags2tree(sentence, chunk_types=('NP','PP','VP'),
                   top_node='S', strict=False):
    """
    Convert the CoNLL IOB format to a tree.
--
            raise ValueError("Bad conll tag %r" % chunktag)
    return tree

def tree2conllstr(t):
    """
    Return a multiline string where each line contains a word, tag and IOB tag.
    Convert a tree to the CoNLL IOB string format
--

_IEER_TYPE_RE = re.compile('<b_\w+\s+[^>]*?type="(?P<type>\w+)"')

def _ieer_read_text(s, top_node):
    stack = [Tree(top_node, [])]
    # s will be None if there is no headline in the text
    # return the empty list in place of a Tree
--
        raise ValueError('Bad IEER string')
    return stack[0]

def ieerstr2tree(s, chunk_types = ['LOCATION', 'ORGANIZATION', 'PERSON', 'DURATION',
               'DATE', 'CARDINAL', 'PERCENT', 'MONEY', 'MEASURE'], top_node="S"):
    """
    Return a chunk structure containing the chunked tagged text that is
--
        return _ieer_read_text(s, top_node)


def demo():

    s = "[ Pierre/NNP Vinken/NNP ] ,/, [ 61/CD years/NNS ] old/JJ ,/, will/MD join/VB [ the/DT board/NN ] ./."
    import nltk
--
    _BRACKETS = re.compile('[^\{\}]+')
    _BALANCED_BRACKETS = re.compile(r'(\{\})*$')

    def __init__(self, chunk_struct, debug_level=1):
        """
        Construct a new ``ChunkString`` that encodes the chunking of
        the text ``tagged_tokens``.
--
        self._str = '<' + '><'.join(tags) + '>'
        self._debug = debug_level

    def _tag(self, tok):
        if isinstance(tok, tuple):
            return tok[1]
        elif isinstance(tok, Tree):
--
            raise ValueError('chunk structures must contain tagged '
                             'tokens or trees')

    def _verify(self, s, verify_tags):
        """
        Check to make sure that ``s`` still corresponds to some chunked
        version of ``_pieces``.
--
            raise ValueError('Transformation generated invalid '
                             'chunkstring: tag changed')

    def to_chunkstruct(self, chunk_node='CHUNK'):
        """
        Return the chunk structure encoded by this ``ChunkString``.

--

        return Tree(self._top_node, pieces)

    def xform(self, regexp, repl):
        """
        Apply the given transformation to the string encoding of this
        ``ChunkString``.  In particular, find all occurrences that match
--
        # Commit the transformation.
        self._str = s

    def __repr__(self):
        """
        Return a string representation of this ``ChunkString``.
        It has the form::
--
        """
        return '<ChunkString: %s>' % unicode_repr(self._str)

    def __str__(self):
        """
        Return a formatted representation of this ``ChunkString``.
        This representation will include extra spaces to ensure that
--
    gives a short (typically less than 75 characters) description of
    the purpose of the rule.

    This transformation defined by this ``RegexpChunkRule`` should
    only add and remove braces; it should *not* modify the sequence
    of angle-bracket delimited tags.  Furthermore, this transformation
    may not result in nested or mismatched bracketing.
    """
    def __init__(self, regexp, repl, descr):
        """
        Construct a new RegexpChunkRule.

--
        self._descr = descr
        self._regexp = regexp

    def apply(self, chunkstr):
        # Keep docstring generic so we can inherit it.
        """
        Apply this rule to the given ``ChunkString``.  See the
--
        """
        chunkstr.xform(self._regexp, self._repl)

    def descr(self):
        """
        Return a short description of the purpose and/or effect of
        this rule.
--
        """
        return self._descr

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
                '->'+unicode_repr(self._repl)+'>')

    @staticmethod
    def parse(s):
        """
        Create a RegexpChunkRule from a string description.
        Currently, the following formats are supported::
--
    already part of a chunk, and create a new chunk containing that
    substring.
    """
    def __init__(self, tag_pattern, descr):

        """
        Construct a new ``ChunkRule``.
--
                             ChunkString.IN_CHINK_PATTERN))
        RegexpChunkRule.__init__(self, regexp, '{\g<chunk>}', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    tag pattern and that is contained in a chunk, and remove it
    from that chunk, thus creating two new chunks.
    """
    def __init__(self, tag_pattern, descr):
        """
        Construct a new ``ChinkRule``.

--
                             ChunkString.IN_CHUNK_PATTERN))
        RegexpChunkRule.__init__(self, regexp, '}\g<chink>{', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    ``ChunkString``, it will find any complete chunk that matches this
    tag pattern, and un-chunk it.
    """
    def __init__(self, tag_pattern, descr):
        """
        Construct a new ``UnChunkRule``.

--
                            tag_pattern2re_pattern(tag_pattern))
        RegexpChunkRule.__init__(self, regexp, '\g<chunk>', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    beginning matches right pattern.  It will then merge those two
    chunks into a single chunk.
    """
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        """
        Construct a new ``MergeRule``.

--
                             tag_pattern2re_pattern(right_tag_pattern)))
        RegexpChunkRule.__init__(self, regexp, '\g<left>', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    then split the chunk into two new chunks, at the point between the
    two pattern matches.
    """
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        """
        Construct a new ``SplitRule``.

--
                             tag_pattern2re_pattern(right_tag_pattern)))
        RegexpChunkRule.__init__(self, regexp, r'\g<left>}{', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    end matches left pattern.  It will then expand the chunk to incorporate
    the new material on the left.
    """
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        """
        Construct a new ``ExpandRightRule``.

--
                             tag_pattern2re_pattern(right_tag_pattern)))
        RegexpChunkRule.__init__(self, regexp, '{\g<left>\g<right>', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    a chink whose beginning matches right pattern.  It will then
    expand the chunk to incorporate the new material on the right.
    """
    def __init__(self, left_tag_pattern, right_tag_pattern, descr):
        """
        Construct a new ``ExpandRightRule``.

--
                             tag_pattern2re_pattern(right_tag_pattern)))
        RegexpChunkRule.__init__(self, regexp, '\g<left>\g<right>}', descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
    rule matches; therefore, if you need to find overlapping matches,
    you will need to apply your rule more than once.
    """
    def __init__(self, left_context_tag_pattern, chunk_tag_pattern,
                 right_context_tag_pattern, descr):
        """
        Construct a new ``ChunkRuleWithContext``.
--
        replacement = r'\g<left>{\g<chunk>}\g<right>'
        RegexpChunkRule.__init__(self, regexp, replacement, descr)

    def __repr__(self):
        """
        Return a string representation of this rule.  It has the form::

--
                                ('[^\{\}<>]+',
                                 '[^\{\}<>]+'))

def tag_pattern2re_pattern(tag_pattern):
    """
    Convert a tag pattern to a regular expression pattern.  A "tag
    pattern" is a modified version of a regular expression, designed
--
    # PRE doesn't have lookback assertions, so reverse twice, and do
    # the pattern backwards (with lookahead assertions).  This can be
    # made much cleaner once we can switch back to SRE.
    def reverse_str(str):
        lst = list(str)
        lst.reverse()
        return ''.join(lst)
--

    The ``RegexpChunkRule`` class and its subclasses (``ChunkRule``,
    ``ChinkRule``, ``UnChunkRule``, ``MergeRule``, and ``SplitRule``)
    define the rules that are used by ``RegexpChunkParser``.  Each rule
    defines an ``apply()`` method, which modifies the chunking encoded
    by a given ``ChunkString``.

    :type _rules: list(RegexpChunkRule)
    :ivar _rules: The list of rules that should be applied to a text.
    :type _trace: int
    :ivar _trace: The default level of tracing.

    """
    def __init__(self, rules, chunk_node='NP', top_node='S', trace=0):
        """
        Construct a new ``RegexpChunkParser``.

--
        self._chunk_node = chunk_node
        self._top_node = top_node

    def _trace_apply(self, chunkstr, verbose):
        """
        Apply each rule of this ``RegexpChunkParser`` to ``chunkstr``, in
        turn.  Generate trace output between each rule.  If ``verbose``
--
                print('#', rule.descr()+':')
            print(chunkstr)

    def _notrace_apply(self, chunkstr):
        """
        Apply each rule of this ``RegexpChunkParser`` to ``chunkstr``, in
        turn.
--
        for rule in self._rules:
            rule.apply(chunkstr)

    def parse(self, chunk_struct, trace=None):
        """
        :type chunk_struct: Tree
        :param chunk_struct: the chunk structure to be (further) chunked
--
            tagged sentence.  A chunk is a non-overlapping linguistic
            group, such as a noun phrase.  The set of chunks
            identified in the chunk structure depends on the rules
            used to define this ``RegexpChunkParser``.
        """
        if len(chunk_struct) == 0:
            print('Warning: parsing empty text')
--
        except AttributeError:
            chunk_struct = Tree(self._top_node, chunk_struct)

        # Use the default trace value?
        if trace is None: trace = self._trace

        chunkstr = ChunkString(chunk_struct)
--
        # Use the chunkstring to create a chunk structure.
        return chunkstr.to_chunkstruct(self._chunk_node)

    def rules(self):
        """
        :return: the sequence of rules used by ``RegexpChunkParser``.
        :rtype: list(RegexpChunkRule)
        """
        return self._rules

    def __repr__(self):
        """
        :return: a concise string representation of this
            ``RegexpChunkParser``.
--
        """
        return "<RegexpChunkParser with %d rules>" % len(self._rules)

    def __str__(self):
        """
        :return: a verbose string representation of this ``RegexpChunkParser``.
        :rtype: str
--
    :ivar _stages: The list of parsing stages corresponding to the grammar

    """
    def __init__(self, grammar, top_node='S', loop=1, trace=0):
        """
        Create a new chunk parser, from the given start state
        and set of chunk patterns.
--
                    raise TypeError(type_err)
            self._stages = grammar

    def _parse_grammar(self, grammar, top_node, trace):
        """
        Helper function for __init__: parse the grammar if it is a
        string.
--
        # Record the final stage
        self._add_stage(rules, lhs, top_node, trace)

    def _add_stage(self, rules, lhs, top_node, trace):
        """
        Helper function for __init__: add a new stage to the parser.
        """
--
                                       top_node=top_node, trace=trace)
            self._stages.append(parser)

    def parse(self, chunk_struct, trace=None):
        """
        Apply the chunk parser to this input.

--
                chunk_struct = parser.parse(chunk_struct, trace=trace)
        return chunk_struct

    def __repr__(self):
        """
        :return: a concise string representation of this ``chunk.RegexpParser``.
        :rtype: str
        """
        return "<chunk.RegexpParser with %d stages>" % len(self._stages)

    def __str__(self):
        """
        :return: a verbose string representation of this
            ``RegexpParser``.
--
##  Demonstration code
##//////////////////////////////////////////////////////

def demo_eval(chunkparser, text):
    """
    Demonstration code for evaluating a chunk parser, using a
    ``ChunkScore``.  This function assumes that ``text`` contains one
--
    print('\\'+('='*75)+'/')
    print()

def demo():
    """
    A demonstration for the ``RegexpChunkParser`` class.  A single text is
    parsed with four different chunk parsers, using a variety of rules
--
    ``ParserI``, ``ChunkParserI`` guarantees that the ``parse()`` method
    will always generate a parse.
    """
    def parse(self, tokens):
        """
        Return the best chunk structure for the given tokens
        and return a tree.
--
        """
        raise NotImplementedError()

    def evaluate(self, gold):
        """
        Score the accuracy of the chunker against the gold standard.
        Remove the chunking the gold standard text, rechunk it using
--
        >>> len(t)
        2

    Any other properties that a Tree defines are known as node
    properties, and are used to add information about individual
    hierarchical groupings.  For example, syntax trees use a NODE
    property to label syntactic constituents with phrase tags, such as
    "NP" and "VP".

    Several Tree methods use "tree positions" to specify
    children or descendants of a tree.  Tree positions are defined as
    follows:

      - The tree position *i* specifies a Tree's *i*\ th child.
--
    - ``Tree(s)`` constructs a new tree by parsing the string ``s``.
        It is equivalent to calling the class method ``Tree.parse(s)``.
    """
    def __init__(self, node_or_str, children=None):
        if children is None:
            if not isinstance(node_or_str, string_types):
                raise TypeError("%s: Expected a node value and child list "
--
    # Comparison operators
    #////////////////////////////////////////////////////////////

    def __eq__(self, other):
        return (self.__class__ is other.__class__ and
                (self.node, list(self)) == (other.node, list(other)))

    def __lt__(self, other):
        if not isinstance(other, Tree):
            # raise_unorderable_types("<", self, other)
            # Sometimes children can be pure strings,
--
    # Disabled list operations
    #////////////////////////////////////////////////////////////

    def __mul__(self, v):
        raise TypeError('Tree does not support multiplication')
    def __rmul__(self, v):
        raise TypeError('Tree does not support multiplication')
    def __add__(self, v):
        raise TypeError('Tree does not support addition')
    def __radd__(self, v):
        raise TypeError('Tree does not support addition')

    #////////////////////////////////////////////////////////////
    # Indexing (with support for tree positions)
    #////////////////////////////////////////////////////////////

    def __getitem__(self, index):
        if isinstance(index, (int, slice)):
            return list.__getitem__(self, index)
        elif isinstance(index, (list, tuple)):
--
            raise TypeError("%s indices must be integers, not %s" %
                            (type(self).__name__, type(index).__name__))

    def __setitem__(self, index, value):
        if isinstance(index, (int, slice)):
            return list.__setitem__(self, index, value)
        elif isinstance(index, (list, tuple)):
--
            raise TypeError("%s indices must be integers, not %s" %
                            (type(self).__name__, type(index).__name__))

    def __delitem__(self, index):
        if isinstance(index, (int, slice)):
            return list.__delitem__(self, index)
        elif isinstance(index, (list, tuple)):
--
    # Basic tree operations
    #////////////////////////////////////////////////////////////

    def leaves(self):
        """
        Return the leaves of the tree.

--
                leaves.append(child)
        return leaves

    def flatten(self):
        """
        Return a flat version of the tree, with all non-root non-terminals removed.

--
        """
        return Tree(self.node, self.leaves())

    def height(self):
        """
        Return the height of the tree.

--
                max_child_height = max(max_child_height, 1)
        return 1 + max_child_height

    def treepositions(self, order='preorder'):
        """
            >>> t = Tree("(S (NP (D the) (N dog)) (VP (V chased) (NP (D the) (N cat))))")
            >>> t.treepositions() # doctest: +ELLIPSIS
--
        if order in ('postorder', 'bothorder'): positions.append( () )
        return positions

    def subtrees(self, filter=None):
        """
        Generate all the subtrees of this tree, optionally restricted
        to trees matching the filter function.
--
                for subtree in child.subtrees(filter):
                    yield subtree

    def productions(self):
        """
        Generate the productions that correspond to the non-terminal nodes of the tree.
        For each subtree of the form (P: C1 C2 ... Cn) this produces a production of the
--
                prods += child.productions()
        return prods

    def pos(self):
        """
        Return a sequence of pos-tagged words extracted from the tree.

--
                pos.append((child, self.node))
        return pos

    def leaf_treeposition(self, index):
        """
        :return: The tree position of the ``index``-th leaf in this
            tree.  I.e., if ``tp=self.leaf_treeposition(i)``, then
--

        raise IndexError('index must be less than or equal to len(self)')

    def treeposition_spanning_leaves(self, start, end):
        """
        :return: The tree position of the lowest descendant of this
            tree that dominates ``self.leaves()[start:end]``.
--
    # Transforms
    #////////////////////////////////////////////////////////////

    def chomsky_normal_form(self, factor = "right", horzMarkov = None, vertMarkov = 0, childChar = "|", parentChar = "^"):
        """
        This method can modify a tree in three ways:

--
             nodes
          3. Horizontal (parent) annotation of nodes

        :param factor: Right or left factoring method (default = "right")
        :type  factor: str = [left|right]
        :param horzMarkov: Markov order for sibling smoothing in artificial nodes (None (default) = include all siblings)
        :type  horzMarkov: int | None
        :param vertMarkov: Markov order for parent smoothing (0 (default) = no vertical annotation)
        :type  vertMarkov: int | None
        :param childChar: A string used in construction of the artificial nodes, separating the head of the
                          original subtree from the child nodes that have yet to be expanded (default = "|")
        :type  childChar: str
        :param parentChar: A string used to separate the node representation from its vertical annotation
        :type  parentChar: str
--
        from .treetransforms import chomsky_normal_form
        chomsky_normal_form(self, factor, horzMarkov, vertMarkov, childChar, parentChar)

    def un_chomsky_normal_form(self, expandUnary = True, childChar = "|", parentChar = "^", unaryChar = "+"):
        """
        This method modifies the tree in three ways:

--
          3. (optional) expands unary subtrees (if previously
             collapsed with collapseUnary(...) )

        :param expandUnary: Flag to expand unary or not (default = True)
        :type  expandUnary: bool
        :param childChar: A string separating the head node from its children in an artificial node (default = "|")
        :type  childChar: str
        :param parentChar: A sting separating the node label from its parent annotation (default = "^")
        :type  parentChar: str
        :param unaryChar: A string joining two non-terminals in a unary production (default = "+")
        :type  unaryChar: str
        """
        from .treetransforms import un_chomsky_normal_form
        un_chomsky_normal_form(self, expandUnary, childChar, parentChar, unaryChar)

    def collapse_unary(self, collapsePOS = False, collapseRoot = False, joinChar = "+"):
        """
        Collapse subtrees with a single child (ie. unary productions)
        into a new non-terminal (Tree node) joined by 'joinChar'.
--
        would require loss of useful information.  The Tree is modified
        directly (since it is passed by reference) and no value is returned.

        :param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.
                            Part-of-Speech tags) since they are always unary productions
        :type  collapsePOS: bool
        :param collapseRoot: 'False' (default) will not modify the root production
                             if it is unary.  For the Penn WSJ treebank corpus, this corresponds
                             to the TOP -> productions.
        :type collapseRoot: bool
        :param joinChar: A string used to connect collapsed node values (default = "+")
        :type  joinChar: str
        """
        from .treetransforms import collapse_unary
--
    #////////////////////////////////////////////////////////////

    @classmethod
    def convert(cls, tree):
        """
        Convert a tree between different subtypes of Tree.  ``cls`` determines
        which class will be used to encode the new tree.
--
        else:
            return tree

    def copy(self, deep=False):
        if not deep: return type(self)(self.node, self)
        else: return type(self).convert(self)

    def _frozen_class(self): return ImmutableTree
    def freeze(self, leaf_freezer=None):
        frozen_class = self._frozen_class()
        if leaf_freezer is None:
            newcopy = frozen_class.convert(self)
--
    #////////////////////////////////////////////////////////////

    @classmethod
    def parse(cls, s, brackets='()', parse_node=None, parse_leaf=None,
              node_pattern=None, leaf_pattern=None,
              remove_empty_top_bracketing=False):
        """
--
            For example, these functions could be used to parse nodes
            and leaves whose values should be some type other than
            string (such as ``FeatStruct``).
            Note that by default, node strings and leaf strings are
            delimited by whitespace and brackets; to override this
            default, use the ``node_pattern`` and ``leaf_pattern``
            arguments.

        :type node_pattern: str
        :type leaf_pattern: str
        :param node_pattern, leaf_pattern: Regular expression patterns
            used to find node and leaf substrings in ``s``.  By
            default, both nodes patterns are defined to match any
            sequence of non-whitespace non-bracket characters.

        :type remove_empty_top_bracketing: bool
--
        return tree

    @classmethod
    def _parse_error(cls, s, match, expecting):
        """
        Display a friendly error message when parsing a tree string fails.
        :param s: The string we're parsing.
--
    # Visualization & String Representation
    #////////////////////////////////////////////////////////////

    def draw(self):
        """
        Open a new window containing a graphical diagram of this tree.
        """
        from nltk.draw.tree import draw_trees
        draw_trees(self)

    def __repr__(self):
        childstr = ", ".join(unicode_repr(c) for c in self)
        return '%s(%s, [%s])' % (type(self).__name__, unicode_repr(self.node), childstr)

    def __str__(self):
        return self.pprint()

    def pprint(self, margin=70, indent=0, nodesep='', parens='()', quotes=False):
        """
        :return: A pretty-printed string representation of this tree.
        :rtype: str
--
            subsequent lines.
        :type indent: int
        :param nodesep: A string that is used to separate the node
            from the children.  E.g., the default value ``':'`` gives
            trees like ``(S: (NP: I) (VP: (V: saw) (NP: it)))``.
        """

--
                s += '\n'+' '*(indent+2)+ unicode_repr(child)
        return s+parens[1]

    def pprint_latex_qtree(self):
        r"""
        Returns a representation of the tree compatible with the
        LaTeX qtree package. This consists of the string ``\Tree``
--
        pprint = self.pprint(indent=6, nodesep='', parens=('[.', ' ]'))
        return r'\Tree ' + re.sub(reserved_chars, r'\\\1', pprint)

    def _pprint_flat(self, nodesep, parens, quotes):
        childstrs = []
        for child in self:
            if isinstance(child, Tree):
--


class ImmutableTree(Tree):
    def __init__(self, node_or_str, children=None):
        super(ImmutableTree, self).__init__(node_or_str, children)
        # Precompute our hash value.  This ensures that we're really
        # immutable.  It also means we only have to calculate it once.
--
            raise ValueError("%s: node value and children "
                             "must be immutable" % type(self).__name__)

    def __setitem__(self, index, value):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __setslice__(self, i, j, value):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __delitem__(self, index):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __delslice__(self, i, j):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __iadd__(self, other):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __imul__(self, other):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def append(self, v):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def extend(self, v):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def pop(self, v=None):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def remove(self, v):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def reverse(self):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def sort(self):
        raise ValueError('%s may not be modified' % type(self).__name__)
    def __hash__(self):
        return self._hash

    def _get_node(self):
        """Get the node value"""
        return self._node
    def _set_node(self, value):
        """
        Set the node value.  This will only succeed the first time the
        node value is set, which should occur in ImmutableTree.__init__().
--
    An abstract base class for a ``Tree`` that automatically maintains
    pointers to parent nodes.  These parent pointers are updated
    whenever any change is made to a tree's structure.  Two subclasses
    are currently defined:

      - ``ParentedTree`` is used for tree structures where each subtree
        has at most one parent.  This class should be used in cases
--

    Subclassing
    ===========
    The ``AbstractParentedTree`` class redefines all operations that
    modify a tree's structure to call two methods, which are used by
    subclasses to update parent information:

--
      - ``_delparent()`` is called whenever a child is removed.
    """

    def __init__(self, node_or_str, children=None):
        super(AbstractParentedTree, self).__init__(node_or_str, children)
        # If children is None, the tree is parsed from node_or_str, and
        # all parents will be set during parsing.
--
    # Parent management
    #////////////////////////////////////////////////////////////

    def _setparent(self, child, index, dry_run=False):
        """
        Update the parent pointer of ``child`` to point to ``self``.  This
        method is only called if the type of ``child`` is ``Tree``;
--
        """
        raise NotImplementedError()

    def _delparent(self, child, index):
        """
        Update the parent pointer of ``child`` to not point to self.  This
        method is only called if the type of ``child`` is ``Tree``; i.e., it
--
    # Every method that adds or removes a child must make
    # appropriate calls to _setparent() and _delparent().

    def __delitem__(self, index):
        # del ptree[start:stop]
        if isinstance(index, slice):
            start, stop, step = slice_bounds(self, index, allow_step=True)
--
            raise TypeError("%s indices must be integers, not %s" %
                            (type(self).__name__, type(index).__name__))

    def __setitem__(self, index, value):
        # ptree[start:stop] = value
        if isinstance(index, slice):
            start, stop, step = slice_bounds(self, index, allow_step=True)
--
            raise TypeError("%s indices must be integers, not %s" %
                            (type(self).__name__, type(index).__name__))

    def append(self, child):
        if isinstance(child, Tree):
            self._setparent(child, len(self))
        super(AbstractParentedTree, self).append(child)

    def extend(self, children):
        for child in children:
            if isinstance(child, Tree):
                self._setparent(child, len(self))
            super(AbstractParentedTree, self).append(child)

    def insert(self, index, child):
        # Handle negative indexes.  Note that if index < -len(self),
        # we do *not* raise an IndexError, unlike __getitem__.  This
        # is done for consistency with list.__getitem__ and list.index.
--
            self._setparent(child, index)
        super(AbstractParentedTree, self).insert(index, child)

    def pop(self, index=-1):
        if index < 0: index += len(self)
        if index < 0: raise IndexError('index out of range')
        if isinstance(self[index], Tree):
--

    # n.b.: like `list`, this is done by equality, not identity!
    # To remove a specific child, use del ptree[i].
    def remove(self, child):
        index = self.index(child)
        if isinstance(self[index], Tree):
            self._delparent(self[index], index)
--
    # because negative indices are already handled *before*
    # __getslice__ is called; and we don't want to double-count them.
    if hasattr(list, '__getslice__'):
        def __getslice__(self, start, stop):
            return self.__getitem__(slice(max(0, start), max(0, stop)))
        def __delslice__(self, start, stop):
            return self.__delitem__(slice(max(0, start), max(0, stop)))
        def __setslice__(self, start, stop, value):
            return self.__setitem__(slice(max(0, start), max(0, stop)), value)

class ParentedTree(AbstractParentedTree):
--
    or ``MultiParentedTrees``.  Mixing tree implementations may result
    in incorrect parent pointers and in ``TypeError`` exceptions.
    """
    def __init__(self, node_or_str, children=None):
        self._parent = None
        """The parent of this Tree, or None if it has no parent."""
        super(ParentedTree, self).__init__(node_or_str, children)
--
                    child._parent = None
                    self._setparent(child, i)

    def _frozen_class(self): return ImmutableParentedTree

    #/////////////////////////////////////////////////////////////////
    # Methods
    #/////////////////////////////////////////////////////////////////

    def parent(self):
        """The parent of this tree, or None if it has no parent."""
        return self._parent

    def parent_index(self):
        """
        The index of this tree in its parent.  I.e.,
        ``ptree.parent()[ptree.parent_index()] is ptree``.  Note that
--
            if child is self: return i
        assert False, 'expected to find self in self._parent!'

    def left_sibling(self):
        """The left sibling of this tree, or None if it has none."""
        parent_index = self.parent_index()
        if self._parent and parent_index > 0:
            return self._parent[parent_index-1]
        return None # no left sibling

    def right_sibling(self):
        """The right sibling of this tree, or None if it has none."""
        parent_index = self.parent_index()
        if self._parent and parent_index < (len(self._parent)-1):
            return self._parent[parent_index+1]
        return None # no right sibling

    def root(self):
        """
        The root of this tree.  I.e., the unique ancestor of this tree
        whose parent is None.  If ``ptree.parent()`` is None, then
--
            root = root.parent()
        return root

    def treeposition(self):
        """
        The tree position of this tree, relative to the root of the
        tree.  I.e., ``ptree.root[ptree.treeposition] is ptree``.
--
    # Parent Management
    #/////////////////////////////////////////////////////////////////

    def _delparent(self, child, index):
        # Sanity checks
        assert isinstance(child, ParentedTree)
        assert self[index] is child
--
        # Delete child's parent pointer.
        child._parent = None

    def _setparent(self, child, index, dry_run=False):
        # If the child's type is incorrect, then complain.
        if not isinstance(child, ParentedTree):
            raise TypeError('Can not insert a non-ParentedTree '+
--
    ``Trees`` or ``ParentedTrees``.  Mixing tree implementations may
    result in incorrect parent pointers and in ``TypeError`` exceptions.
    """
    def __init__(self, node_or_str, children=None):
        self._parents = []
        """A list of this tree's parents.  This list should not
           contain duplicates, even if a parent contains this tree
--
                    child._parents = []
                    self._setparent(child, i)

    def _frozen_class(self): return ImmutableMultiParentedTree

    #/////////////////////////////////////////////////////////////////
    # Methods
    #/////////////////////////////////////////////////////////////////

    def parents(self):
        """
        The set of parents of this tree.  If this tree has no parents,
        then ``parents`` is the empty set.  To check if a tree is used
--
        """
        return list(self._parents)

    def left_siblings(self):
        """
        A list of all left siblings of this tree, in any of its parent
        trees.  A tree may be its own left sibling if it is used as
--
                for (parent, index) in self._get_parent_indices()
                if index > 0]

    def right_siblings(self):
        """
        A list of all right siblings of this tree, in any of its parent
        trees.  A tree may be its own right sibling if it is used as
--
                for (parent, index) in self._get_parent_indices()
                if index < (len(parent)-1)]

    def _get_parent_indices(self):
        return [(parent, index)
                for parent in self._parents
                for index, child in enumerate(parent)
                if child is self]

    def roots(self):
        """
        The set of all roots of this tree.  This set is formed by
        tracing all possible parent paths until trees with no parents
--
        """
        return list(self._get_roots_helper({}).values())

    def _get_roots_helper(self, result):
        if self._parents:
            for parent in self._parents:
                parent._get_roots_helper(result)
--
            result[id(self)] = self
        return result

    def parent_indices(self, parent):
        """
        Return a list of the indices where this tree occurs as a child
        of ``parent``.  If this child does not occur as a child of
--
        else: return [index for (index, child) in enumerate(parent)
                      if child is self]

    def treepositions(self, root):
        """
        Return a list of all tree positions that can be used to reach
        this multi-parented tree starting from ``root``.  I.e., the
--
    # Parent Management
    #/////////////////////////////////////////////////////////////////

    def _delparent(self, child, index):
        # Sanity checks
        assert isinstance(child, MultiParentedTree)
        assert self[index] is child
--
        else:
            child._parents.remove(self)

    def _setparent(self, child, index, dry_run=False):
        # If the child's type is incorrect, then complain.
        if not isinstance(child, MultiParentedTree):
            raise TypeError('Can not insert a non-MultiParentedTree '+
--

@python_2_unicode_compatible
class ProbabilisticTree(Tree, ProbabilisticMixIn):
    def __init__(self, node_or_str, children=None, **prob_kwargs):
        Tree.__init__(self, node_or_str, children)
        ProbabilisticMixIn.__init__(self, **prob_kwargs)

    # We have to patch up these methods to make them work right:
    def _frozen_class(self): return ImmutableProbabilisticTree
    def __repr__(self):
        return '%s (p=%r)' % (Tree.unicode_repr(self), self.prob())
    def __str__(self):
        return '%s (p=%.6g)' % (self.pprint(margin=60), self.prob())
    def copy(self, deep=False):
        if not deep: return type(self)(self.node, self, prob=self.prob())
        else: return type(self).convert(self)
    @classmethod
    def convert(cls, val):
        if isinstance(val, Tree):
            children = [cls.convert(child) for child in val]
            if isinstance(val, ProbabilisticMixIn):
--
        else:
            return val

    def __eq__(self, other):
        return (self.__class__ is other.__class__ and
                (self.node, list(self), self.prob()) ==
                (other.node, list(other), other.prob()))

    def __lt__(self, other):
        if not isinstance(other, Tree):
            raise_unorderable_types("<", self, other)
        if self.__class__ is other.__class__:
--

@python_2_unicode_compatible
class ImmutableProbabilisticTree(ImmutableTree, ProbabilisticMixIn):
    def __init__(self, node_or_str, children=None, **prob_kwargs):
        ImmutableTree.__init__(self, node_or_str, children)
        ProbabilisticMixIn.__init__(self, **prob_kwargs)
        self._hash = hash((self.node, tuple(self), self.prob()))

    # We have to patch up these methods to make them work right:
    def _frozen_class(self): return ImmutableProbabilisticTree
    def __repr__(self):
        return '%s [%s]' % (Tree.unicode_repr(self), self.prob())
    def __str__(self):
        return '%s [%s]' % (self.pprint(margin=60), self.prob())
    def copy(self, deep=False):
        if not deep: return type(self)(self.node, self, prob=self.prob())
        else: return type(self).convert(self)
    @classmethod
    def convert(cls, val):
        if isinstance(val, Tree):
            children = [cls.convert(child) for child in val]
            if isinstance(val, ProbabilisticMixIn):
--
            return val


def _child_names(tree):
    names = []
    for child in tree:
        if isinstance(child, Tree):
--
## Parsing
######################################################################

def bracket_parse(s):
    """
    Use Tree.parse(s, remove_empty_top_bracketing=True) instead.
    """
    raise NameError("Use Tree.parse(s, remove_empty_top_bracketing=True) instead.")

def sinica_parse(s):
    """
    Parse a Sinica Treebank string and return a tree.  Trees are represented as nested brackettings,
    as shown in the following example (X represents a Chinese character):
--
## Demonstration
######################################################################

def demo():
    """
    A demonstration showing how Trees and Trees can be
    used.  This demonstration creates a Tree, and loads a
--
# For license information, see LICENSE.TXT
from __future__ import print_function, unicode_literals
import logging
from collections import defaultdict

from nltk import compat
from nltk.metrics import precision, recall
--
    :type alignment: Alignment
    """

    def __init__(self, words=[], mots=[], alignment='', encoding='utf8'):
        self._words = words
        self._mots = mots
        self.alignment = alignment

    @property
    def words(self):
        return self._words

    @property
    def mots(self):
        return self._mots

    def _get_alignment(self):
        return self._alignment
    def _set_alignment(self, alignment):
        if not isinstance(alignment, Alignment):
            alignment = Alignment(alignment)
        self._check_align(alignment)
        self._alignment = alignment
    alignment = property(_get_alignment, _set_alignment)

    def _check_align(self, a):
        """
        Check whether the alignments are legal.

--
            raise IndexError("Alignment is outside boundary of mots")
        return True

    def __repr__(self):
        """
        Return a string representation for this ``AlignedSent``.

--

        return "AlignedSent(%s, %s, %r)" % (words, mots, self._alignment)

    def __str__(self):
        """
        Return a human-readable string representation for this ``AlignedSent``.

--
        target = " ".join(self._mots)[:20] + "..."
        return "<AlignedSent: '%s' -> '%s'>" % (source, target)

    def invert(self):
        """
        Return the aligned sentence pair, reversing the directionality

--
        return AlignedSent(self._mots, self._words,
                               self._alignment.invert())

    def precision(self, reference):
        """
        Return the precision of an aligned sentence with respect to a
        "gold standard" reference ``AlignedSent``.
--
        return precision(possible, align)


    def recall(self, reference):
        """
        Return the recall of an aligned sentence with respect to a
        "gold standard" reference ``AlignedSent``.
--
        return recall(sure, align)


    def alignment_error_rate(self, reference, possible=None):
        """
        Return the Alignment Error Rate (AER) of an aligned sentence
        with respect to a "gold standard" reference ``AlignedSent``.
--
        :param reference: A "gold standard" reference aligned sentence.
        :type possible: AlignedSent or Alignment or None
        :param possible: A "gold standard" reference of possible alignments
            (defaults to *reference* if None)
        :rtype: float or None
        """
        # Get alignments in set of 2-tuples form
--
        True
    """

    def __new__(cls, string_or_pairs):
        if isinstance(string_or_pairs, compat.string_types):
            string_or_pairs = [_giza2pair(p) for p in string_or_pairs.split()]
        self = frozenset.__new__(cls, string_or_pairs)
--
        self._index = None
        return self

    def __getitem__(self, key):
        """
        Look up the alignments that map from a given index or slice.
        """
--
            self._build_index()
        return self._index.__getitem__(key)

    def invert(self):
        """
        Return an Alignment object, being the inverted mapping.
        """
        return Alignment(((p[1], p[0]) + p[2:]) for p in self)

    def range(self, positions=None):
        """
        Work out the range of the mapping from the given positions.
        If no positions are specified, compute the range of the entire mapping.
--
            image.update(f for _,f in self._index[p])
        return sorted(image)

    def __repr__(self):
        """
        Produce a Giza-formatted string representing the alignment.
        """
        return "Alignment(%r)" % sorted(self)

    def __str__(self):
        """
        Produce a Giza-formatted string representing the alignment.
        """
        return " ".join("%d-%d" % p[:2] for p in sorted(self))

    def _build_index(self):
        """
        Build a list self._index such that self._index[i] is a list
        of the alignments originating from word i.
--
    :param convergent_threshold: The threshold value of convergence. An
        entry is considered converged if the delta from ``old_t`` to ``new_t``
        is less than this value. The algorithm terminates when all entries
        are converged. This parameter is optional, default is 0.01
    :type convergent_threshold: float
    """

    def __init__(self, aligned_sents, convergent_threshold=1e-2, debug=False):
        self.aligned_sents = aligned_sents
        self.convergent_threshold = convergent_threshold
        # Dictionary of translation probabilities t(e,f).
        self.probabilities = None
        self._train()

    def _train(self):
        """
        Perform Expectation Maximization training to learn
        word-to-word translation probabilities.
--
        num_probs = len(english_words) * len(foreign_words)

        # Initialise t(e|f) uniformly
        default_prob = 1.0 / len(english_words)
        t = defaultdict(lambda: default_prob)

        convergent_threshold = self.convergent_threshold
        globally_converged = False
        iteration_count = 0
        while not globally_converged:
            # count(e|f)
            count = defaultdict(float)
            # total(f)
            total = defaultdict(float)

            for aligned_sent in self.aligned_sents:
                s_total = {}
--

        self.probabilities = dict(t)

    def aligned(self):
        """
        Return a list of AlignedSents with Alignments calculated using
        IBM-Model 1.
--
        return aligned


def _giza2pair(pair_string):
    i, j = pair_string.split("-")
    return int(i), int(j)

def _naacl2pair(pair_string):
    i, j, p = pair_string.split("-")
    return int(i), int(j)

--
PY3 = sys.version_info[0] == 3

if PY3:
    def b(s):
        return s.encode("latin-1")
    def u(s):
        return s

    string_types = str,
--
    StringIO = io.StringIO
    BytesIO = io.BytesIO

    import html.entities as htmlentitydefs
    from urllib.request import (urlopen, ProxyHandler, build_opener,
        install_opener, getproxies, HTTPPasswordMgrWithDefaultRealm,
        ProxyBasicAuthHandler, ProxyDigestAuthHandler, Request)
    from urllib.error import HTTPError, URLError
    from urllib.parse import quote_plus, unquote_plus, urlencode

else:
    def b(s):
        return s
    def u(s):
        return unicode(s, "unicode_escape")

    string_types = basestring,
--
        from StringIO import StringIO
    BytesIO = StringIO

    import htmlentitydefs
    from urllib2 import (urlopen, HTTPError, URLError,
        ProxyHandler, build_opener, install_opener,
        HTTPPasswordMgrWithDefaultRealm, ProxyBasicAuthHandler,
        ProxyDigestAuthHandler, Request)
    from urllib import getproxies, quote_plus, unquote_plus, urlencode

    # Maps py2 tkinter package structure to py3 using import hook (PEP 302)
    class TkinterPackage(object):
        def __init__(self):
            self.mod = __import__("Tkinter")
            self.__path__ = ["nltk_py2_tkinter_package_path"]
        def __getattr__(self, name):
            return getattr(self.mod, name)

    class TkinterLoader(object):
        def __init__(self):
            # module name mapping from py3 to py2
            self.module_map = {
                "tkinter": "Tkinter",
--
                "tkinter.font": "tkFont",
                "tkinter.messagebox": "tkMessageBox",
            }
        def find_module(self, name, path=None):
            # we are only interested in tkinter modules listed
            # in self.module_map
            if name in self.module_map:
                return self
        def load_module(self, name):
            if name not in sys.modules:
                if name == 'tkinter':
                    mod = TkinterPackage()
--
    sys.meta_path = [TkinterLoader()]


def iterkeys(d):
    """Return an iterator over the keys of a dictionary."""
    return getattr(d, _iterkeys)()

def itervalues(d):
    """Return an iterator over the values of a dictionary."""
    return getattr(d, _itervalues)()

def iteritems(d):
    """Return an iterator over the (key, value) pairs of a dictionary."""
    return getattr(d, _iteritems)()

try:
    from functools import total_ordering
except ImportError: # python 2.6
    def total_ordering(cls):
        """Class decorator that fills in missing ordering methods"""
        convert = {
            '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),
--
        }
        roots = set(dir(cls)) & set(convert)
        if not roots:
            raise ValueError('must define at least one ordering operation: < > <= >=')
        root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__
        for opname, opfunc in convert[root]:
            if opname not in roots:
--

# for use in adding /PY3 to the second (filename) argument
# of the file pointers in data.py
def py3_data(init_func):
    def _decorator(*args, **kwargs):
        if PY3:
            path = args[1]
            for item in _PY3_DATA_UPDATES:
--
import unicodedata
import functools

def remove_accents(text):

    if isinstance(text, bytes):
        text = text.decode('ascii')
--
        transliterate = remove_accents


def python_2_unicode_compatible(klass):
    """
    This decorator defines __unicode__ method and fixes
    __repr__ and __str__ methods under Python 2.

    To support Python 2 and 3 with a single code base,
    define __str__ and __repr__ methods returning unicode
    text and apply this decorator to the class.

    Original __repr__ and __str__ would be available
--
    return klass


def unicode_repr(obj):
    """
    For classes that was fixed with @python_2_unicode_compatible
    ``unicode_repr`` returns ``obj.unicode_repr()``; for unicode strings
--
    return repr(obj)


def _transliterated(method):
    def wrapper(self):
        return transliterate(method(self))

    functools.update_wrapper(wrapper, method, ["__name__", "__doc__"])
--
    return wrapper


def _7bit(method):
    def wrapper(self):
        return method(self).encode('ascii', 'backslashreplace')

    functools.update_wrapper(wrapper, method, ["__name__", "__doc__"])
--
    return wrapper


def _was_fixed(method):
    return (getattr(method, "_nltk_compat_7bit", False) or
            getattr(method, "_nltk_compat_transliterated", False))
--
# URL: <http://www.nltk.org>
# For license information, see LICENSE.TXT

from collections import defaultdict

class MinimalSet(object):
    """
--
    cases like wind (noun) 'air in rapid motion', vs wind (verb)
    'coil, wrap'.
    """
    def __init__(self, parameters=None):
        """
        Create a new minimal set.

--
        """
        self._targets = set()  # the contrastive information
        self._contexts = set() # what we are controlling for
        self._seen = defaultdict(set)  # to record what we have seen
        self._displays = {}    # what we will display

        if parameters:
            for context, target, display in parameters:
                self.add(context, target, display)

    def add(self, context, target, display):
        """
        Add a new item to the minimal set, having the specified
        context, target, and display form.
--
        # For a given context and target, store the display form
        self._displays[(context, target)] = display

    def contexts(self, minimum=2):
        """
        Determine which contexts occurred with enough distinct targets.

--
        """
        return [c for c in self._contexts if len(self._seen[c]) >= minimum]

    def display(self, context, target, default=""):
        if (context, target) in self._displays:
            return self._displays[(context, target)]
        else:
            return default

    def display_all(self, context):
        result = []
        for target in self._targets:
            x = self.display(context, target)
            if x: result.append(x)
        return result

    def targets(self):
        return self._targets

--
    With this clarification,
    Conversely,
    We have already seen that
    By combining adjunctions and certain deformations,
    I suggested that these results would follow from the assumption that
    If the position of the trace in (99c) were only relatively \
inaccessible to movement,
--
    the theory of syntactic features developed earlier"""
# List of SUBJECTs chosen for maximum professorial macho.

verbs = """can be defined in such a way as to impose
    delimits
    suffices to account for
    cannot be arbitrary in
--
#List of VERBs chosen for autorecursive obfuscation.

objects = """ problems of phonemic and morphological analysis.
    a corpus of utterance tokens upon which conformity has been defined \
by the paired utterance test.
    the traditional practice of grammarians.
    the levels of acceptability from fairly high (e.g. (99a)) to virtual \
--
from itertools import chain, islice
from nltk.compat import izip

def generate_chomsky(times=5, line_length=72):
    parts = []
    for part in (leadins, subjects, verbs, objects):
        phraselist = list(map(str.strip, part.splitlines()))
--


# reverse a word with probability 0.5
def revword(word):
    if random.randint(1,2) == 1:
        return word[::-1]
    return word

# try to insert word at position x,y; direction encoded in xf,yf
def step(word, x, xf, y, yf, grid):
    for i in range(len(word)):
        if grid[xf(i)][yf(i)] != "" and grid[xf(i)][yf(i)] != word[i]:
            return False
--
    return True

# try to insert word at position x,y, in direction dir
def check(word, dir, x, y, grid, rows, cols):
    if dir==1:
        if x-len(word)<0 or y-len(word)<0:
            return False
--
            return False
        return step(word, x, lambda i:x, y, lambda i:y-i, grid)

def wordfinder(words, rows=20, cols=20, attempts=50,
               alph='ABCDEFGHIJKLMNOPQRSTUVWXYZ'):
    """
    Attempt to arrange words into a letter-grid with the specified
    number of rows and columns.  Try each word in several positions
--

    return grid, used

def word_finder():
    from nltk.corpus import words
    wordlist = words.words()
    random.shuffle(wordlist)
--
"""
from __future__ import print_function

def babelize_shell():
    print("Babelfish online translation service is no longer available.")
--
# Selection Sort
##################################################################

def selection(a):
    """
    Selection Sort: scan the list to find its smallest element, then
    swap it with the first element.  The remainder of the list is one
--
# Bubble Sort
##################################################################

def bubble(a):
    """
    Bubble Sort: compare adjacent elements of the list left-to-right,
    and swap them if they are out of order.  After one pass through
--
# Merge Sort
##################################################################

def _merge_lists(b, c):
    count = 0
    i = j = 0
    a = []
--
        a += b[i:]
    return a, count

def merge(a):
    """
    Merge Sort: split the list in half, and sort each half, then
    combine the sorted halves.
--
# Quick Sort
##################################################################

def _partition(a, l, r):
    p = a[l]; i = l; j = r+1
    count = 0
    while True:
--
    a[l],a[j] = a[j],a[l]
    return j, count

def _quick(a, l, r):
    count = 0
    if l<r:
        s, count = _partition(a, l, r)
--
        count += _quick(a, s+1, r)
    return count

def quick(a):
    return _quick(a, 0, len(a)-1)

##################################################################
# Demonstration
##################################################################

def demo():
    from random import shuffle

    for size in (10, 20, 50, 100, 200, 500, 1000):
--
    package at *path*.  NLTK will search for these files in the
    directories specified by ``nltk.data.path``.

If no protocol is specified, then the default protocol ``nltk:`` will
be used.

This module provides to functions that can be used to access a
--
# Util Functions
######################################################################

def gzip_open_unicode(filename, mode="rb", compresslevel=9,
                      encoding='utf-8', fileobj=None, errors=None, newline=None):
    if fileobj is None:
        fileobj=GzipFile(filename, mode, compresslevel, fileobj)
    return io.TextIOWrapper(fileobj, encoding, errors, newline)

def split_resource_url(resource_url):
    """
    Splits a resource url into "<protocol>:<path>".

--
        path = re.sub(r'^/{0,2}', '', path)
    return protocol, path

def normalize_resource_url(resource_url):
    """
    Normalizes a resource url

--
    try:
        protocol, name = split_resource_url(resource_url)
    except ValueError:
        # the resource url has no protocol, use the nltk protocol by default
        protocol = 'nltk'
        name = resource_url
    # use file protocol if the path is an absolute path
--
        name = name.lstrip('/')
    return ''.join([protocol,name])

def normalize_resource_name(resource_name, allow_relative = True):
    """
    :type resource_name: str or unicode
    :param resource_name: The name of the resource to search for.
--
    identifies a file contained within a zipfile, that can be accessed
    by reading that zipfile.
    """
    def open(self, encoding=None):
        """
        Return a seekable read-only stream that can be used to read
        the contents of the file identified by this path pointer.
--
        """
        raise NotImplementedError('abstract base class')

    def file_size(self):
        """
        Return the size of the file pointed to by this path pointer,
        in bytes.
--
        """
        raise NotImplementedError('abstract base class')

    def join(self, fileid):
        """
        Return a new path pointer formed by starting at the path
        identified by this pointer, and then following the relative
--
    directly via a given absolute path.
    """
    @py3_data
    def __init__(self, _path):
        """
        Create a new path pointer for the given absolute path.

--
        # str does all of its setup work in __new__.

    @property
    def path(self):
        """The absolute path identified by this path pointer."""
        return self._path

    def open(self, encoding=None):
        stream = open(self._path, 'rb')
        if encoding is not None:
            stream = SeekableUnicodeStreamReader(stream, encoding)
        return stream

    def file_size(self):
        return os.stat(self._path).st_size

    def join(self, fileid):
        _path = os.path.join(self._path, fileid)
        return FileSystemPathPointer(_path)

    def __repr__(self):
        # This should be a byte string under Python 2.x;
        # we don't want transliteration here so
        # @python_2_unicode_compatible is not used.
        return str('FileSystemPathPointer(%r)' % self._path)

    def __str__(self):
        return self._path


--
    This allows faster reads and writes of data to and from gzip-compressed
    files at the cost of using more memory.

    The default buffer size is 2MB.

    ``BufferedGzipFile`` is useful for loading large gzipped pickle objects
    as well as writing large encoded feature files for classifier training.
--
    SIZE = 2 * 2**20

    @py3_data
    def __init__(self, filename=None, mode=None, compresslevel=9,
                 fileobj=None, **kwargs):
        """
        Return a buffered gzip file object.
--
        :param compresslevel: The compresslevel argument is an integer from 1
            to 9 controlling the level of compression; 1 is fastest and
            produces the least compression, and 9 is slowest and produces the
            most compression. The default is 9.
        :type compresslevel: int
        :param fileobj: a BytesIO stream to read from instead of a file.
        :type fileobj: BytesIO
--
        # cStringIO does not support len.
        self._len = 0

    def _reset_buffer(self):
        # For some reason calling BytesIO.truncate() here will lead to
        # inconsistent writes so just set _buffer to a new BytesIO object.
        self._buffer = compat.BytesIO()
        self._len = 0

    def _write_buffer(self, data):
        # Simply write to the buffer and increment the buffer size.
        if data is not None:
            self._buffer.write(data)
            self._len += len(data)

    def _write_gzip(self, data):
        # Write the current buffer to the GzipFile.
        GzipFile.write(self, self._buffer.getvalue())
        # Then reset the buffer and write the new data to the buffer.
        self._reset_buffer()
        self._write_buffer(data)

    def close(self):
        # GzipFile.close() doesn't actuallly close anything.
        if self.mode == GZ_WRITE:
            self._write_gzip(None)
            self._reset_buffer()
        return GzipFile.close(self)

    def flush(self, lib_mode=FLUSH):
        self._buffer.flush()
        GzipFile.flush(self, lib_mode)

    def read(self, size=None):
        if not size:
            size = self._size
            contents = compat.BytesIO()
--
        else:
            return GzipFile.read(self, size)

    def write(self, data, size=-1):
        """
        :param data: bytes to write to file or buffer
        :type data: bytes
--
    file located at a given absolute path.  ``GzipFileSystemPathPointer`` is
    appropriate for loading large gzip-compressed pickle objects efficiently.
    """
    def open(self, encoding=None):
        stream = BufferedGzipFile(self._path, 'rb')
        if encoding:
            stream = SeekableUnicodeStreamReader(stream, encoding)
--
    which can be accessed by reading that zipfile.
    """
    @py3_data
    def __init__(self, zipfile, entry=''):
        """
        Create a new path pointer pointing at the specified entry
        in the given zipfile.
--
        self._entry = entry

    @property
    def zipfile(self):
        """
        The zipfile.ZipFile object used to access the zip file
        containing the entry identified by this path pointer.
--
        return self._zipfile

    @property
    def entry(self):
        """
        The name of the file within zipfile that this path
        pointer points to.
        """
        return self._entry

    def open(self, encoding=None):
        data = self._zipfile.read(self._entry)
        stream = compat.BytesIO(data)
        if self._entry.endswith('.gz'):
--
            stream = SeekableUnicodeStreamReader(stream, encoding)
        return stream

    def file_size(self):
        return self._zipfile.getinfo(self._entry).file_size

    def join(self, fileid):
        entry = '%s/%s' % (self._entry, fileid)
        return ZipFilePathPointer(self._zipfile, entry)

    def __repr__(self):
        return str('ZipFilePathPointer(%r, %r)') % (
            self._zipfile.filename, self._entry)

    def __str__(self):
        return os.path.normpath(os.path.join(self._zipfile.filename, self._entry))

######################################################################
--
"""A dictionary used to cache resources so that they won't
   need to be loaded more than once."""

def find(resource_name, paths=None):
    """
    Find the given resource by searching through the directories and
    zip files in paths, where a None or empty string specifies an absolute path.
--
    """
    resource_name = normalize_resource_name(resource_name, True)

    # Resolve default paths at runtime in-case the user overrides nltk.data.path
    if paths is None:
        paths=path

--
    resource_not_found = '\n%s\n%s\n%s' % (sep, msg, sep)
    raise LookupError(resource_not_found)

def retrieve(resource_url, filename=None, verbose=True):
    """
    Copy the given resource to a local file.  If no filename is
    specified, then use the URL's filename.  If there is already a
--

    :type resource_url: str
    :param resource_url: A URL specifying where the resource should be
        loaded from.  The default protocol is "nltk:", which searches
        for the file in the the NLTK data package.
    """
    resource_url = normalize_resource_url(resource_url)
--
    'text': 'text',
}

def load(resource_url, format='auto', cache=True, verbose=False,
         logic_parser=None, fstruct_parser=None, encoding=None):
    """
    Load a given resource from the NLTK data package.  The following
--

    :type resource_url: str
    :param resource_url: A URL specifying where the resource should be
        loaded from.  The default protocol is "nltk:", which searches
        for the file in the the NLTK data package.
    :type cache: bool
    :param cache: If true, add this resource to a cache.  If load()
--

    return resource_val

def show_cfg(resource_url, escape='##'):
    """
    Write out a grammar file, ignoring escaped and empty lines.

    :type resource_url: str
    :param resource_url: A URL specifying where the resource should be
        loaded from.  The default protocol is "nltk:", which searches
        for the file in the the NLTK data package.
    :type escape: str
    :param escape: Prepended string that signals lines to be ignored
--
        print(l)


def clear_cache():
    """
    Remove all objects from the resource cache.
    :see: load()
    """
    _resource_cache.clear()

def _open(resource_url):
    """
    Helper function that returns an open file object for a resource,
    given its resource URL.  If the given resource URL uses the "nltk:"
--

    :type resource_url: str
    :param resource_url: A URL specifying where the resource should be
        loaded from.  The default protocol is "nltk:", which searches
        for the file in the the NLTK data package.
    """
    resource_url = normalize_resource_url(resource_url)
--

class LazyLoader(object):
    @py3_data
    def __init__(self, _path):
        self._path = _path

    def __load(self):
        resource = load(self._path)
        # This is where the magic happens!  Transform ourselves into
        # the object by modifying our own __dict__ and __class__ to
--
        self.__dict__ = resource.__dict__
        self.__class__ = resource.__class__

    def __getattr__(self, attr):
        self.__load()
        # This looks circular, but its not, since __load() changes our
        # __class__ to something new:
        return getattr(self, attr)

    def __repr__(self):
        self.__load()
        # This looks circular, but its not, since __load() changes our
        # __class__ to something new:
--
    read-only (i.e. ``write()`` and ``writestr()`` are disabled.
    """
    @py3_data
    def __init__(self, filename):
        if not isinstance(filename, compat.string_types):
            raise TypeError('ReopenableZipFile filename must be a string')
        zipfile.ZipFile.__init__(self, filename)
        assert self.filename == filename
        self.close()

    def read(self, name):
        assert self.fp is None
        self.fp = open(self.filename, 'rb')
        value = zipfile.ZipFile.read(self, name)
        self.close()
        return value

    def write(self, *args, **kwargs):
        """:raise NotImplementedError: OpenOnDemandZipfile is read-only"""
        raise NotImplementedError('OpenOnDemandZipfile is read-only')

    def writestr(self, *args, **kwargs):
        """:raise NotImplementedError: OpenOnDemandZipfile is read-only"""
        raise NotImplementedError('OpenOnDemandZipfile is read-only')

    def __repr__(self):
        return repr(str('OpenOnDemandZipFile(%r)') % self.filename)

######################################################################
--
    DEBUG = True #: If true, then perform extra sanity checks.

    @py3_data
    def __init__(self, stream, encoding, errors='strict'):
        # Rewind the stream to its beginning.
        stream.seek(0)

--
    # Read methods
    #/////////////////////////////////////////////////////////////////

    def read(self, size=None):
        """
        Read up to ``size`` bytes, decode them using this reader's
        encoding, and return the resulting unicode string.
--

        return chars

    def readline(self, size=None):
        """
        Read a line of text, decode it using this reader's encoding,
        and return the resulting unicode string.
--

        return line

    def readlines(self, sizehint=None, keepends=True):
        """
        Read this file's contents, decode them using this reader's
        encoding, and return it as a list of unicode lines.
--
        """
        return self.read().splitlines(keepends)

    def next(self):
        """Return the next decoded line from the underlying stream."""
        line = self.readline()
        if line: return line
        else: raise StopIteration

    def __next__(self):
        return self.next()

    def __iter__(self):
        """Return self"""
        return self

    def xreadlines(self):
        """Return self"""
        return self

--
    #/////////////////////////////////////////////////////////////////

    @property
    def closed(self):
        """True if the underlying stream is closed."""
        return self.stream.closed

    @property
    def name(self):
        """The name of the underlying stream."""
        return self.stream.name

    @property
    def mode(self):
        """The mode of the underlying stream."""
        return self.stream.mode

    def close(self):
        """
        Close the underlying stream.
        """
--
    # Seek and tell
    #/////////////////////////////////////////////////////////////////

    def seek(self, offset, whence=0):
        """
        Move the stream to a new file position.  If the reader is
        maintaining any buffers, tehn they will be cleared.
--
        self._rewind_numchars = None
        self._rewind_checkpoint = self.stream.tell()

    def char_seek_forward(self, offset):
        """
        Move the read pointer forward by ``offset`` characters.
        """
--
        # Perform the seek operation.
        self._char_seek_forward(offset)

    def _char_seek_forward(self, offset, est_bytes=None):
        """
        Move the file position forward by ``offset`` characters,
        ignoring all buffers.

        :param est_bytes: A hint, giving an estimate of the number of
            bytes that will be neded to move forward by ``offset`` chars.
            Defaults to ``offset``.
        """
        if est_bytes is None: est_bytes = offset
        bytes = b''
--
            # Otherwise, we haven't read enough bytes yet; loop again.
            est_bytes += offset - len(chars)

    def tell(self):
        """
        Return the current file position on the underlying byte
        stream.  If this reader is maintaining any buffers, then the
--
    # Helper methods
    #/////////////////////////////////////////////////////////////////

    def _read(self, size=None):
        """
        Read up to ``size`` bytes from the underlying stream, decode
        them using this reader's encoding, and return the resulting
--
        # Return the result
        return chars

    def _incr_decode(self, bytes):
        """
        Decode the given byte string into a unicode string, using this
        reader's encoding.  If an exception is encountered that
--
        'utf32be': [(codecs.BOM_UTF32_BE, None)],
        }

    def _check_bom(self):
        # Normalize our encoding name
        enc = re.sub('[ -]', '', self.encoding.lower())

--
# Regular Expression Processing
######################################################################

def convert_regexp_to_nongrouping(pattern):
    """
    Convert all grouping parentheses in the given regexp pattern to
    non-grouping parentheses, and return the result.  E.g.:
--

    # This regexp substitution function replaces the string '('
    # with the string '(?:', but otherwise makes no changes.
    def subfunc(m):
        return re.sub('^\((\?P<[^>]*>)?$', '(?:', m.group())

    # Scan through the regular expression.  If we see any backslashed
--
_java_bin = None
_java_options = []
# [xx] add classpath option to config_java?
def config_java(bin=None, options=None, verbose=True):
    """
    Configure nltk's java interface, by letting nltk know where it can
    find the Java binary, and what extra options (if any) should be
--
            options = options.split()
        _java_options = list(options)

def java(cmd, classpath=None, stdin=None, stdout=None, stderr=None,
         blocking=True):
    """
    Execute the given java command, by opening a subprocess that calls
--
    :param position: The index in the input string where an error occurred.
    :param expected: What was expected when an error occurred.
    """
    def __init__(self, expected, position):
        ValueError.__init__(self, expected, position)
        self.expected = expected
        self.position = position
    def __str__(self):
        return 'Expected %s at %s' % (self.expected, self.position)

_STRING_START_RE = re.compile(r"[uU]?[rR]?(\"\"\"|\'\'\'|\"|\')")
def parse_str(s, start_position):
    """
    If a Python string literal begins at the specified position in the
    given string, then return a tuple ``(val, end_position)``
--
        raise ParseError('valid string (%s)' % e, start)

_PARSE_INT_RE = re.compile(r'-?\d+')
def parse_int(s, start_position):
    """
    If an integer begins at the specified position in the given
    string, then return a tuple ``(val, end_position)`` containing the
--
    return int(m.group()), m.end()

_PARSE_NUMBER_VALUE = re.compile(r'-?(\d*)([.]?\d*)?')
def parse_number(s, start_position):
    """
    If an integer or float begins at the specified position in the
    given string, then return a tuple ``(val, end_position)``
--
# Check if a method has been overridden
######################################################################

def overridden(method):
    """
    :return: True if ``method`` overrides some method with the same
    name in a base class.  This is typically used when defining
    abstract base classes or interfaces, to allow subclasses to define
    either of two related methods:

        >>> class EaterI:
        ...     '''Subclass must define eat() or batch_eat().'''
        ...     def eat(self, food):
        ...         if overridden(self.batch_eat):
        ...             return self.batch_eat([food])[0]
        ...         else:
        ...             raise NotImplementedError()
        ...     def batch_eat(self, foods):
        ...         return [self.eat(food) for food in foods]

    :type method: instance method
--
    else:
        raise TypeError('Expected an instance method.')

def _mro(cls):
    """
    Return the method resolution order for ``cls`` -- i.e., a list
    containing ``cls`` and all its base classes, in the order in which
--
######################################################################
# [xx] dedent msg first if it comes from  a docstring.

def _add_epytext_field(obj, field, message):
    """Add an epytext @field to a given object's docstring."""
    indent = ''
    # If we already have a docstring, then add a blank line to separate
--
                                 initial_indent=indent,
                                 subsequent_indent=indent+'    ')

def deprecated(message):
    """
    A decorator used to mark functions as deprecated.  This will cause
    a warning to be printed the when the function is used.  Usage:

        >>> from nltk.internals import deprecated
        >>> @deprecated('Use foo() instead')
        ... def bar(x):
        ...     print(x/10)

    """

    def decorator(func):
        msg = ("Function %s() has been deprecated.  %s"
               % (func.__name__, message))
        msg = '\n' + textwrap.fill(msg, initial_indent='  ',
                                   subsequent_indent='  ')
        def newFunc(*args, **kwargs):
            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)
            return func(*args, **kwargs)

--
    The docstring of the deprecated class will be used in the
    deprecation warning message.
    """
    def __new__(cls, *args, **kwargs):
        # Figure out which class is the deprecated one.
        dep_cls = None
        for base in _mro(cls):
--
    """
    A counter that auto-increments each time its value is read.
    """
    def __init__(self, initial_value=0):
        self._value = initial_value
    def get(self):
        self._value += 1
        return self._value

--
# Search for files/binaries
##########################################################################

def find_file(filename, env_vars=(), searchpath=(),
        file_names=None, url=None, verbose=True):
    """
    Search for a file to be used by nltk.
--
    div = '='*75
    raise LookupError('\n\n%s\n%s\n%s' % (div, msg, div))

def find_binary(name, path_to_bin=None, env_vars=(), searchpath=(),
                binary_names=None, url=None, verbose=True):
    """
    Search for a file to be used by nltk.
--
# TODO: Add support for jar names specified as regular expressions
##########################################################################

def find_jar(name, path_to_jar=None, env_vars=(),
        searchpath=(), url=None, verbose=True):
    """
    Search for a jar that is used by nltk.
--
    :param path_to_jar: The user-supplied jar location, or None.
    :param env_vars: A list of environment variable names to check
                     in addition to the CLASSPATH variable which is
                     checked by default.
    :param searchpath: List of directories to search.
    """

--
# Import Stdlib Module
##########################################################################

def import_from_stdlib(module):
    """
    When python is run from within the nltk/ directory tree, the
    current directory is included at the beginning of the search path.
--
    """

    # Prevent double-wrapping:
    def __new__(cls, etree):
        """
        Create and return a wrapper around a given Element object.
        If ``etree`` is an ``ElementWrapper``, then ``etree`` is
--
        else:
            return object.__new__(ElementWrapper)

    def __init__(self, etree):
        r"""
        Initialize a new Element wrapper for ``etree``.

--
            etree = ElementTree.fromstring(etree)
        self.__dict__['_etree'] = etree

    def unwrap(self):
        """
        Return the Element object wrapped by this wrapper.
        """
--
    #{ String Representation
    ##////////////////////////////////////////////////////////////

    def __repr__(self):
        s = ElementTree.tostring(self._etree, encoding='utf8').decode('utf8')
        if len(s) > 60:
            e = s.rfind('<')
--
            s = '%s...%s' % (s[:30], s[e:])
        return '<Element %r>' % s

    def __str__(self):
        """
        :return: the result of applying ``ElementTree.tostring()`` to
        the wrapped Element object.
--
    #{ Element interface Delegation (pass-through)
    ##////////////////////////////////////////////////////////////

    def __getattr__(self, attrib):
        return getattr(self._etree, attrib)

    def __setattr__(self, attr, value):
        return setattr(self._etree, attr, value)

    def __delattr__(self, attr):
        return delattr(self._etree, attr)

    def __setitem__(self, index, element):
        self._etree[index] = element

    def __delitem__(self, index):
        del self._etree[index]

    def __setslice__(self, start, stop, elements):
        self._etree[start:stop] = elements

    def __delslice__(self, start, stop):
        del self._etree[start:stop]

    def __len__(self):
        return len(self._etree)

    ##////////////////////////////////////////////////////////////
    #{ Element interface Delegation (wrap result)
    ##////////////////////////////////////////////////////////////

    def __getitem__(self, index):
        return ElementWrapper(self._etree[index])

    def __getslice__(self, start, stop):
        return [ElementWrapper(elt) for elt in self._etree[start:stop]]

    def getchildren(self):
        return [ElementWrapper(elt) for elt in self._etree]

    def getiterator(self, tag=None):
        return (ElementWrapper(elt)
                for elt in self._etree.getiterator(tag))

    def makeelement(self, tag, attrib):
        return ElementWrapper(self._etree.makeelement(tag, attrib))

    def find(self, path):
        elt = self._etree.find(path)
        if elt is None: return elt
        else: return ElementWrapper(elt)

    def findall(self, path):
        return [ElementWrapper(elt) for elt in self._etree.findall(path)]

######################################################################
# Helper for Handling Slicing
######################################################################

def slice_bounds(sequence, slice_obj, allow_step=False):
    """
    Given a slice, return the corresponding (start, stop) bounds,
    taking into account None indices and negative indices.  The
--
        if step is None: step = 1
        # Use a recursive call without allow_step to find the slice
        # bounds.  If step is negative, then the roles of start and
        # stop (in terms of default values, etc), are swapped.
        if step < 0:
            start, stop = slice_bounds(sequence, slice(stop, start))
        else:
            start, stop = slice_bounds(sequence, slice(start, stop))
        return start, stop, step

    # Otherwise, make sure that no non-default step value is used.
    elif slice_obj.step not in (None, 1):
        raise ValueError('slices with steps are not supported by %s' %
                         sequence.__class__.__name__)

    # Supply default offsets.
    if start is None: start = 0
    if stop is None: stop = len(sequence)

--
# Permission Checking
######################################################################

def is_writable(path):
    # Ensure that it exists.
    if not os.path.exists(path):
        return False
--
# NLTK Error reporting
######################################################################

def raise_unorderable_types(ordering, a, b):
    raise TypeError("unorderable types: %s() %s %s()" % (type(a).__name__, ordering, type(b).__name__))


--
    not in the grammar.
  - button/window to change and/or select grammar.  Select from
    several alternative grammars?  Or actually change the grammar?  If
    the later, then I'd want to define nltk.draw.cfg, which would be
    responsible for that.
"""

--
    the parsing process, performing the operations that
    ``nltk.parse.ShiftReduceParser`` would use.
    """
    def __init__(self, grammar, sent, trace=0):
        self._sent = sent
        self._parser = SteppingShiftReduceParser(grammar, trace)

--
    ##  Initialization Helpers
    #########################################

    def _init_fonts(self, root):
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = tkinter.font.Font(font=Button()["font"])
        root.option_add("*Font", self._sysfont)

        # TWhat's our font size (default=same as sysfont)
        self._size = IntVar(root)
        self._size.set(self._sysfont.cget('size'))

--
        self._font = tkinter.font.Font(family='helvetica',
                                    size=self._size.get())

    def _init_grammar(self, parent):
        # Grammar view.
        self._prodframe = listframe = Frame(parent)
        self._prodframe.pack(fill='both', side='left', padx=2)
--
        self._prodlist.bind('<Motion>', self._highlight_hover)
        self._prodlist.bind('<Leave>', self._clear_hover)

    def _init_bindings(self):
        # Quit
        self._top.bind('<Control-q>', self.destroy)
        self._top.bind('<Control-x>', self.destroy)
--
        self._top.bind('=', lambda e,a=self._animate:a.set(10))
        self._top.bind('+', lambda e,a=self._animate:a.set(4))

    def _init_buttons(self, parent):
        # Set up the frames.
        self._buttonframe = buttonframe = Frame(parent)
        buttonframe.pack(fill='none', side='bottom')
--
               background='#f0a0a0', foreground='black',
               command=self.undo).pack(side='left')

    def _init_menubar(self, parent):
        menubar = Menu(parent)

        filemenu = Menu(menubar, tearoff=0)
--

        parent.config(menu=menubar)

    def _init_feedback(self, parent):
        self._feedbackframe = feedbackframe = Frame(parent)
        feedbackframe.pack(fill='x', side='bottom', padx=3, pady=3)
        self._lastoper_label = Label(feedbackframe, text='Last Operation:',
--
        self._lastoper1.pack(side='left')
        self._lastoper2.pack(side='left', fill='x', expand=1)

    def _init_canvas(self, parent):
        self._cframe = CanvasFrame(parent, background='white',
                                   width=525, closeenough=10,
                                   border=2, relief='sunken')
--
    ##  Main draw procedure
    #########################################

    def _redraw(self):
        scrollregion = self._canvas['scrollregion'].split()
        (cx1, cy1, cx2, cy2) = [int(c) for c in scrollregion]

--

        # Set up binding to allow them to shift a token by dragging it.
        if len(self._rtextwidgets) > 0:
            def drag_shift(widget, midx=midx, self=self):
                if widget.bbox()[0] < midx: self.shift()
                else: self._redraw()
            self._rtextwidgets[0].bind_drag(drag_shift)
--
        # Draw the stack top.
        self._highlight_productions()

    def _draw_stack_top(self, widget):
        # hack..
        midx = widget.bbox()[2]+50
        self._canvas.coords(self._stacktop, midx, 0, midx, 5000)

    def _highlight_productions(self):
        # Highlight the productions that can be reduced.
        self._prodlist.selection_clear(0, 'end')
        for prod in self._parser.reducible_productions():
--
    ##  Button Callbacks
    #########################################

    def destroy(self, *e):
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def reset(self, *e):
        self._parser.initialize(self._sent)
        self._lastoper1['text'] = 'Reset App'
        self._lastoper2['text'] = ''
        self._redraw()

    def step(self, *e):
        if self.reduce(): return 1
        elif self.shift(): return 1
        else:
--
                self._lastoper1['text'] = 'Finished:'
                self._lastoper2['text'] = 'Failure'

    def shift(self, *e):
        if self._animating_lock: return
        if self._parser.shift():
            tok = self._parser.stack()[-1]
--
            return 1
        return 0

    def reduce(self, *e):
        if self._animating_lock: return
        production = self._parser.reduce()
        if production:
--
                self._redraw()
        return production

    def undo(self, *e):
        if self._animating_lock: return
        if self._parser.undo():
            self._redraw()

    def postscript(self, *e):
        self._cframe.print_to_file()

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
    ##  Menubar callbacks
    #########################################

    def resize(self, size=None):
        if size is not None: self._size.set(size)
        size = self._size.get()
        self._font.configure(size=-(abs(size)))
--
        #self._prodlist_label['font'] = ('helvetica', -size-2, 'bold')
        self._redraw()

    def help(self, *e):
        # The default font's not very legible; try using 'fixed' instead.
        try:
            ShowText(self._top, 'Help: Shift-Reduce Parser Application',
                     (__doc__ or '').strip(), width=75, font='fixed')
--
            ShowText(self._top, 'Help: Shift-Reduce Parser Application',
                     (__doc__ or '').strip(), width=75)

    def about(self, *e):
        ABOUT = ("NLTK Shift-Reduce Parser Application\n"+
                 "Written by Edward Loper")
        TITLE = 'About: Shift-Reduce Parser Application'
--
        except:
            ShowText(self._top, TITLE, ABOUT)

    def edit_grammar(self, *e):
        CFGEditor(self._top, self._parser.grammar(), self.set_grammar)

    def set_grammar(self, grammar):
        self._parser.set_grammar(grammar)
        self._productions = list(grammar.productions())
        self._prodlist.delete(0, 'end')
        for production in self._productions:
            self._prodlist.insert('end', (' %s' % production))

    def edit_sentence(self, *e):
        sentence = " ".join(self._sent)
        title = 'Edit Text'
        instr = 'Enter a new sentence to parse.'
        EntryDialog(self._top, sentence, instr, self.set_sentence, title)

    def set_sentence(self, sent):
        self._sent = sent.split() #[XX] use tagged?
        self.reset()

--
    ##  Reduce Production Selection
    #########################################

    def _toggle_grammar(self, *e):
        if self._show_grammar.get():
            self._prodframe.pack(fill='both', side='left', padx=2,
                                 after=self._feedbackframe)
--
            self._lastoper1['text'] = 'Hide Grammar'
        self._lastoper2['text'] = ''

    def _prodlist_select(self, event):
        selection = self._prodlist.curselection()
        if len(selection) != 1: return
        index = int(selection[0])
--
                index = self._productions.index(prod)
                self._prodlist.selection_set(index)

    def _popup_reduce(self, widget):
        # Remove old commands.
        productions = self._parser.reducible_productions()
        if len(productions) == 0: return
--
    ##  Animations
    #########################################

    def _animate_shift(self):
        # What widget are we shifting?
        widget = self._rtextwidgets[0]

--
        dx = (left-right)*1.0/dt
        self._animate_shift_frame(dt, widget, dx)

    def _animate_shift_frame(self, frame, widget, dx):
        if frame > 0:
            self._animating_lock = 1
            widget.move(dx, 0)
--
            self._draw_stack_top(widget)
            self._highlight_productions()

    def _animate_reduce(self):
        # What widgets are we shifting?
        numwidgets = len(self._parser.stack()[-1]) # number of children
        widgets = self._stackwidgets[-numwidgets:]
--
        dy = ydist*2.0/dt
        self._animate_reduce_frame(dt/2, widgets, dy)

    def _animate_reduce_frame(self, frame, widgets, dy):
        if frame > 0:
            self._animating_lock = 1
            for widget in widgets: widget.move(0, dy)
--
    ##  Hovering.
    #########################################

    def _highlight_hover(self, event):
        # What production are we hovering over?
        index = self._prodlist.nearest(event.y)
        if self._hover == index: return
--
        # Remember what production we're hovering over.
        self._hover = index

    def _clear_hover(self, *event):
        # Clear any previous hover highlighting.
        if self._hover == -1: return
        self._hover = -1
--
                stackwidget['color'] = 'black'


def app():
    """
    Create a shift reduce parser app, using a simple grammar and
    text.
--
        from .wordfreq_app import app as wordfreq

# skip doctests from this package
def setup_module(module):
    from nose import SkipTest
    raise SkipTest("nltk.app examples are not doctests")
--
ERROR_LOADING_CORPUS_EVENT = '<<ELC_EVENT>>'
POLL_INTERVAL = 100

_DEFAULT = 'English: Brown Corpus (Humor)'
_CORPORA = {
            'Catalan: CESS-CAT Corpus':
                lambda: cess_cat.words(),
--
class CollocationsView:
    _BACKGROUND_COLOUR='#FFF' #white

    def __init__(self):
        self.queue = q.Queue()
        self.model = CollocationsModel(self.queue)
        self.top = Tk()
        self._init_top(self.top)
        self._init_menubar()
        self._init_widgets(self.top)
        self.load_corpus(self.model.DEFAULT_CORPUS)
        self.after = self.top.after(POLL_INTERVAL, self._poll)

    def _init_top(self, top):
        top.geometry('550x650+50+50')
        top.title('NLTK Collocations List')
        top.bind('<Control-q>', self.destroy)
        top.protocol('WM_DELETE_WINDOW', self.destroy)
        top.minsize(550,650)

    def _init_widgets(self, parent):
        self.main_frame = Frame(parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1))
        self._init_corpus_select(self.main_frame)
        self._init_results_box(self.main_frame)
--
        self._init_status(self.main_frame)
        self.main_frame.pack(fill='both', expand=True)

    def _init_corpus_select(self, parent):
        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
        self.var = StringVar(innerframe)
        self.var.set(self.model.DEFAULT_CORPUS)
        Label(innerframe, justify=LEFT, text=' Corpus: ', background=self._BACKGROUND_COLOUR, padx = 2, pady = 1, border = 0).pack(side='left')

        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)
        om = OptionMenu(innerframe, self.var, self.model.DEFAULT_CORPUS, command=self.corpus_selected, *self.model.non_default_corpora())
        om['borderwidth'] = 0
        om['highlightthickness'] = 1
        om.pack(side='left')
        innerframe.pack(side='top', fill='x', anchor='n')

    def _init_status(self, parent):
        self.status = Label(parent, justify=LEFT, relief=SUNKEN, background=self._BACKGROUND_COLOUR, border=0, padx = 1, pady = 0)
        self.status.pack(side='top', anchor='sw')

    def _init_menubar(self):
        self._result_size = IntVar(self.top)
        menubar = Menu(self.top)

--
        menubar.add_cascade(label='Edit', underline=0, menu=editmenu)
        self.top.config(menu=menubar)

    def set_result_size(self, **kwargs):
        self.model.result_count = self._result_size.get()

    def _init_results_box(self, parent):
        innerframe = Frame(parent)
        i1 = Frame(innerframe)
        i2 = Frame(innerframe)
--
        i2.pack(side='bottom', fill='x', anchor='s')
        innerframe.pack(side='top', fill='both', expand=True)

    def _init_paging(self, parent):
        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
        self.prev = prev = Button(innerframe, text='Previous', command=self.previous, width='10', borderwidth=1, highlightthickness=1, state='disabled')
        prev.pack(side='left', anchor='center')
--
        innerframe.pack(side='top', fill='y')
        self.reset_current_page()

    def reset_current_page(self):
        self.current_page = -1

    def _poll(self):   
        try:
            event = self.queue.get(block=False)
        except q.Empty:
--
                self.handle_error_loading_corpus(event)
        self.after = self.top.after(POLL_INTERVAL, self._poll)

    def handle_error_loading_corpus(self, event):
        self.status['text'] = 'Error in loading ' + self.var.get()
        self.unfreeze_editable()
        self.clear_results_box()
        self.freeze_editable()
        self.reset_current_page()

    def handle_corpus_loaded(self, event):
        self.status['text'] = self.var.get() + ' is loaded'
        self.unfreeze_editable()
        self.clear_results_box()
--
        self.write_results(collocations)
        self.current_page += 1

    def corpus_selected(self, *args):
        new_selection = self.var.get()
        self.load_corpus(new_selection)

    def previous(self):
        self.freeze_editable()
        collocations = self.model.prev(self.current_page - 1)
        self.current_page= self.current_page - 1
--
        self.write_results(collocations)
        self.unfreeze_editable()

    def __next__(self):
        self.freeze_editable()
        collocations = self.model.next(self.current_page + 1)
        self.clear_results_box()
--
        self.current_page += 1
        self.unfreeze_editable()

    def load_corpus(self, selection):
        if self.model.selected_corpus != selection:
            self.status['text'] = 'Loading ' + selection + '...'
            self.freeze_editable()
            self.model.load_corpus(selection)

    def freeze_editable(self):
        self.prev['state'] = 'disabled'
        self.next['state'] = 'disabled'

    def clear_results_box(self):
        self.results_box['state'] = 'normal'
        self.results_box.delete("1.0", END)
        self.results_box['state'] = 'disabled'

    def fire_event(self, event):
        #Firing an event so that rendering of widgets happen in the mainloop thread
        self.top.event_generate(event, when='tail')

    def destroy(self, *e):
        if self.top is None: return
        self.top.after_cancel(self.after)
        self.top.destroy()
        self.top = None

    def mainloop(self, *args, **kwargs):
        if in_idle(): return
        self.top.mainloop(*args, **kwargs)

    def unfreeze_editable(self):
        self.set_paging_button_states()

    def set_paging_button_states(self):
        if self.current_page == -1 or self.current_page == 0:
            self.prev['state'] = 'disabled'
        else:
--
        else:
            self.next['state'] = 'normal'

    def write_results(self, results):
        self.results_box['state'] = 'normal'
        row = 1
        for each in results:
--
        self.results_box['state'] = 'disabled'

class CollocationsModel:
    def __init__(self, queue):
        self.result_count = None
        self.selected_corpus = None
        self.collocations = None
        self.CORPORA = _CORPORA
        self.DEFAULT_CORPUS = _DEFAULT
        self.queue = queue
        self.reset_results()

    def reset_results(self):
        self.result_pages = []
        self.results_returned = 0

    def load_corpus(self, name):
        self.selected_corpus = name
        self.collocations = None
        runner_thread = self.LoadCorpus(name, self)
        runner_thread.start()
        self.reset_results()

    def non_default_corpora(self):
        copy = []
        copy.extend(list(self.CORPORA.keys()))
        copy.remove(self.DEFAULT_CORPUS)
        copy.sort()
        return copy

    def is_last_page(self, number):
        if number < len(self.result_pages):
            return False
        return self.results_returned + (number - len(self.result_pages)) * self.result_count >= len(self.collocations)

    def next(self, page):
        if (len(self.result_pages) - 1) < page:
            for i in range(page - (len(self.result_pages) - 1)):
                self.result_pages.append(self.collocations[self.results_returned:self.results_returned+self.result_count])
                self.results_returned += self.result_count
        return self.result_pages[page]

    def prev(self, page):
        if page == -1:
            return []
        return self.result_pages[page]

    class LoadCorpus(threading.Thread):
        def __init__(self, name, model):
            threading.Thread.__init__(self)
            self.model, self.name = model, name

        def run(self):
            try:
                words = self.model.CORPORA[self.name]()
                from operator import itemgetter
--
                print(e)
                self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)

#def collocations():
#    colloc_strings = [w1 + ' ' + w2 for w1, w2 in self._collocations[:num]]

def app():
    c = CollocationsView()
    c.mainloop()

--
    through the parsing process, performing the operations that
    ``RecursiveDescentParser`` would use.
    """
    def __init__(self, grammar, sent, trace=0):
        self._sent = sent
        self._parser = SteppingRecursiveDescentParser(grammar, trace)

--
    ##  Initialization Helpers
    #########################################

    def _init_fonts(self, root):
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = tkinter.font.Font(font=Button()["font"])
        root.option_add("*Font", self._sysfont)

        # TWhat's our font size (default=same as sysfont)
        self._size = IntVar(root)
        self._size.set(self._sysfont.cget('size'))

--
        self._bigfont = tkinter.font.Font(family='helvetica', weight='bold',
                                    size=big)

    def _init_grammar(self, parent):
        # Grammar view.
        self._prodframe = listframe = Frame(parent)
        self._prodframe.pack(fill='both', side='left', padx=2)
--
        # If they select a production, apply it.
        self._prodlist.bind('<<ListboxSelect>>', self._prodlist_select)

    def _init_bindings(self):
        # Key bindings are a good thing.
        self._top.bind('<Control-q>', self.destroy)
        self._top.bind('<Control-x>', self.destroy)
--
        self._top.bind('<Control-g>', self.edit_grammar)
        self._top.bind('<Control-t>', self.edit_sentence)

    def _init_buttons(self, parent):
        # Set up the frames.
        self._buttonframe = buttonframe = Frame(parent)
        buttonframe.pack(fill='none', side='bottom', padx=3, pady=2)
--
#                                        underline=0, command=self.autostep)
#         self._autostep_button.pack(side='left')

    def _configure(self, event):
        self._autostep = 0
        (x1, y1, x2, y2) = self._cframe.scrollregion()
        y2 = event.height - 6
        self._canvas['scrollregion'] = '%d %d %d %d' % (x1,y1,x2,y2)
        self._redraw()

    def _init_feedback(self, parent):
        self._feedbackframe = feedbackframe = Frame(parent)
        feedbackframe.pack(fill='x', side='bottom', padx=3, pady=3)
        self._lastoper_label = Label(feedbackframe, text='Last Operation:',
--
        self._lastoper1.pack(side='left')
        self._lastoper2.pack(side='left', fill='x', expand=1)

    def _init_canvas(self, parent):
        self._cframe = CanvasFrame(parent, background='white',
                                   #width=525, height=250,
                                   closeenough=10,
--
        self._textwidgets = []
        self._textline = None

    def _init_menubar(self, parent):
        menubar = Menu(parent)

        filemenu = Menu(menubar, tearoff=0)
--
    ##  Helper
    #########################################

    def _get(self, widget, treeloc):
        for i in treeloc: widget = widget.subtrees()[i]
        if isinstance(widget, TreeSegmentWidget):
            widget = widget.node()
--
    ##  Main draw procedure
    #########################################

    def _redraw(self):
        canvas = self._canvas

        # Delete the old tree, widgets, etc.
--
        self._position_text()


    def _redraw_quick(self):
        # This should be more-or-less sufficient after an animation.
        self._highlight_nodes()
        self._highlight_prodlist()
        self._position_text()

    def _highlight_nodes(self):
        # Highlight the list of nodes to be checked.
        bold = ('helvetica', -self._size.get(), 'bold')
        for treeloc in self._parser.frontier()[:1]:
--
        for treeloc in self._parser.frontier()[1:]:
            self._get(self._tree, treeloc)['color'] = '#008080'

    def _highlight_prodlist(self):
        # Highlight the productions that can be expanded.
        # Boy, too bad tkinter doesn't implement Listbox.itemconfig;
        # that would be pretty useful here.
--
            else:
                self._prodlist.insert(index, ' %s' % productions[index])

    def _position_text(self):
        # Line up the text widgets that are matched against the tree
        numwords = len(self._sent)
        num_matched = numwords - len(self._parser.remaining_text())
--
            dy = max(dy, leaf.parent().node().bbox()[3] - leaf.bbox()[3] + 10)
            leaf.move(0, dy)

    def _tree_leaves(self, tree=None):
        if tree is None: tree = self._tree
        if isinstance(tree, TreeSegmentWidget):
            leaves = []
--
    ##  Button Callbacks
    #########################################

    def destroy(self, *e):
        self._autostep = 0
        if self._top is None: return
        self._top.destroy()
        self._top = None

    def reset(self, *e):
        self._autostep = 0
        self._parser.initialize(self._sent)
        self._lastoper1['text'] = 'Reset Application'
        self._lastoper2['text'] = ''
        self._redraw()

    def autostep(self, *e):
        if self._animation_frames.get() == 0:
            self._animation_frames.set(2)
        if self._autostep:
--
            self._autostep = 1
            self._step()

    def cancel_autostep(self, *e):
        #self._autostep_button['text'] = 'Autostep'
        self._autostep = 0

    # Make sure to stop auto-stepping if we get any user input.
    def step(self, *e): self._autostep = 0; self._step()
    def match(self, *e): self._autostep = 0; self._match()
    def expand(self, *e): self._autostep = 0; self._expand()
    def backtrack(self, *e): self._autostep = 0; self._backtrack()

    def _step(self):
        if self._animating_lock: return

        # Try expanding, matching, and backtracking (in that order)
--
            self._autostep = 0
            self._lastoper2['text'] += '    [COMPLETE PARSE]'

    def _expand(self, *e):
        if self._animating_lock: return
        old_frontier = self._parser.frontier()
        rv = self._parser.expand()
--
            self._lastoper2['text'] = '(all expansions tried)'
            return 0

    def _match(self, *e):
        if self._animating_lock: return
        old_frontier = self._parser.frontier()
        rv = self._parser.match()
--
            self._lastoper2['text'] = '(failed)'
            return 0

    def _backtrack(self, *e):
        if self._animating_lock: return
        if self._parser.backtrack():
            elt = self._parser.tree()
--
            self._lastoper2['text'] = ''
            return 0

    def about(self, *e):
        ABOUT = ("NLTK Recursive Descent Parser Application\n"+
                 "Written by Edward Loper")
        TITLE = 'About: Recursive Descent Parser Application'
--
        except:
            ShowText(self._top, TITLE, ABOUT)

    def help(self, *e):
        self._autostep = 0
        # The default font's not very legible; try using 'fixed' instead.
        try:
            ShowText(self._top, 'Help: Recursive Descent Parser Application',
                     (__doc__ or '').strip(), width=75, font='fixed')
--
            ShowText(self._top, 'Help: Recursive Descent Parser Application',
                     (__doc__ or '').strip(), width=75)

    def postscript(self, *e):
        self._autostep = 0
        self._cframe.print_to_file()

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
        if in_idle(): return
        self._top.mainloop(*args, **kwargs)

    def resize(self, size=None):
        if size is not None: self._size.set(size)
        size = self._size.get()
        self._font.configure(size=-(abs(size)))
--
    ##  Expand Production Selection
    #########################################

    def _toggle_grammar(self, *e):
        if self._show_grammar.get():
            self._prodframe.pack(fill='both', side='left', padx=2,
                                 after=self._feedbackframe)
--
            self._lastoper1['text'] = 'Hide Grammar'
        self._lastoper2['text'] = ''

#     def toggle_grammar(self, *e):
#         self._show_grammar = not self._show_grammar
#         if self._show_grammar:
#             self._prodframe.pack(fill='both', expand='y', side='left',
--
#             self._lastoper1['text'] = 'Hide Grammar'
#         self._lastoper2['text'] = ''

    def _prodlist_select(self, event):
        selection = self._prodlist.curselection()
        if len(selection) != 1: return
        index = int(selection[0])
--
    ##  Animation
    #########################################

    def _animate_expand(self, treeloc):
        oldwidget = self._get(self._tree, treeloc)
        oldtree = oldwidget.parent()
        top = not isinstance(oldtree.parent(), TreeSegmentWidget)
--

        self._animate_expand_frame(widget, colors)

    def _makeroom(self, treeseg):
        """
        Make sure that no sibling tree bbox's overlap.
        """
--
        # Keep working up the tree.
        self._makeroom(parent)

    def _animate_expand_frame(self, widget, colors):
        if len(colors) > 0:
            self._animating_lock = 1
            widget['color'] = colors[0]
--
            self._animating_lock = 0
            if self._autostep: self._step()

    def _animate_backtrack(self, treeloc):
        # Flash red first, if we're animating.
        if self._animation_frames.get() == 0: colors = []
        else: colors = ['#a00000', '#000000', '#a00000']
--

        self._animate_backtrack_frame(widgets, colors)

    def _animate_backtrack_frame(self, widgets, colors):
        if len(colors) > 0:
            self._animating_lock = 1
            for widget in widgets: widget['color'] = colors[0]
--
            self._animating_lock = 0
            if self._autostep: self._step()

    def _animate_match_backtrack(self, treeloc):
        widget = self._get(self._tree, treeloc)
        node = widget.parent().node()
        dy = (1.0 * (node.bbox()[3] - widget.bbox()[1] + 14) /
--
        self._animate_match_backtrack_frame(self._animation_frames.get(),
                                            widget, dy)

    def _animate_match(self, treeloc):
        widget = self._get(self._tree, treeloc)

        dy = ((self._textwidgets[0].bbox()[1] - widget.bbox()[3] - 10.0) /
              max(1, self._animation_frames.get()))
        self._animate_match_frame(self._animation_frames.get(), widget, dy)

    def _animate_match_frame(self, frame, widget, dy):
        if frame > 0:
            self._animating_lock = 1
            widget.move(0, dy)
--
            self._animating_lock = 0
            if self._autostep: self._step()

    def _animate_match_backtrack_frame(self, frame, widget, dy):
        if frame > 0:
            self._animating_lock = 1
            widget.move(0, dy)
--
            self._animating_lock = 0
            if self._autostep: self._step()

    def edit_grammar(self, *e):
        CFGEditor(self._top, self._parser.grammar(), self.set_grammar)

    def set_grammar(self, grammar):
        self._parser.set_grammar(grammar)
        self._productions = list(grammar.productions())
        self._prodlist.delete(0, 'end')
        for production in self._productions:
            self._prodlist.insert('end', (' %s' % production))

    def edit_sentence(self, *e):
        sentence = " ".join(self._sent)
        title = 'Edit Text'
        instr = 'Enter a new sentence to parse.'
        EntryDialog(self._top, sentence, instr, self.set_sentence, title)

    def set_sentence(self, sentence):
        self._sent = sentence.split() #[XX] use tagged?
        self.reset()

def app():
    """
    Create a recursive descent parser demo, using a simple grammar and
    text.
--


class Zone:
    def __init__(self, image, initialField, initialText):
        frm = tk.Frame(root)
        frm.config(background="white")
        self.image = tk.PhotoImage(format='gif',data=images[image.upper()])
--
        for i in range(2):
            self.txt.tag_config(colors[i], background = colors[i])
            self.txt.tag_config("emph"+colors[i], foreground = emphColors[i])
    def initScrollText(self,frm,txt,contents):
        scl = tk.Scrollbar(frm)
        scl.config(command = txt.yview)
        scl.pack(side="right",fill="y")
--
        txt.insert("1.0",contents)
        frm.pack(fill = "x")
        tk.Frame(height=2, bd=1, relief="ridge").pack(fill="x")
    def refresh(self):
        self.colorCycle = itertools.cycle(colors)
        try:
            self.substitute()
--


class FindZone(Zone):
    def addTags(self,m):
        color = next(self.colorCycle)
        self.txt.tag_add(color,"1.0+%sc"%m.start(),"1.0+%sc"%m.end())
        try:
--
                             "1.0+%sc"%m.end("emph"))
        except:
            pass
    def substitute(self,*args):
        for color in colors:
            self.txt.tag_remove(color,"1.0","end")
            self.txt.tag_remove("emph"+color,"1.0","end")
        self.rex = re.compile("") # default value in case of misformed regexp
        self.rex = re.compile(self.fld.get("1.0","end")[:-1],re.MULTILINE)
        try:
            re.compile("(?P<emph>%s)" % self.fld.get(tk.SEL_FIRST,
--


class ReplaceZone(Zone):
    def addTags(self,m):
        s = sz.rex.sub(self.repl,m.group())
        self.txt.delete("1.0+%sc"%(m.start()+self.diff),
                        "1.0+%sc"%(m.end()+self.diff))
        self.txt.insert("1.0+%sc"%(m.start()+self.diff),s,
                        next(self.colorCycle))
        self.diff += len(s) - (m.end() - m.start())
    def substitute(self):
        self.txt.delete("1.0","end")
        self.txt.insert("1.0",sz.txt.get("1.0","end")[:-1])
        self.diff = 0
--
        sz.rex.sub(self.addTags,sz.txt.get("1.0","end")[:-1])


def launchRefresh(_):
    sz.fld.after_idle(sz.refresh)
    rz.fld.after_idle(rz.refresh)


def app():
    global root, sz, rz, rex0
    root = tk.Tk()
    root.resizable(height=False,width=True)
--
class EdgeList(ColorizedList):
    ARROW = SymbolWidget.SYMBOLS['rightarrow']

    def _init_colortags(self, textwidget, options):
        textwidget.tag_config('terminal', foreground='#006000')
        textwidget.tag_config('arrow', font='symbol', underline='0')
        textwidget.tag_config('dot', foreground = '#000000')
        textwidget.tag_config('nonterminal', foreground='blue',
                              font=('helvetica', -12, 'bold'))

    def _item_repr(self, item):
        contents = []
        contents.append(('%s\t' % item.lhs(), 'nonterminal'))
        contents.append((self.ARROW, 'arrow'))
--
    """
    A view of a chart that displays the contents of the corresponding matrix.
    """
    def __init__(self, parent, chart, toplevel=True, title='Chart Matrix',
                 show_numedges=False):
        self._chart = chart
        self._cells = []
--

        self.draw()

    def _init_quit(self, root):
        quit = tkinter.Button(root, text='Quit', command=self.destroy)
        quit.pack(side='bottom', expand=0, fill='none')

    def _init_matrix(self, root):
        cframe = tkinter.Frame(root, border=2, relief='sunken')
        cframe.pack(expand=0, fill='none', padx=1, pady=3, side='top')
        self._canvas = tkinter.Canvas(cframe, width=200, height=200,
                                      background='white')
        self._canvas.pack(expand=0, fill='none')

    def _init_numedges(self, root):
        self._numedges_label = tkinter.Label(root, text='0 edges')
        self._numedges_label.pack(expand=0, fill='none', side='top')

    def _init_list(self, root):
        self._list = EdgeList(root, [], width=20, height=5)
        self._list.pack(side='top', expand=1, fill='both', pady=3)
        def cb(edge, self=self): self._fire_callbacks('select', edge)
        self._list.add_callback('select', cb)
        self._list.focus()

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def set_chart(self, chart):
        if chart is not self._chart:
            self._chart = chart
            self._num_edges = 0
            self.draw()

    def update(self):
        if self._root is None: return

        # Count the edges in each cell
--
        if self._numedges_label is not None:
            self._numedges_label['text'] = '%d edges' % self._num_edges

    def activate(self):
        self._canvas.itemconfig('inactivebox', state='hidden')
        self.update()

    def inactivate(self):
        self._canvas.itemconfig('inactivebox', state='normal')
        self.update()

    def add_callback(self, event, func):
        self._callbacks.setdefault(event,{})[func] = 1

    def remove_callback(self, event, func=None):
        if func is None: del self._callbacks[event]
        else:
            try: del self._callbacks[event][func]
            except: pass

    def _fire_callbacks(self, event, *args):
        if event not in self._callbacks: return
        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)

    def select_cell(self, i, j):
        if self._root is None: return

        # If the cell is already selected (and the chart contents
--
        # Fire the callback.
        self._fire_callbacks('select_cell', i, j)

    def deselect_cell(self):
        if self._root is None: return
        self._selected_cell = None
        self._list.set([])
        self.update()

    def _click_cell(self, i, j):
        if self._selected_cell == (i,j):
            self.deselect_cell()
        else:
            self.select_cell(i, j)

    def view_edge(self, edge):
        self.select_cell(*edge.span())
        self._list.view(edge)

    def mark_edge(self, edge):
        if self._root is None: return
        self.select_cell(*edge.span())
        self._list.mark(edge)

    def unmark_edge(self, edge=None):
        if self._root is None: return
        self._list.unmark(edge)

    def markonly_edge(self, edge):
        if self._root is None: return
        self.select_cell(*edge.span())
        self._list.markonly(edge)

    def draw(self):
        if self._root is None: return
        LEFT_MARGIN = BOT_MARGIN = 15
        TOP_MARGIN = 5
--
                                       (i+1)*dy+TOP_MARGIN,
                                       fill='gray20')
                self._cells[i][j] = t
                def cb(event, self=self, i=i, j=j): self._click_cell(i,j)
                c.tag_bind(t, '<Button-1>', cb)

        # Inactive box
--
        # Update the cells.
        self.update()

    def pack(self, *args, **kwargs):
        self._root.pack(*args, **kwargs)

#######################################################################
--
#######################################################################

class ChartResultsView(object):
    def __init__(self, parent, chart, grammar, toplevel=True):
        self._chart = chart
        self._grammar = grammar
        self._trees = []
--
        # Initial update
        self.update()

    def update(self, edge=None):
        if self._root is None: return
        # If the edge isn't a parse edge, do nothing.
        if edge is not None:
--
            if parse not in self._trees:
                self._add(parse)

    def _add(self, parse):
        # Add it to self._trees.
        self._trees.append(parse)

--
        # Update y.
        self._y = treewidget.bbox()[3] + 10

    def _click(self, widget):
        c = self._cframe.canvas()
        if self._selection is not None:
            c.delete(self._selectbox)
--
        self._selectbox = c.create_rectangle(x1, y1, x2, y2,
                                             width=2, outline='#088')

    def _color(self, treewidget, color):
        treewidget.node()['color'] = color
        for child in treewidget.subtrees():
            if isinstance(child, TreeSegmentWidget):
--
            else:
                child['color'] = color

    def print_all(self, *e):
        if self._root is None: return
        self._cframe.print_to_file()

    def print_selection(self, *e):
        if self._root is None: return
        if self._selection is None:
            tkinter.messagebox.showerror('Print Error', 'No tree selected')
--
            self.clear()
            self.update()

    def clear(self):
        if self._root is None: return
        for treewidget in self._treewidgets:
            self._cframe.destroy_widget(treewidget)
--
        self._selection = None
        self._y = 10

    def set_chart(self, chart):
        self.clear()
        self._chart = chart
        self.update()

    def set_grammar(self, grammar):
        self.clear()
        self._grammar = grammar
        self.update()

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def pack(self, *args, **kwargs):
        self._root.pack(*args, **kwargs)

#######################################################################
--
                 'and': SymbolWidget.SYMBOLS['intersection'],
                 'or': SymbolWidget.SYMBOLS['union']}

    def __init__(self, *chart_filenames):
        # This chart is displayed when we don't have a value (eg
        # before any chart is loaded).
        faketok = [''] * 8
--
        for filename in chart_filenames:
            self.load_chart(filename)

    def destroy(self, *e):
        if self._root is None: return
        try: self._root.destroy()
        except: pass
        self._root = None

    def mainloop(self, *args, **kwargs):
        return
        self._root.mainloop(*args, **kwargs)

--
    # Initialization
    #////////////////////////////////////////////////////////////

    def _init_menubar(self, root):
        menubar = tkinter.Menu(root)

        # File menu
--
        # Add the menu
        self._root.config(menu=menubar)

    def _init_divider(self, root):
        divider = tkinter.Frame(root, border=2, relief='sunken')
        divider.pack(side='top', fill='x', ipady=2)

    def _init_chartviews(self, root):
        opfont=('symbol', -36) # Font for operator.
        eqfont=('helvetica', -36) # Font for equals sign.

--
        self._out_matrix.add_callback('select_cell', self.select_cell)
        self._out_matrix.inactivate()

    def _init_buttons(self, root):
        buttons = tkinter.Frame(root)
        buttons.pack(side='bottom', pady=5, fill='x', expand=0)
        tkinter.Button(buttons, text='Intersection',
--
        tkinter.Button(buttons, text='Detatch Output',
                       command=self._detatch_out).pack(side='right')

    def _init_bindings(self, root):
        #root.bind('<Control-s>', self.save_chart)
        root.bind('<Control-o>', self.load_chart_dialog)
        #root.bind('<Control-r>', self.reset)
--
    # Input Handling
    #////////////////////////////////////////////////////////////

    def _select_left(self, name):
        self._left_name = name
        self._left_chart = self._charts[name]
        self._left_matrix.set_chart(self._left_chart)
        if name == 'None': self._left_matrix.inactivate()
        self._apply_op()

    def _select_right(self, name):
        self._right_name = name
        self._right_chart = self._charts[name]
        self._right_matrix.set_chart(self._right_chart)
        if name == 'None': self._right_matrix.inactivate()
        self._apply_op()

    def _apply_op(self):
        if self._operator == '-': self._difference()
        elif self._operator == 'or': self._union()
        elif self._operator == 'and': self._intersection()
--
    CHART_FILE_TYPES = [('Pickle file', '.pickle'),
                        ('All files', '*')]

    def save_chart_dialog(self, *args):
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
                                     defaultextension='.pickle')
        if not filename: return
        try: pickle.dump((self._out_chart), open(filename, 'w'))
        except Exception as e:
--
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))

    def load_chart_dialog(self, *args):
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
                                   defaultextension='.pickle')
        if not filename: return
        try: self.load_chart(filename)
        except Exception as e:
--
                                   'Unable to open file: %r\n%s' %
                                   (filename, e))

    def load_chart(self, filename):
        chart = pickle.load(open(filename, 'r'))
        name = os.path.basename(filename)
        if name.endswith('.pickle'): name = name[:-7]
--
        elif self._right_chart is self._emptychart:
            self._right_selector.set(name)

    def _update_chartviews(self):
        self._left_matrix.update()
        self._right_matrix.update()
        self._out_matrix.update()
--
    # Selection
    #////////////////////////////////////////////////////////////

    def select_edge(self, edge):
        if edge in self._left_chart:
            self._left_matrix.markonly_edge(edge)
        else:
--
        else:
            self._out_matrix.unmark_edge()

    def select_cell(self, i, j):
        self._left_matrix.select_cell(i, j)
        self._right_matrix.select_cell(i, j)
        self._out_matrix.select_cell(i, j)
--
    # Operations
    #////////////////////////////////////////////////////////////

    def _difference(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
--

        self._update('-', out_chart)

    def _intersection(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
--

        self._update('and', out_chart)

    def _union(self):
        if not self._checkcompat(): return

        out_chart = Chart(self._left_chart.tokens())
--

        self._update('or', out_chart)

    def _swapcharts(self):
        left, right = self._left_name, self._right_name
        self._left_selector.set(right)
        self._right_selector.set(left)

    def _checkcompat(self):
        if (self._left_chart.tokens() != self._right_chart.tokens() or
            self._left_chart.property_names() !=
            self._right_chart.property_names() or
--
        else:
            return True

    def _update(self, operator, out_chart):
        self._operator = operator
        self._op_label['text'] = self._OPSYMBOL[operator]
        self._out_chart = out_chart
--
                                                self._operator,
                                                self._right_name)

    def _clear_out_chart(self):
        self._out_chart = self._emptychart
        self._out_matrix.set_chart(self._out_chart)
        self._op_label['text'] = ' '
        self._out_matrix.inactivate()

    def _detatch_out(self):
        ChartMatrixView(self._root, self._out_chart,
                        title=self._out_label['text'])

--
    _TREE_LEVEL_SIZE = 12
    _CHART_LEVEL_SIZE = 40

    def __init__(self, chart, root=None, **kw):
        """
        Construct a new ``Chart`` display.
        """
--
        if root is None:
            top = tkinter.Tk()
            top.title('Chart View')
            def destroy1(e, top=top): top.destroy()
            def destroy2(top=top): top.destroy()
            top.bind('q', destroy1)
            b = tkinter.Button(top, text='Done', command=destroy2)
            b.pack(side='bottom')
--
        # the window is resized.
        self._chart_canvas.bind('<Configure>', self._configure)

    def _init_fonts(self, root):
        self._boldfont = tkinter.font.Font(family='helvetica', weight='bold',
                                    size=self._fontsize)
        self._font = tkinter.font.Font(family='helvetica',
--
        self._sysfont = tkinter.font.Font(font=tkinter.Button()["font"])
        root.option_add("*Font", self._sysfont)

    def _sb_canvas(self, root, expand='y',
                   fill='both', side='bottom'):
        """
        Helper for __init__: construct a canvas with a scrollbar.
--

        return (sb, canvas)

    def scroll_up(self, *e):
        self._chart_canvas.yview('scroll', -1, 'units')

    def scroll_down(self, *e):
        self._chart_canvas.yview('scroll', 1, 'units')

    def page_up(self, *e):
        self._chart_canvas.yview('scroll', -1, 'pages')

    def page_down(self, *e):
        self._chart_canvas.yview('scroll', 1, 'pages')

    def _grow(self):
        """
        Grow the window, if necessary
        """
--
        if self._sentence_canvas is not None:
            self._sentence_canvas['height'] = self._sentence_height

    def set_font_size(self, size):
        self._font.configure(size=-abs(size))
        self._boldfont.configure(size=-abs(size))
        self._sysfont.configure(size=-abs(size))
--
        self._grow()
        self.draw()

    def get_font_size(self):
        return abs(self._fontsize)

    def _configure(self, e):
        """
        The configure callback.  This is called whenever the window is
        resized.  It is also called when the window is first mapped.
--
        self._unitsize = (e.width - 2*ChartView._MARGIN) / N
        self.draw()

    def update(self, chart=None):
        """
        Draw any edges that have not been drawn.  This is typically
        called when a after modifies the canvas that a CanvasView is
--
            self._resize()


    def _edge_conflict(self, edge, lvl):
        """
        Return 1 if the given edge overlaps with any edge on the given
        level.  This is used by _add_edge to figure out what level a
--
                return 1
        return 0

    def _analyze_edge(self, edge):
        """
        Given a new edge, recalculate:

--
            self._unitsize = max(self._unitsize, width/edgelen)
            self._text_height = max(self._text_height, bbox[3] - bbox[1])

    def _add_edge(self, edge, minlvl=0):
        """
        Add a single edge to the ChartView:

--

        self._draw_edge(edge, lvl)

    def view_edge(self, edge):
        level = None
        for i in range(len(self._edgelevels)):
            if edge in self._edgelevels[i]:
--
            self._chart_canvas.yview('moveto',
                                     float(y-dy)/self._chart_height)

    def _draw_edge(self, edge, lvl):
        """
        Draw a single edge on the ChartView.
        """
--
                                dottag, rhstag2, lhstag)

        # Register a callback for clicking on the edge.
        def cb(event, self=self, edge=edge):
            self._fire_callbacks('select', edge)
        c.tag_bind(rhstag1, '<Button-1>', cb)
        c.tag_bind(rhstag2, '<Button-1>', cb)
--

        self._color_edge(edge)

    def _color_edge(self, edge, linecolor=None, textcolor=None):
        """
        Color in an edge with the given colors.
        If no colors are specified, use intelligent defaults
        (dependent on selection, etc.)
        """
        if edge not in self._edgetags: return
--
            else:
                self._color_edge(edge, '#00f', '#008')

    def mark_edge(self, edge, mark='#0df'):
        """
        Mark an edge
        """
        self._marks[edge] = mark
        self._color_edge(edge)

    def unmark_edge(self, edge=None):
        """
        Unmark an edge (or all edges)
        """
--
            del self._marks[edge]
            self._color_edge(edge)

    def markonly_edge(self, edge, mark='#0df'):
        self.unmark_edge()
        self.mark_edge(edge, mark)

    def _analyze(self):
        """
        Analyze the sentence string, to figure out how big a unit needs
        to be, How big the tree should be, etc.
--
        # Size of chart levels
        self._chart_level_size = self._text_height * 2

        # Default tree size..
        self._tree_height = (3 * (ChartView._TREE_LEVEL_SIZE +
                                  self._text_height))

        # Resize the scrollregions.
        self._resize()

    def _resize(self):
        """
        Update the scroll-regions for each canvas.  This ensures that
        everything is within a scroll-region, so the user can use the
--
            self._tree_canvas['scrollregion'] = (0, 0, width,
                                                 self._tree_height)

    def _draw_loclines(self):
        """
        Draw location lines.  These are vertical gridlines used to
        show where each location unit is.
--
                if c2: c2.itemconfig(t2, fill='gray80')
                c3.itemconfig(t3, fill='gray80')

    def _draw_sentence(self):
        """Draw the sentence string."""
        if self._chart.num_leaves() == 0: return
        c = self._sentence_canvas
--
                                  fill='#f0f0f0', outline='#f0f0f0')
            c.tag_lower(rt)

    def erase_tree(self):
        for tag in self._tree_tags: self._tree_canvas.delete(tag)
        self._treetoks = []
        self._treetoks_edge = None
        self._treetoks_index = 0

    def draw_tree(self, edge=None):
        if edge is None and self._treetoks_edge is None: return
        if edge is None: edge = self._treetoks_edge

--
        h = tree.height() * (ChartView._TREE_LEVEL_SIZE+self._text_height)
        self._tree_canvas['scrollregion'] = (0, 0, w, h)

    def cycle_tree(self):
        self._treetoks_index = (self._treetoks_index+1)%len(self._treetoks)
        self.draw_tree(self._treetoks_edge)

    def _draw_treecycle(self):
        if len(self._treetoks) <= 1: return

        # Draw the label.
--

            # Set up a callback: show the tree if they click on its
            # triangle.
            def cb(event, self=self, i=i):
                self._treetoks_index = i
                self.draw_tree()
            c.tag_bind(tag, '<Button-1>', cb)

    def _draw_treetok(self, treetok, index, depth=0):
        """
        :param index: The index of the first leaf in the tree.
        :return: The index of the first leaf after the tree.
--

        return nodex, index

    def draw(self):
        """
        Draw everything (from scratch).
        """
--

        self._draw_loclines()

    def add_callback(self, event, func):
        self._callbacks.setdefault(event,{})[func] = 1

    def remove_callback(self, event, func=None):
        if func is None: del self._callbacks[event]
        else:
            try: del self._callbacks[event][func]
            except: pass

    def _fire_callbacks(self, event, *args):
        if event not in self._callbacks: return
        for cb_func in list(self._callbacks[event].keys()): cb_func(*args)

--
    EdgeRule as the first base class, and the basic rule as the
    second base class.  (Order matters!)
    """
    def __init__(self, edge):
        super = self.__class__.__bases__[1]
        self._edge = edge
        self.NUM_EDGES = super.NUM_EDGES-1
    def apply_iter(self, chart, grammar, *edges):
        super = self.__class__.__bases__[1]
        edges += (self._edge,)
        for e in super.apply_iter(self, chart, grammar, *edges): yield e
    def __str__(self):
        super = self.__class__.__bases__[1]
        return super.__str__(self)

--
#######################################################################

class ChartParserApp(object):
    def __init__(self, grammar, tokens, title='Chart Parser Application'):
        # Initialize the parser
        self._init_parser(grammar, tokens)

--
            self.destroy()
            raise

    def destroy(self, *args):
        if self._root is None: return
        self._root.destroy()
        self._root = None

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
    # Initialization Helpers
    #////////////////////////////////////////////////////////////

    def _init_parser(self, grammar, tokens):
        self._grammar = grammar
        self._tokens = tokens
        self._reset_parser()

    def _reset_parser(self):
        self._cp = SteppingChartParser(self._grammar)
        self._cp.initialize(self._tokens)
        self._chart = self._cp.chart()
--
        # The currently selected edge
        self._selection = None

    def _init_fonts(self, root):
        # See: <http://www.astro.washington.edu/owen/ROTKFolklore.html>
        self._sysfont = tkinter.font.Font(font=tkinter.Button()["font"])
        root.option_add("*Font", self._sysfont)

        # TWhat's our font size (default=same as sysfont)
        self._size = tkinter.IntVar(root)
        self._size.set(self._sysfont.cget('size'))

--
        self._font = tkinter.font.Font(family='helvetica',
                                    size=self._size.get())

    def _init_animation(self):
        # Are we stepping? (default=yes)
        self._step = tkinter.IntVar(self._root)
        self._step.set(1)

        # What's our animation speed (default=fast)
        self._animate = tkinter.IntVar(self._root)
        self._animate.set(3) # Default speed = fast

        # Are we currently animating?
        self._animating = 0

    def _init_chartview(self, parent):
        self._cv = ChartView(self._chart, parent,
                             draw_tree=1, draw_sentence=1)
        self._cv.add_callback('select', self._click_cv_edge)

    def _init_rulelabel(self, parent):
        ruletxt = 'Last edge generated by:'

        self._rulelabel1 = tkinter.Label(parent,text=ruletxt,
--
                                   text='Step')
        step.pack(side='right')

    def _init_buttons(self, parent):
        frame1 = tkinter.Frame(parent)
        frame2 = tkinter.Frame(parent)
        frame1.pack(side='bottom', fill='x')
--
                       background='#90f090', foreground='black',
                       command=self.fundamental).pack(side='left')

    def _init_bindings(self):
        self._root.bind('<Up>', self._cv.scroll_up)
        self._root.bind('<Down>', self._cv.scroll_down)
        self._root.bind('<Prior>', self._cv.page_up)
--
        # Step control
        self._root.bind('s', lambda e,s=self._step:s.set(not s.get()))

    def _init_menubar(self):
        menubar = tkinter.Menu(self._root)

        filemenu = tkinter.Menu(menubar, tearoff=0)
--
    # Selection Handling
    #////////////////////////////////////////////////////////////

    def _click_cv_edge(self, edge):
        if edge != self._selection:
            # Clicking on a new edge selects it.
            self._select_edge(edge)
--
            # [XX] this can get confused if animation is running
            # faster than the callbacks...

    def _select_matrix_edge(self, edge):
        self._select_edge(edge)
        self._cv.view_edge(edge)

    def _select_edge(self, edge):
        self._selection = edge
        # Update the chart view.
        self._cv.markonly_edge(edge, '#f00')
--
        if self._matrix: self._matrix.markonly_edge(edge)
        if self._matrix: self._matrix.view_edge(edge)

    def _deselect_edge(self):
        self._selection = None
        # Update the chart view.
        self._cv.unmark_edge()
--
        # Update the matrix view
        if self._matrix: self._matrix.unmark_edge()

    def _show_new_edge(self, edge):
        self._display_rule(self._cp.current_chartrule())
        # Update the chart view.
        self._cv.update()
--
    # Help/usage
    #////////////////////////////////////////////////////////////

    def help(self, *e):
        self._animating = 0
        # The default font's not very legible; try using 'fixed' instead.
        try:
            ShowText(self._root, 'Help: Chart Parser Application',
                     (__doc__ or '').strip(), width=75, font='fixed')
--
            ShowText(self._root, 'Help: Chart Parser Application',
                     (__doc__ or '').strip(), width=75)

    def about(self, *e):
        ABOUT = ("NLTK Chart Parser Application\n"+
                 "Written by Edward Loper")
        tkinter.messagebox.showinfo('About: Chart Parser Application', ABOUT)
--
                          ('Pickle file', '.pickle'),
                          ('All files', '*')]

    def load_chart(self, *args):
        "Load a chart from a pickle file"
        filename = askopenfilename(filetypes=self.CHART_FILE_TYPES,
                                   defaultextension='.pickle')
        if not filename: return
        try:
            chart = pickle.load(open(filename, 'r'))
--
            tkinter.messagebox.showerror('Error Loading Chart',
                                   'Unable to open file: %r' % filename)

    def save_chart(self, *args):
        "Save a chart to a pickle file"
        filename = asksaveasfilename(filetypes=self.CHART_FILE_TYPES,
                                     defaultextension='.pickle')
        if not filename: return
        try:
            pickle.dump(self._chart, open(filename, 'w'))
--
            tkinter.messagebox.showerror('Error Saving Chart',
                                   'Unable to open file: %r' % filename)

    def load_grammar(self, *args):
        "Load a grammar from a pickle file"
        filename = askopenfilename(filetypes=self.GRAMMAR_FILE_TYPES,
                                   defaultextension='.cfg')
        if not filename: return
        try:
            if filename.endswith('.pickle'):
--
            tkinter.messagebox.showerror('Error Loading Grammar',
                                   'Unable to open file: %r' % filename)

    def save_grammar(self, *args):
        filename = asksaveasfilename(filetypes=self.GRAMMAR_FILE_TYPES,
                                     defaultextension='.cfg')
        if not filename: return
        try:
            if filename.endswith('.pickle'):
--
            tkinter.messagebox.showerror('Error Saving Grammar',
                                   'Unable to open file: %r' % filename)

    def reset(self, *args):
        self._animating = 0
        self._reset_parser()
        self._cv.update(self._chart)
--
    # Edit
    #////////////////////////////////////////////////////////////

    def edit_grammar(self, *e):
        CFGEditor(self._root, self._grammar, self.set_grammar)

    def set_grammar(self, grammar):
        self._grammar = grammar
        self._cp.set_grammar(grammar)
        if self._results: self._results.set_grammar(grammar)

    def edit_sentence(self, *e):
        sentence = " ".join(self._tokens)
        title = 'Edit Text'
        instr = 'Enter a new sentence to parse.'
        EntryDialog(self._root, sentence, instr, self.set_sentence, title)

    def set_sentence(self, sentence):
        self._tokens = list(sentence.split())
        self.reset()

--
    # View Menu
    #////////////////////////////////////////////////////////////

    def view_matrix(self, *e):
        if self._matrix is not None: self._matrix.destroy()
        self._matrix = ChartMatrixView(self._root, self._chart)
        self._matrix.add_callback('select', self._select_matrix_edge)

    def view_results(self, *e):
        if self._results is not None: self._results.destroy()
        self._results = ChartResultsView(self._root, self._chart,
                                         self._grammar)
--
    # Zoom Menu
    #////////////////////////////////////////////////////////////

    def resize(self):
        self._animating = 0
        self.set_font_size(self._size.get())

    def set_font_size(self, size):
        self._cv.set_font_size(size)
        self._font.configure(size=-abs(size))
        self._boldfont.configure(size=-abs(size))
        self._sysfont.configure(size=-abs(size))

    def get_font_size(self):
        return abs(self._size.get())

    #////////////////////////////////////////////////////////////
    # Parsing
    #////////////////////////////////////////////////////////////

    def apply_strategy(self, strategy, edge_strategy=None):
        # If we're animating, then stop.
        if self._animating:
            self._animating = 0
--
                if self._matrix: self._matrix.update()
                if self._results: self._results.update()

    def _stop_animation(self, *e):
        self._animating = 0

    def _animate_strategy(self, speed=1):
        if self._animating == 0: return
        if self._apply_strategy() is not None:
            if self._animate.get() == 0 or self._step.get() == 1:
--
            else:
                self._root.after(20, self._animate_strategy)

    def _apply_strategy(self):
        new_edge = next(self._cpstep)

        if new_edge is not None:
            self._show_new_edge(new_edge)
        return new_edge

    def _display_rule(self, rule):
        if rule is None:
            self._rulelabel2['text'] = ''
        else:
--
    _BU_LC_STRATEGY = _BU_LC_RULE + _FUNDAMENTAL

    # Button callback functions:
    def top_down_init(self, *e):
        self.apply_strategy(self._TD_INIT, None)
    def top_down_predict(self, *e):
        self.apply_strategy(self._TD_PREDICT, TopDownPredictEdgeRule)
    def bottom_up(self, *e):
        self.apply_strategy(self._BU_RULE, BottomUpEdgeRule)
    def bottom_up_leftcorner(self, *e):
        self.apply_strategy(self._BU_LC_RULE, BottomUpLeftCornerEdgeRule)
    def fundamental(self, *e):
        self.apply_strategy(self._FUNDAMENTAL, FundamentalEdgeRule)
    def bottom_up_strategy(self, *e):
        self.apply_strategy(self._BU_STRATEGY, BottomUpEdgeRule)
    def bottom_up_leftcorner_strategy(self, *e):
        self.apply_strategy(self._BU_LC_STRATEGY, BottomUpLeftCornerEdgeRule)
    def top_down_strategy(self, *e):
        self.apply_strategy(self._TD_STRATEGY, TopDownPredictEdgeRule)

def app():
    grammar = parse_cfg("""
    # Grammatical productions.
        S -> NP VP
--
    _HELPTAB_BG_PARAMS = dict(background='#aba')
    _HELPTAB_SPACER = 6

    def normalize_grammar(self, grammar):
        # Strip comments
        grammar = re.sub(r'((\\.|[^#])*)(#.*)?', r'\1', grammar)
        # Normalize whitespace
--
        grammar = re.sub(r'([^\\])\$', r'\1\\$', grammar)
        return grammar

    def __init__(self, devset_name='conll2000', devset=None,
                 grammar = '', chunk_node='NP', tagset=None):
        """
        :param devset_name: The name of the development set; used for
--
        :param devset: A list of chunked sentences
        :param grammar: The initial grammar to display.
        :param tagset: Dictionary from tags to string descriptions, used
            for the help page.  Defaults to ``self.TAGSET``.
        """
        self._chunk_node = chunk_node

--
        self.show_devset(0)
        self.update()

    def _init_bindings(self, top):
        top.bind('<Control-n>', self._devset_next)
        top.bind('<Control-p>', self._devset_prev)
        top.bind('<Control-t>', self.toggle_show_trace)
--
        # Redraw the eval graph when the window size changes
        self.evalbox.bind('<Configure>', self._eval_plot)

    def _init_fonts(self, top):
        # TWhat's our font size (default=same as sysfont)
        self._size = IntVar(top)
        self._size.set(20)
        self._font = tkinter.font.Font(family='helvetica',
--
        self._smallfont = tkinter.font.Font(family='helvetica',
                                      size=-(int(self._size.get()*14/20)))

    def _init_menubar(self, parent):
        menubar = Menu(parent)

        filemenu = Menu(menubar, tearoff=0)
--

        parent.config(menu=menubar)

    def toggle_show_trace(self, *e):
        if self._showing_trace:
            self.show_devset()
        else:
--

    _SCALE_N = 5 # center on the last 5 examples.
    _DRAW_LINES = False
    def _eval_plot(self, *e, **config):
        width = config.get('width', self.evalbox.winfo_width())
        height = config.get('height', self.evalbox.winfo_height())

--
            prev_x, prev_y = x, y

    _eval_demon_running = False
    def _eval_demon(self):
        if self.top is None: return
        if self.chunker is None:
            self._eval_demon_running = False
--
            self._adaptively_modify_eval_chunk(time.time() - t0)
            self.top.after(int(self._EVAL_FREQ*1000), self._eval_demon)

    def _adaptively_modify_eval_chunk(self, t):
        """
        Modify _EVAL_CHUNK to try to keep the amount of time that the
        eval demon takes between _EVAL_DEMON_MIN and _EVAL_DEMON_MAX.
--
                         min(int(self._EVAL_CHUNK*(self._EVAL_DEMON_MIN/t)),
                             self._EVAL_CHUNK+10))

    def _init_widgets(self, top):
        frame0 = Frame(top, **self._FRAME_PARAMS)
        frame0.grid_columnconfigure(0, weight=4)
        frame0.grid_columnconfigure(3, weight=2)
--
        self.grammarbox.tag_config('hangindent', lmargin1=0, lmargin2=40)

    _showing_trace = False
    def show_trace(self, *e):
        self._showing_trace = True
        self.trace_button['state'] = 'disabled'
        self.devset_button['state'] = 'normal'
--
        # though.  (This is on OS X w/ python 2.5)
        self.top.after(100, self.devset_xscroll.set, 0, .3)

    def show_help(self, tab):
        self.helpbox['state'] = 'normal'
        self.helpbox.delete('1.0', 'end')
        for (name, tabstops, text) in self.HELP:
--
                self.helptabs[name].config(**self._HELPTAB_BG_PARAMS)
        self.helpbox['state'] = 'disabled'

    def _history_prev(self, *e):
        self._view_history(self._history_index-1)
        return 'break'

    def _history_next(self, *e):
        self._view_history(self._history_index+1)
        return 'break'

    def _view_history(self, index):
        # Bounds & sanity checking:
        index = max(0, min(len(self._history)-1, index))
        if not self._history: return
--
        else:
            self.grammarlabel['text'] = 'Grammar:'

    def _devset_next(self, *e):
        self._devset_scroll('scroll', 1, 'page')
        return 'break'

    def _devset_prev(self, *e):
        self._devset_scroll('scroll', -1, 'page')
        return 'break'

    def destroy(self, *e):
        if self.top is None: return
        self.top.destroy()
        self.top = None

    def _devset_scroll(self, command, *args):
        N = 1 # size of a page -- one sentence.
        showing_trace = self._showing_trace
        if command == 'scroll' and args[1].startswith('unit'):
--
        if showing_trace:
            self.show_trace()

    def show_devset(self, index=None):
        if index is None: index = self.devset_index

        # Bounds checking
--
        last = float(self.devset_index+2)/self._devset_size.get()
        self.devset_scroll.set(first, last)

    def _chunks(self, tree):
        chunks = set()
        wordnum = 0
        for child in tree:
--
                wordnum += 1
        return chunks

    def _syntax_highlight_grammar(self, grammar):
        if self.top is None: return
        self.grammarbox.tag_remove('comment', '1.0', 'end')
        self.grammarbox.tag_remove('angle', '1.0', 'end')
--
                    self.grammarbox.tag_add('brace', s, e)


    def _grammarcheck(self, grammar):
        if self.top is None: return
        self.grammarbox.tag_remove('error', '1.0', 'end')
        self._grammarcheck_errs = []
--
                                            '%s.0 lineend' % (lineno+1))
        self.status['text'] = ''

    def update(self, *event):
        # Record when update was called (for grammarcheck)
        if event:
            self._last_keypress = time.time()
--
        if not self._eval_demon_running:
            self._eval_demon()

    def _highlight_devset(self, sample=None):
        if sample is None:
            sample = self.devset[self.devset_index:self.devset_index+1]

--
            for chunk in test_chunks - gold_chunks:
                self._color_chunk(sentnum, chunk, 'false-pos')

    def _chunkparse(self, words):
        try:
            return self.chunker.parse(words)
        except (ValueError, IndexError) as e:
--
            # Treat it as tagging nothing:
            return words

    def _color_chunk(self, sentnum, chunk, tag):
        start, end = chunk
        self.devsetbox.tag_add(tag,
            '%s.%s' % (self.linenum[sentnum], self.charnum[sentnum, start]),
            '%s.%s' % (self.linenum[sentnum], self.charnum[sentnum, end]-1))

    def reset(self):
        # Clear various variables
        self.chunker = None
        self.grammar = None
--
        '#   F-score:   %(fscore)s\n\n'
        '%(grammar)s\n')

    def save_grammar(self, filename=None):
        if not filename:
            ftypes = [('Chunk Gramamr', '.chunk'),
                      ('All files', '*')]
            filename = tkinter.filedialog.asksaveasfilename(filetypes=ftypes,
                                                      defaultextension='.chunk')
            if not filename: return
        if (self._history and self.normalized_grammar ==
            self.normalize_grammar(self._history[-1][0])):
--
            grammar=self.grammar.strip()))
        out.close()

    def load_grammar(self, filename=None):
        if not filename:
            ftypes = [('Chunk Gramamr', '.chunk'),
                      ('All files', '*')]
            filename = tkinter.filedialog.askopenfilename(filetypes=ftypes,
                                                    defaultextension='.chunk')
            if not filename: return
        self.grammarbox.delete('1.0', 'end')
        self.update()
--
        self.grammarbox.insert('1.0', grammar)
        self.update()

    def save_history(self, filename=None):
        if not filename:
            ftypes = [('Chunk Gramamr History', '.txt'),
                      ('All files', '*')]
            filename = tkinter.filedialog.asksaveasfilename(filetypes=ftypes,
                                                      defaultextension='.txt')
            if not filename: return

        out = open(filename, 'w')
--
                              in self.grammar.strip().split()))
        out.close()

    def about(self, *e):
        ABOUT = ("NLTK RegExp Chunk Parser Application\n"+
                 "Written by Edward Loper")
        TITLE = 'About: Regular Expression Chunk Parser Application'
--
        except:
            ShowText(self.top, TITLE, ABOUT)

    def set_devset_size(self, size=None):
        if size is not None: self._devset_size.set(size)
        self._devset_size.set(min(len(self.devset), self._devset_size.get()))
        self.show_devset(1)
        self.show_devset(0)
        # what about history?  Evaluated at diff dev set sizes!

    def resize(self, size=None):
        if size is not None: self._size.set(size)
        size = self._size.get()
        self._font.configure(size=-(abs(size)))
        self._smallfont.configure(size=min(-10, -(abs(size))*14/20))

    def mainloop(self, *args, **kwargs):
        """
        Enter the Tkinter mainloop.  This function must be called if
        this demo is created from a non-interactive program (e.g.
--
        if in_idle(): return
        self.top.mainloop(*args, **kwargs)

def app():
    RegexpChunkApp().mainloop()

if __name__ == '__main__':
--
# NB All corpora must be specified in a lambda expression so as not to be
# loaded when the module is imported.

_DEFAULT = 'English: Brown Corpus (Humor, simplified)'
_CORPORA = {
            'Catalan: CESS-CAT Corpus (simplified)':
                lambda: cess_cat.tagged_sents(tagset='simple'),
--
    #Percentage of text left of the scrollbar position
    _FRACTION_LEFT_TEXT=0.30

    def __init__(self):
        self.queue = q.Queue()
        self.model = ConcordanceSearchModel(self.queue)
        self.top = Tk()
        self._init_top(self.top)
        self._init_menubar()
        self._init_widgets(self.top)
        self.load_corpus(self.model.DEFAULT_CORPUS)
        self.after = self.top.after(POLL_INTERVAL, self._poll)

    def _init_top(self, top):
        top.geometry('950x680+50+50')
        top.title('NLTK Concordance Search')
        top.bind('<Control-q>', self.destroy)
        top.protocol('WM_DELETE_WINDOW', self.destroy)
        top.minsize(950,680)

    def _init_widgets(self, parent):
        self.main_frame = Frame(parent, dict(background=self._BACKGROUND_COLOUR, padx=1, pady=1, border=1))
        self._init_corpus_select(self.main_frame)
        self._init_query_box(self.main_frame)
--
        self._init_status(self.main_frame)
        self.main_frame.pack(fill='both', expand=True)

    def _init_menubar(self):
        self._result_size = IntVar(self.top)
        self._cntx_bf_len = IntVar(self.top)
        self._cntx_af_len = IntVar(self.top)
--

        self.top.config(menu=menubar)

    def set_result_size(self, **kwargs):
        self.model.result_count = self._result_size.get()

    def set_cntx_af_len(self, **kwargs):
        self._char_after = self._cntx_af_len.get()

    def set_cntx_bf_len(self, **kwargs):
        self._char_before = self._cntx_bf_len.get()

    def _init_corpus_select(self, parent):
        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
        self.var = StringVar(innerframe)
        self.var.set(self.model.DEFAULT_CORPUS)
        Label(innerframe, justify=LEFT, text=' Corpus: ',
              background=self._BACKGROUND_COLOUR, padx = 2, pady = 1, border = 0).pack(side='left')

        other_corpora = list(self.model.CORPORA.keys()).remove(self.model.DEFAULT_CORPUS)
        om = OptionMenu(innerframe, self.var, self.model.DEFAULT_CORPUS, command=self.corpus_selected, *self.model.non_default_corpora())
        om['borderwidth'] = 0
        om['highlightthickness'] = 1
        om.pack(side='left')
        innerframe.pack(side='top', fill='x', anchor='n')

    def _init_status(self, parent):
        self.status = Label(parent, justify=LEFT, relief=SUNKEN, background=self._BACKGROUND_COLOUR, border=0, padx = 1, pady = 0)
        self.status.pack(side='top', anchor='sw')

    def _init_query_box(self, parent):
        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
        another = Frame(innerframe, background=self._BACKGROUND_COLOUR)
        self.query_box = Entry(another, width=60)
--
        another.pack()
        innerframe.pack(side='top', fill='x', anchor='n')

    def search_enter_keypress_handler(self, *event):
        self.search()

    def _init_results_box(self, parent):
        innerframe = Frame(parent)
        i1 = Frame(innerframe)
        i2 = Frame(innerframe)
--
        i2.pack(side='bottom', fill='x', anchor='s')
        innerframe.pack(side='top', fill='both', expand=True)

    def _init_paging(self, parent):
        innerframe = Frame(parent, background=self._BACKGROUND_COLOUR)
        self.prev = prev = Button(innerframe, text='Previous', command=self.previous, width='10', borderwidth=1, highlightthickness=1, state='disabled')
        prev.pack(side='left', anchor='center')
--
        innerframe.pack(side='top', fill='y')
        self.current_page = 0

    def previous(self):
        self.clear_results_box()
        self.freeze_editable()
        self.model.prev(self.current_page - 1)

    def __next__(self):
        self.clear_results_box()
        self.freeze_editable()
        self.model.next(self.current_page + 1)

    def about(self, *e):
        ABOUT = ("NLTK Concordance Search Demo\n")
        TITLE = 'About: NLTK Concordance Search Demo'
        try:
--
        except:
            ShowText(self.top, TITLE, ABOUT)

    def _bind_event_handlers(self):
        self.top.bind(CORPUS_LOADED_EVENT, self.handle_corpus_loaded)
        self.top.bind(SEARCH_TERMINATED_EVENT, self.handle_search_terminated)
        self.top.bind(SEARCH_ERROR_EVENT, self.handle_search_error)
        self.top.bind(ERROR_LOADING_CORPUS_EVENT, self.handle_error_loading_corpus)

    def _poll(self):   
        try:
            event = self.queue.get(block=False)
        except q.Empty:
--
                self.handle_error_loading_corpus(event)
        self.after = self.top.after(POLL_INTERVAL, self._poll)

    def handle_error_loading_corpus(self, event):
        self.status['text'] = 'Error in loading ' + self.var.get()
        self.unfreeze_editable()
        self.clear_all()
        self.freeze_editable()

    def handle_corpus_loaded(self, event):
        self.status['text'] = self.var.get() + ' is loaded'
        self.unfreeze_editable()
        self.clear_all()
        self.query_box.focus_set()

    def handle_search_terminated(self, event):
        #todo: refactor the model such that it is less state sensitive
        results = self.model.get_results()
        self.write_results(results)
--
        self.unfreeze_editable()
        self.results_box.xview_moveto(self._FRACTION_LEFT_TEXT)

    def handle_search_error(self, event):
        self.status['text'] = 'Error in query ' + self.model.query
        self.unfreeze_editable()

    def corpus_selected(self, *args):
        new_selection = self.var.get()
        self.load_corpus(new_selection)

    def load_corpus(self, selection):
        if self.model.selected_corpus != selection:
            self.status['text'] = 'Loading ' + selection + '...'
            self.freeze_editable()
            self.model.load_corpus(selection)

    def search(self):
        self.current_page = 0
        self.clear_results_box()
        self.model.reset_results()
--
        self.model.search(query, self.current_page + 1, )


    def write_results(self, results):
        self.results_box['state'] = 'normal'
        row = 1
        for each in results:
--
                row += 1
        self.results_box['state'] = 'disabled'

    def words_and_labels(self, sentence, pos1, pos2):
        search_exp = sentence[pos1:pos2]
        words, labels = [], []
        labeled_words = search_exp.split(' ')
--
            index += 1
        return words, labels

    def pad(self, sent, hstart, hend):
        if hstart >= self._char_before:
            return sent, hstart, hend
        d = self._char_before - hstart
        sent = ''.join([' '] * d) + sent
        return sent, hstart + d, hend + d

    def destroy(self, *e):
        if self.top is None: return
        self.top.after_cancel(self.after)
        self.top.destroy()
        self.top = None

    def clear_all(self):
        self.query_box.delete(0, END)
        self.model.reset_query()
        self.clear_results_box()

    def clear_results_box(self):
        self.results_box['state'] = 'normal'
        self.results_box.delete("1.0", END)
        self.results_box['state'] = 'disabled'

    def freeze_editable(self):
        self.query_box['state'] = 'disabled'
        self.search_button['state'] = 'disabled'
        self.prev['state'] = 'disabled'
        self.next['state'] = 'disabled'

    def unfreeze_editable(self):
        self.query_box['state'] = 'normal'
        self.search_button['state'] = 'normal'
        self.set_paging_button_states()

    def set_paging_button_states(self):
        if self.current_page == 0 or self.current_page == 1:
            self.prev['state'] = 'disabled'
        else:
--
        else:
            self.next['state'] = 'disabled'

    def fire_event(self, event):
        #Firing an event so that rendering of widgets happen in the mainloop thread
        self.top.event_generate(event, when='tail')

    def mainloop(self, *args, **kwargs):
        if in_idle(): return
        self.top.mainloop(*args, **kwargs)

class ConcordanceSearchModel(object):
    def __init__(self, queue):
        self.queue = queue
        self.CORPORA = _CORPORA
        self.DEFAULT_CORPUS = _DEFAULT
        self.selected_corpus = None
        self.reset_query()
        self.reset_results()
        self.result_count = None
        self.last_sent_searched = 0

    def non_default_corpora(self):
        copy = []
        copy.extend(list(self.CORPORA.keys()))
        copy.remove(self.DEFAULT_CORPUS)
        copy.sort()
        return copy

    def load_corpus(self, name):
        self.selected_corpus = name
        self.tagged_sents = []
        runner_thread = self.LoadCorpus(name, self)
        runner_thread.start()

    def search(self, query, page):
        self.query = query
        self.last_requested_page = page
        self.SearchCorpus(self, page, self.result_count).start()

    def next(self, page):
        self.last_requested_page = page
        if len(self.results) < page:
            self.search(self.query, page)
        else:
            self.queue.put(SEARCH_TERMINATED_EVENT)

    def prev(self, page):
        self.last_requested_page = page
        self.queue.put(SEARCH_TERMINATED_EVENT)

    def reset_results(self):
        self.last_sent_searched = 0
        self.results = []
        self.last_page = None

    def reset_query(self):
        self.query = None

    def set_results(self, page, resultset):
        self.results.insert(page - 1, resultset)

    def get_results(self):
        return self.results[self.last_requested_page - 1]

    def has_more_pages(self, page):
        if self.results == [] or self.results[0] == []:
            return False
        if self.last_page is None:
--
        return page < self.last_page

    class LoadCorpus(threading.Thread):
        def __init__(self, name, model):
            threading.Thread.__init__(self)
            self.model, self.name = model, name

        def run(self):
            try:
                ts = self.model.CORPORA[self.name]()
                self.model.tagged_sents = [' '.join(w+'/'+t for (w,t) in sent) for sent in ts]
--
                self.model.queue.put(ERROR_LOADING_CORPUS_EVENT)

    class SearchCorpus(threading.Thread):
        def __init__(self, model, page, count):
            self.model, self.count, self.page = model, count, page
            threading.Thread.__init__(self)

        def run(self):
            q = self.processed_query()
            sent_pos, i, sent_count = [], 0, 0
            for sent in self.model.tagged_sents[self.model.last_sent_searched:]:
--
                self.model.set_results(self.page, sent_pos[:-1])
            self.model.queue.put(SEARCH_TERMINATED_EVENT)

        def processed_query(self):
            new = []
            for term in self.model.query.split():
                term = re.sub(r'\.', r'[^/ ]', term)
--
                    new.append(BOUNDARY + term + '/' + WORD_OR_TAG + BOUNDARY)
            return ' '.join(new)

def app():
    d = ConcordanceSearchView()
    d.mainloop()

--
import nltk.text
from nltk.corpus import gutenberg

def plot_word_freq_dist(text):
    fd = text.vocab()

    samples = fd.keys()[:50]
--
    pylab.xticks(range(len(samples)), [str(s) for s in samples], rotation=90)
    pylab.show()

def app():
    t1 = nltk.Text(gutenberg.words('melville-moby_dick.txt'))
    plot_word_freq_dist(t1)

--
# For license information, see LICENSE.TXT

"""
A WordNet Browser application which launches the default browser
(if it is not already running) and opens a new tab with a connection
to http://localhost:port/ .  It also starts an HTTP server on the
specified port and begins serving browser requests.  The default
port is 8000.  (For command-line help, run "python wordnet -h")
This application requires that the user's web browser supports
Javascript.
--
        messages are silently dropped.

    -p <port> or --port <port>
        Run the web server on this TCP port, defaults to 8000.

    -s or --server-mode
        Do not start a web browser, and do not allow a user to
--

import os
from sys import argv
from collections import defaultdict
import webbrowser
import datetime
import re
--

class MyServerHandler(BaseHTTPRequestHandler):

    def do_HEAD(self):
        self.send_head()

    def do_GET(self):
        global firstClient
        sp = self.path[1:]
        if compat.unquote_plus(sp) == 'SHUTDOWN THE SERVER':
--
            page, word = page_from_href(sp)
        elif sp == "start_page":
            # if this is the first request we should display help
            # information, and possibly set a default word.
            type = 'text/html'
            page, word = page_from_word("wordnet")
        else:
--
        self.wfile.write(page.encode('utf8'))


    def send_head(self, type=None):
        self.send_response(200)
        self.send_header('Content-type', type)
        self.end_headers()

    def log_message(self, format, *args):
        global logfile

        if logfile:
--
                 format%args))


def get_unique_counter_from_url(sp):
    """
    Extract the unique counter from the URL if it has one.  Otherwise return
    null.
--
        return None


def wnb(port=8000, runBrowser=True, logfilename=None):
    """
    Run NLTK Wordnet Browser Server.

    :param port: The port number for the server to listen on, defaults to
                 8000
    :type  port: int

--
    # tab, or exits the text-mode browser.  Both of these are unfreasable.
    #
    # The next best alternative is to start the server, have it close when
    # it receives SIGTERM (default), and run the browser as well.  The user
    # may have to shutdown both programs.
    #
    # Since webbrowser may block, and the webserver will block, we must run
--
        browser_thread.join()


def startBrowser(url, server_ready):
    def run():
        server_ready.wait()
        time.sleep(1) # Wait a little bit more, there's still the chance of
                      # a race condition.
--

# This is wrapped inside a function since wn is only available if the
# WordNet corpus is installed.
def _pos_tuples():
    return [
        (wn.NOUN,'N','noun'),
        (wn.VERB,'V','verb'),
        (wn.ADJ,'J','adj'),
        (wn.ADV,'R','adv')]

def _pos_match(pos_tuple):
    """
    This function returns the complete pos tuple for the partial pos
    tuple given to it.  It attempts to match it against the first
--
INDIRECT_HYPERNYMS = 26


def lemma_property(word, synset, func):

    def flattern(l):
        if l == []:
            return []
        else:
--
    return flattern([func(l) for l in synset.lemmas if l.name == word])


def rebuild_tree(orig_tree):
    node = orig_tree[0]
    children = orig_tree[1:]
    return (node, [rebuild_tree(t) for t in children])


def get_relations_data(word, synset):
    """
    Get synset relations data for a synset.  Note that this doesn't
    yet support things such as full hyponym vs direct hyponym.
--

# HTML oriented functions

def _bold(txt): return '<b>%s</b>' % txt

def _center(txt): return '<center>%s</center>' % txt

def _hlev(n,txt): return '<h%d>%s</h%d>' % (n,txt,n)

def _italic(txt): return '<i>%s</i>' % txt

def _li(txt): return '<li>%s</li>' % txt

def pg(word, body):
    '''
    Return a HTML page of NLTK Browser format constructed from the
    word and body
--
    '''
    return (html_header % word) + body + html_trailer

def _ul(txt): return '<ul>' + txt + '</ul>'

def _abbc(txt):
    """
    abbc = asterisks, breaks, bold, center
    """
--
    _ul(_li(_italic('(has full hyponym continuation)'))) + '\n'


def _get_synset(synset_key):
    """
    The synset key is the unique name of the synset, this can be
    retrived via synset.name
    """
    return wn.synset(synset_key)

def _collect_one_synset(word, synset, synset_relations):
    '''
    Returns the HTML string for one synset or word

--
    if synset.name in synset_relations:
        synset_label = _bold(synset_label)
    s = '<li>%s (%s) ' % (make_lookup_link(ref, synset_label), descr)
    def format_lemma(w):
        w = w.replace('_', ' ')
        if w.lower() == word:
            return _bold(w)
--
    s += ', '.join(format_lemma(l.name) for l in synset.lemmas)

    gl = " (%s) <i>%s</i> " % \
        (synset.definition,
         "; ".join("\"%s\"" % e for e in synset.examples))
    return s + gl + _synset_relations(word, synset, synset_relations) + '</li>\n'

def _collect_all_synsets(word, pos, synset_relations=dict()):
    """
    Return a HTML unordered list of synsets for the given word and
    part of speech.
--
                 for synset
                 in wn.synsets(word, pos)))

def _synset_relations(word, synset, synset_relations):
    '''
    Builds the HTML string for the relations of a synset

--
        return ""
    ref = Reference(word, synset_relations)

    def relation_html(r):
        if isinstance(r, Synset):
            return make_lookup_link(Reference(r.lemma_names[0]), r.lemma_names[0])
        elif isinstance(r, Lemma):
--
        else:
            raise TypeError("r must be a synset, lemma or list, it was: type(r) = %s, r = %s" % (type(r), r))

    def make_synset_html(db_name, disp_name, rels):
        synset_html = '<i>%s</i>\n' % \
            make_lookup_link(
                copy.deepcopy(ref).toggle_synset_relation(synset, db_name).encode(),
--
    A reference to a page that may be generated by page_word
    """

    def __init__(self, word, synset_relations=dict()):
        """
        Build a reference to a new page.

--
        self.word = word
        self.synset_relations = synset_relations

    def encode(self):
        """
        Encode this reference into a string to be used in a URL.
        """
--
        string = pickle.dumps((self.word, self.synset_relations), -1)
        return base64.urlsafe_b64encode(string)

    def toggle_synset_relation(self, synset, relation):
        """
        Toggle the display of the relations for the given synset and
        relation type.
--

        return self

    def toggle_synset(self, synset):
        """
        Toggle displaying of the relation types for the given synset
        """
--
        return self


def decode_reference(string):
    """
    Decode a reference encoded with Reference.encode
    """
--
    word, synset_relations = pickle.loads(string)
    return Reference(word, synset_relations)

def make_lookup_link(ref, label):
    return '<a href="lookup_%s">%s</a>' % (ref.encode(), label)


def page_from_word(word):
    """
    Return a HTML page for the given word.

--
    """
    return page_from_reference(Reference(word))

def page_from_href(href):
    '''
    Returns a tuple of the HTML page built and the new current word

--
    '''
    return page_from_reference(decode_reference(href))

def page_from_reference(href):
    '''
    Returns a tuple of the HTML page built and the new current word

--
    :rtype: A tuple (str,str)
    '''
    word = href.word
    pos_forms = defaultdict(list)
    words = word.split(',')
    words = [w for w in [w.strip().lower().replace(' ', '_')
                         for w in words]
--
# Static pages
#####################################################################

def get_static_page_by_path(path):
    """
    Return a static HTML page from the path given.
    """
--
    return page


def get_static_web_help_page():
    """
    Return the static web help page.
    """
--
"""


def get_static_welcome_message():
    """
    Get the static welcome page.
    """
--
</ul>
"""

def get_static_index_page(with_shutdown):
    """
    Get the static index page.
    """
--
    return template % upper_link


def get_static_upper_page(with_shutdown):
    """
    Return the upper frame page,

--



def usage():
    """
    Display the command line help message.
    """
    print(__doc__)

def app():
    # Parse and interpret options.
    (opts, _) = getopt.getopt(argv[1:], "l:p:sh",
                              ["logfile=", "port=", "server-mode", "help"])
--
 1. Chomsky Normal Form (binarization)

    It is well known that any grammar has a Chomsky Normal Form (CNF)
    equivalent grammar where CNF is defined by every production having
    either two non-terminals or one terminal on its right hand side.
    When we have hierarchically structured data (ie. a treebank), it is
    natural to view this in terms of productions where the root of every
--

from nltk.tree import Tree

def chomsky_normal_form(tree, factor = "right", horzMarkov = None, vertMarkov = 0, childChar = "|", parentChar = "^"):
    # assume all subtrees have homogeneous children
    # assume all terminals have no siblings

--
                curNode[0:] = [child for child in nodeCopy]


def un_chomsky_normal_form(tree, expandUnary = True, childChar = "|", parentChar = "^", unaryChar = "+"):
    # Traverse the tree-depth first keeping a pointer to the parent for modification purposes.
    nodeList = [(tree,[])]
    while nodeList != []:
--
                nodeList.append((child,node))


def collapse_unary(tree, collapsePOS = False, collapseRoot = False, joinChar = "+"):
    """
    Collapse subtrees with a single child (ie. unary productions)
    into a new non-terminal (Tree node) joined by 'joinChar'.
--

    :param tree: The Tree to be collapsed
    :type  tree: Tree
    :param collapsePOS: 'False' (default) will not collapse the parent of leaf nodes (ie.
                        Part-of-Speech tags) since they are always unary productions
    :type  collapsePOS: bool
    :param collapseRoot: 'False' (default) will not modify the root production
                         if it is unary.  For the Penn WSJ treebank corpus, this corresponds
                         to the TOP -> productions.
    :type collapseRoot: bool
    :param joinChar: A string used to connect collapsed node values (default = "+")
    :type  joinChar: str
    """

--
# Demonstration
#################################################################

def demo():
    """
    A demonstration showing how each tree transform can be used.
    """
--

import yaml

def custom_import(name):
    components = name.split('.')
    module_path = '.'.join(components[:-1])
    mod = __import__(module_path)
--
        mod = getattr(mod, comp)
    return mod

def metaloader(classpath):
    def loader(*args, **kwds):
        classref = custom_import(classpath)
        return classref.from_yaml(*args, **kwds)
    return loader

def register_tag(tag, classpath):
    yaml.add_constructor('!'+tag, metaloader(classpath))
    yaml.add_constructor('tag:nltk.org,2011:'+tag,
                         metaloader(classpath))
--
except NameError:
    from sets import Set as set

def getinfo(func):
    """
    Returns an info dictionary containing:
    - name (the name of the function : str)
    - argnames (the names of the arguments : list)
    - defaults (the values of the default arguments : tuple)
    - signature (the signature : str)
    - doc (the docstring : str)
    - module (the module name : str)
    - dict (the function __dict__ : str)

    >>> def f(self, x=1, y=2, *args, **kw): pass

    >>> info = getinfo(f)

--
    >>> info["argnames"]
    ['self', 'x', 'y', 'args', 'kw']

    >>> info["defaults"]
    (1, 2)

    >>> info["signature"]
    'self, x, y, *args, **kw'
    """
    assert inspect.ismethod(func) or inspect.isfunction(func)
    regargs, varargs, varkwargs, defaults = inspect.getargspec(func)
    argnames = list(regargs)
    if varargs:
        argnames.append(varargs)
    if varkwargs:
        argnames.append(varkwargs)
    signature = inspect.formatargspec(regargs, varargs, varkwargs, defaults,
                                      formatvalue=lambda value: "")[1:-1]

    # pypy compatibility
--
        _globals = func.func_globals

    return dict(name=func.__name__, argnames=argnames, signature=signature,
                defaults = func.__defaults__, doc=func.__doc__,
                module=func.__module__, dict=func.__dict__,
                globals=_globals, closure=_closure)

# akin to functools.update_wrapper
def update_wrapper(wrapper, model, infodict=None):
    infodict = infodict or getinfo(model)
    wrapper.__name__ = infodict['name']
    wrapper.__doc__ = infodict['doc']
    wrapper.__module__ = infodict['module']
    wrapper.__dict__.update(infodict['dict'])
    wrapper.__defaults__ = infodict['defaults']
    wrapper.undecorated = model
    return wrapper

def new_wrapper(wrapper, model):
    """
    An improvement over functools.update_wrapper. The wrapper is a generic
    callable object. It works by generating a copy of the wrapper with the
    right signature and by updating the copy, not the original.
    Moreovoer, 'model' can be a dictionary with keys 'name', 'doc', 'module',
    'dict', 'defaults'.
    """
    if isinstance(model, dict):
        infodict = model
--
    return update_wrapper(funcopy, model, infodict)

# helper used in decorator_factory
def __call__(self, func):
    return new_wrapper(lambda *a, **k : self.call(func, *a, **k), func)

def decorator_factory(cls):
    """
    Take a class with a ``.caller`` method and return a callable decorator
    object. It works by adding a suitable __call__ method to the class;
--
    cls.__call__ = __call__
    return cls

def decorator(caller):
    """
    General purpose decorator factory: takes a caller function as
    input and returns a decorator with the same attributes.
    A caller function is any function like this::

     def caller(func, *args, **kw):
         # do something
         return func(*args, **kw)

    Here is an example of usage:

    >>> @decorator
    ... def chatty(f, *args, **kw):
    ...     print("Calling %r" % f.__name__)
    ...     return f(*args, **kw)

--
    'chatty'

    >>> @chatty
    ... def f(): pass
    ...
    >>> f()
    Calling 'f'
--
    """
    if inspect.isclass(caller):
        return decorator_factory(caller)
    def _decorator(func): # the real meat is here
        infodict = getinfo(func)
        argnames = infodict['argnames']
        assert not ('_call_' in argnames or '_func_' in argnames), (
--
        return update_wrapper(dec_func, func, infodict)
    return update_wrapper(_decorator, caller)

def getattr_(obj, name, default_thunk):
    "Similar to .setdefault in dictionaries."
    try:
        return getattr(obj, name)
    except AttributeError:
        default = default_thunk()
        setattr(obj, name, default)
        return default

@decorator
def memoize(func, *args):
    dic = getattr_(func, "memoize_dic", dict)
    # memoize_dic is created at the first call
    if args in dic:
--
ptext4 = Text(mac_morpho.words('mu94se01.txt'), name="Folha de Sao Paulo (1994)")
print("ptext4:", ptext4.name)

def texts():
    print("ptext1:", ptext1.name)
    print("ptext2:", ptext2.name)
    print("ptext3:", ptext3.name)
--
psent3 = "No princpio, criou Deus os cus e a terra.".split()
psent4 = "A Critas acredita que outros cubanos devem chegar ao Brasil .".split()

def sents():
    print("psent1:", " ".join(psent1))
    print("psent2:", " ".join(psent2))
    print("psent3:", " ".join(psent3))
--
from nltk.tokenize.texttiling import TextTilingTokenizer

# Standard sentence tokenizer.
def sent_tokenize(text):
    """
    Return a sentence-tokenized copy of *text*,
    using NLTK's recommended sentence tokenizer
--

# Standard word tokenizer.
_word_tokenize = TreebankWordTokenizer().tokenize
def word_tokenize(text):
    """
    Return a tokenized copy of *text*,
    using NLTK's recommended word tokenizer
--

BLOCK_COMPARISON, VOCABULARY_INTRODUCTION = 0, 1
LC, HC = 0, 1
DEFAULT_SMOOTHING = [0]


class TextTilingTokenizer(TokenizerI):
--
    :param k: Size (in sentences) of the block used in the block comparison method
    :type k: int
    :param similarity_method: The method used for determining similarity scores:
       `BLOCK_COMPARISON` (default) or `VOCABULARY_INTRODUCTION`.
    :type similarity_method: constant
    :param stopwords: A list of stopwords that are filtered out (defaults to NLTK's stopwords corpus)
    :type stopwords: list(str)
    :param smoothing_method: The method used for smoothing the score plot:
      `DEFAULT_SMOOTHING` (default)
    :type smoothing_method: constant
    :param smoothing_width: The width of the window used by the smoothing method
    :type smoothing_width: int
    :param smoothing_rounds: The number of smoothing passes
    :type smoothing_rounds: int
    :param cutoff_policy: The policy used to determine the number of boundaries:
      `HC` (default) or `LC`
    :type cutoff_policy: constant
    """

    def __init__(self,
                 w=20,
                 k=10,
                 similarity_method=BLOCK_COMPARISON,
                 stopwords=None,
                 smoothing_method=DEFAULT_SMOOTHING,
                 smoothing_width=2,
                 smoothing_rounds=1,
                 cutoff_policy=HC,
--
        self.__dict__.update(locals())
        del self.__dict__['self']

    def tokenize(self, text):
        """Return a tokenized copy of *text*, where each "token" represents
        a separate topic."""

--
        elif self.similarity_method == VOCABULARY_INTRODUCTION:
            raise NotImplementedError("Vocabulary introduction not implemented")

        if self.smoothing_method == DEFAULT_SMOOTHING:
            smooth_scores = self._smooth_scores(gap_scores)
        # End of Lexical score Determination

--
            return gap_scores, smooth_scores, depth_scores, segment_boundaries
        return segmented_text

    def _block_comparison(self, tokseqs, token_table):
        "Implements the block comparison method"
        def blk_frq(tok, block):
            ts_occs = filter(lambda o: o[0] in block,
                             token_table[tok].ts_occurences)
            freq = sum([tsocc[1] for tsocc in ts_occs])
--

        return gap_scores

    def _smooth_scores(self, gap_scores):
        "Wraps the smooth function from the SciPy Cookbook"
        return list(smooth(numpy.array(gap_scores[:]),
                           window_len = self.smoothing_width+1))

    def _mark_paragraph_breaks(self, text):
        """Identifies indented text or line breaks as the beginning of
        paragraphs"""

--

        return pbreaks

    def _divide_to_tokensequences(self, text):
        "Divides the text into pseudosentences of fixed size"
        w = self.w
        wrdindex_list = []
--
        return [TokenSequence(i/w, wrdindex_list[i:i+w])
                for i in range(0, len(wrdindex_list), w)]

    def _create_token_table(self, token_sequences, par_breaks):
        "Creates a table of TokenTableFields"
        token_table = {}
        current_par = 0
--

        return token_table

    def _identify_boundaries(self, depth_scores):
        """Identifies boundaries at the peaks of similarity score
        differences"""

--
                    boundaries[dt[1]] = 0
        return boundaries

    def _depth_scores(self, scores):
        """Calculates the depth of each gap, i.e. the average difference
        between the left and right peaks and the gap's score"""

--

        return depth_scores

    def _normalize_boundaries(self, text, boundaries, paragraph_breaks):
        """Normalize the boundaries identified to the original text's
        paragraph breaks"""

--
class TokenTableField(object):
    """A field in the token table holding parameters for each token,
    used later in the process"""
    def __init__(self,
                 first_pos,
                 ts_occurences,
                 total_count=1,
--

class TokenSequence(object):
    "A token list with its original length and its index"
    def __init__(self,
                 index,
                 wrdindex_list,
                 original_length=None):
--
        del self.__dict__['self']

#Pasted from the SciPy cookbook: http://www.scipy.org/Cookbook/SignalSmooth
def smooth(x,window_len=11,window='flat'):
    """smooth the data using a window with requested size.

    This method is based on the convolution of a scaled window with the signal.
--
    return y[window_len-1:-window_len+1]


def demo(text=None):
    from nltk.corpus import brown
    import pylab
    tt=TextTilingTokenizer(demo_mode=True)
--

from re import finditer

def string_span_tokenize(s, sep):
    r"""
    Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``
    tuples, by splitting the string at each occurrence of *sep*.
--

        left = right + len(sep)

def regexp_span_tokenize(s, regexp):
    r"""
    Return the offsets of the tokens in *s*, as a sequence of ``(start, end)``
    tuples, by splitting the string at each successive match of *regexp*.
--
        left = next
    yield left, len(s)

def spans_to_relative(spans):
    r"""
    Return a sequence of relative spans, given a sequence of spans.

--
    is ever required directly, use ``for char in string``.
    """

    def tokenize(self, s):
        return list(s)

    def span_tokenize(self, s):
        for i, j in enumerate(range(1, len(s+1))):
            yield i, j

--
           a corresponding token ``''`` after that newline.
    """

    def __init__(self, blanklines='discard'):
        valid_blanklines = ('discard', 'keep', 'discard-eof')
        if blanklines not in valid_blanklines:
            raise ValueError('Blank lines must be one of: %s' %
--

        self._blanklines = blanklines

    def tokenize(self, s):
        lines = s.splitlines()
        # If requested, strip off blank lines.
        if self._blanklines == 'discard':
--
        return lines

    # discard-eof not implemented
    def span_tokenize(self, s):
        if self._blanklines == 'keep':
            for span in string_span_tokenize(s, r'\n'):
                yield span
--
######################################################################
# XXX: it is stated in module docs that there is no function versions

def line_tokenize(text, blanklines='discard'):
    return LineTokenizer(blanklines).tokenize(text)


--
    >>> SExprTokenizer().tokenize('(a b (c d)) e f (g)')
    ['(a b (c d))', 'e', 'f', '(g)']

By default, `SExprTokenizer` will raise a ``ValueError`` exception if
used to tokenize an expression with non-matching parentheses:

    >>> SExprTokenizer().tokenize('c) d) e (f (g')
--
    For example, the string ``(a (b c)) d e (f)`` consists of four
    s-expressions: ``(a (b c))``, ``d``, ``e``, and ``(f)``.

    By default, the characters ``(`` and ``)`` are treated as open and
    close parentheses, but alternative strings may be specified.

    :param parens: A two-element sequence specifying the open and close parentheses
--
    :param strict: If true, then raise an exception when tokenizing an ill-formed sexpr.
    """

    def __init__(self, parens='()', strict=True):
        if len(parens) != 2:
            raise ValueError('parens must contain exactly two strings')
        self._strict = strict
--
        self._paren_regexp = re.compile('%s|%s' % (re.escape(parens[0]),
                                                   re.escape(parens[1])))

    def tokenize(self, text):
        """
        Return a list of s-expressions extracted from *text*.
        For example:
--
    ['Good', 'New', 'York', 'Please', 'Thanks']

This module contains several subclasses of ``RegexpTokenizer``
that use pre-defined regular expressions.

    >>> from nltk.tokenize import BlanklineTokenizer
    >>> # Uses '\s*\n\s*\n\s*':
--
        tokens can only be generated if `_gaps == True`.
    :type flags: int
    :param flags: The regexp flags used to compile this
        tokenizer's pattern.  By default, the following flags are
        used: `re.UNICODE | re.MULTILINE | re.DOTALL`.

    """
    def __init__(self, pattern, gaps=False, discard_empty=True,
                 flags=re.UNICODE | re.MULTILINE | re.DOTALL):
        # If they gave us a regexp object, extract the pattern.
        pattern = getattr(pattern, 'pattern', pattern)
--
            raise ValueError('Error in regular expression %r: %s' %
                             (pattern, e))

    def tokenize(self, text):
        # If our regexp matches gaps, use re.split:
        if self._gaps:
            if self._discard_empty:
--
        else:
            return self._regexp.findall(text)

    def span_tokenize(self, text):
        if self._gaps:
            for left, right in regexp_span_tokenize(text, self._regexp):
                if not (self._discard_empty and left == right):
--
            for m in re.finditer(self._regexp, text):
                yield m.span()

    def __repr__(self):
        return ('%s(pattern=%r, gaps=%r, discard_empty=%r, flags=%r)' %
                (self.__class__.__name__, self._pattern, self._gaps,
                 self._discard_empty, self._flags))
--
        'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']
    """

    def __init__(self):
        RegexpTokenizer.__init__(self, r'\s+', gaps=True)

class BlanklineTokenizer(RegexpTokenizer):
    """
    Tokenize a string, treating any sequence of blank lines as a delimiter.
    Blank lines are defined as lines containing no characters, except for
    space or tab characters.
    """
    def __init__(self):
        RegexpTokenizer.__init__(self, r'\s*\n\s*\n\s*', gaps=True)

class WordPunctTokenizer(RegexpTokenizer):
--
        ['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York',
        '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']
    """
    def __init__(self):
        RegexpTokenizer.__init__(self, r'\w+|[^\w\s]+')

######################################################################
#{ Tokenization Functions
######################################################################

def regexp_tokenize(text, pattern, gaps=False, discard_empty=True,
                    flags=re.UNICODE | re.MULTILINE | re.DOTALL):
    """
    Return a tokenized copy of *text*.  See :class:`.RegexpTokenizer`
--
(Note that whitespace from the original text, including newlines, is
retained in the output.)

Punctuation following sentences is also included by default
(from NLTK 3.0 onwards). It can be excluded with the realign_boundaries
flag.

--

import re
import math
from collections import defaultdict

from nltk.compat import unicode_repr, python_2_unicode_compatible, string_types
from nltk.probability import FreqDist
--
#{ Decision reasons for debugging
######################################################################

REASON_DEFAULT_DECISION = 'default decision'
REASON_KNOWN_COLLOCATION = 'known collocation (both words)'
REASON_ABBR_WITH_ORTHOGRAPHIC_HEURISTIC = 'abbreviation + orthographic heuristic'
REASON_ABBR_WITH_SENTENCE_STARTER = 'abbreviation + frequent sentence starter'
--

    __slots__ = ('_re_period_context', '_re_word_tokenizer')

    def __getstate__(self):
        # All modifications to the class are performed by inheritance.
        # Non-default parameters to be pickled must be defined in the inherited
        # class.
        return 1

    def __setstate__(self, state):
        return 1

    sent_end_chars = ('.', '?', '!')
    """Characters which are candidates for sentence boundaries"""

    @property
    def _re_sent_end_chars(self):
        return '[%s]' % re.escape(''.join(self.sent_end_chars))

    internal_punctuation = ',:;' # might want to extend this..
--
    """Format of a regular expression to split punctuation from words,
    excluding period."""

    def _word_tokenizer_re(self):
        """Compiles and returns a regular expression for word tokenization"""
        try:
            return self._re_word_tokenizer
--
            )
            return self._re_word_tokenizer

    def word_tokenize(self, s):
        """Tokenize a string to split off punctuation other than periods"""
        return self._word_tokenizer_re().findall(s)

--
    sentence boundaries. Matches token which the possible sentence boundary
    ends, and matches the following token within a lookahead expression."""

    def period_context_re(self):
        """Compiles and returns a regular expression to find contexts
        including possible sentence boundaries."""
        try:
--

class PunktWordTokenizer(TokenizerI):
    # Retained for backward compatibility
    def __init__(self, lang_vars=PunktLanguageVars()):
        self._lang_vars = lang_vars

    def tokenize(self, text):
        return self._lang_vars.word_tokenize(text)

#}
--
#{ Helper Functions
#////////////////////////////////////////////////////////////

def _pair_iter(it):
    """
    Yields pairs of tokens from the given iterator such that each input
    token will appear as the first element in a yielded tuple. The last
--
class PunktParameters(object):
    """Stores data used to perform sentence boundary detection with Punkt."""

    def __init__(self):
        self.abbrev_types = set()
        """A set of word types for known abbreviations."""

--
        """A set of word types for words that often appear at the
        beginning of sentences."""

        self.ortho_context = defaultdict(int)
        """A dictionary mapping word types to the set of orthographic
        contexts that word type appears in.  Contexts are represented
        by adding orthographic context flags: ..."""

    def clear_abbrevs(self):
        self.abbrev_types = set()

    def clear_collocations(self):
        self.collocations = set()

    def clear_sent_starters(self):
        self.sent_starters = set()

    def clear_ortho_context(self):
        self.ortho_context = defaultdict(int)

    def add_ortho_context(self, typ, flag):
        self.ortho_context[typ] |= flag

    def _debug_ortho_context(self, typ):
        c = self.ortho_context[typ]
        if c & _ORTHO_BEG_UC:
            yield 'BEG-UC'
--
    ]
    __slots__ = ['tok', 'type', 'period_final'] + _properties

    def __init__(self, tok, **params):
        self.tok = tok
        self.type = self._get_type(tok)
        self.period_final = tok.endswith('.')
--
    #{ Derived properties
    #////////////////////////////////////////////////////////////

    def _get_type(self, tok):
        """Returns a case-normalized representation of the token."""
        return self._RE_NUMERIC.sub('##number##', tok.lower())

    @property
    def type_no_period(self):
        """
        The type with its final period removed if it has one.
        """
--
        return self.type

    @property
    def type_no_sentperiod(self):
        """
        The type with its final period removed if it is marked as a
        sentence break.
--
        return self.type

    @property
    def first_upper(self):
        """True if the token's first character is uppercase."""
        return self.tok[0].isupper()

    @property
    def first_lower(self):
        """True if the token's first character is lowercase."""
        return self.tok[0].islower()

    @property
    def first_case(self):
        if self.first_lower:
            return 'lower'
        elif self.first_upper:
--
        return 'none'

    @property
    def is_ellipsis(self):
        """True if the token text is that of an ellipsis."""
        return self._RE_ELLIPSIS.match(self.tok)

    @property
    def is_number(self):
        """True if the token text is that of a number."""
        return self.type.startswith('##number##')

    @property
    def is_initial(self):
        """True if the token text is that of an initial."""
        return self._RE_INITIAL.match(self.tok)

    @property
    def is_alpha(self):
        """True if the token text is all alphabetic."""
        return self._RE_ALPHA.match(self.tok)

    @property
    def is_non_punct(self):
        """True if the token is either a number or is alphabetic."""
        return _re_non_punct.search(self.type)

--
    #{ String representation
    #////////////////////////////////////////////////////////////

    def __repr__(self):
        """
        A string representation of the token that can reproduce it
        with eval(), which lists all the token's non-default
        annotations.
        """
        typestr = (' type=%s,' % unicode_repr(self.type)
--
        return '%s(%s,%s %s)' % (self.__class__.__name__,
            unicode_repr(self.tok), typestr, propvals)

    def __str__(self):
        """
        A string representation akin to that used by Kiss and Strunk.
        """
--
    Includes common components of PunktTrainer and PunktSentenceTokenizer.
    """

    def __init__(self, lang_vars=PunktLanguageVars(), token_cls=PunktToken,
            params=PunktParameters()):
        self._params = params
        self._lang_vars = lang_vars
--
    #{ Word tokenization
    #////////////////////////////////////////////////////////////

    def _tokenize_words(self, plaintext):
        """
        Divide the given text into tokens, using the punkt word
        segmentation regular expression, and generate the resulting list
--
    #{ Annotation Procedures
    #////////////////////////////////////////////////////////////

    def _annotate_first_pass(self, tokens):
        """
        Perform the first pass of annotation, which makes decisions
        based purely based on the word type of each word:
--
            self._first_pass_annotation(aug_tok)
            yield aug_tok

    def _first_pass_annotation(self, aug_tok):
        """
        Performs type-based annotation on a single token.
        """
--
class PunktTrainer(PunktBaseClass):
    """Learns parameters used in Punkt sentence boundary detection."""

    def __init__(self, train_text=None, verbose=False,
            lang_vars=PunktLanguageVars(), token_cls=PunktToken):

        PunktBaseClass.__init__(self, lang_vars=lang_vars,
--
        if train_text:
            self.train(train_text, verbose, finalize=True)

    def get_params(self):
        """
        Calculates and returns parameters for sentence boundary detection as
        derived from training."""
--
    #{ Training..
    #////////////////////////////////////////////////////////////

    def train(self, text, verbose=False, finalize=True):
        """
        Collects training data from a given text. If finalize is True, it
        will determine all the parameters for sentence boundary detection. If
--
        if finalize:
            self.finalize_training(verbose)

    def train_tokens(self, tokens, verbose=False, finalize=True):
        """
        Collects training data from a given list of tokens.
        """
--
        if finalize:
            self.finalize_training(verbose)

    def _train_tokens(self, tokens, verbose):
        self._finalized = False

        # Ensure tokens are a list
--
                self._collocation_fdist.inc(
                    (aug_tok1.type_no_period, aug_tok2.type_no_sentperiod))

    def _unique_types(self, tokens):
        return set(aug_tok.type for aug_tok in tokens)

    def finalize_training(self, verbose=False):
        """
        Uses data that has been gathered in training to determine likely
        collocations and sentence starters.
--
    #{ Overhead reduction
    #////////////////////////////////////////////////////////////

    def freq_threshold(self, ortho_thresh=2, type_thresh=2, colloc_thres=2,
            sentstart_thresh=2):
        """
        Allows memory use to be reduced after much training by removing data
--
        self._sent_starter_fdist = self._freq_threshold(
                self._sent_starter_fdist, sentstart_thresh)

    def _freq_threshold(self, fdist, threshold):
        """
        Returns a FreqDist containing only data with counts below a given
        threshold, as well as a mapping (None -> count_removed).
--
    #{ Orthographic data
    #////////////////////////////////////////////////////////////

    def _get_orthography_data(self, tokens):
        """
        Collect information about whether each token type occurs
        with different case patterns (i) overall, (ii) at
--
    #{ Abbreviations
    #////////////////////////////////////////////////////////////

    def _reclassify_abbrev_types(self, types):
        """
        (Re)classifies each given token if
          - it is period-final and not a known abbreviation; or
--

            yield typ, score, is_add

    def find_abbrev_types(self):
        """
        Recalculates abbreviations given type frequencies, despite no prior
        determination of abbreviations.
--
    # This function combines the work done by the original code's
    # functions `count_orthography_context`, `get_orthography_count`,
    # and `get_rare_abbreviations`.
    def _is_rare_abbrev_type(self, cur_tok, next_tok):
        """
        A word type is counted as a rare abbreviation if...
          - it's not already marked as an abbreviation
--

    # helper for _reclassify_abbrev_types:
    @staticmethod
    def _dunning_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that calculates the modified Dunning log-likelihood
        ratio scores for abbreviation candidates.  The details of how
--
        return (-2.0 * likelihood)

    @staticmethod
    def _col_log_likelihood(count_a, count_b, count_ab, N):
        """
        A function that will just compute log-likelihood estimate, in
        the original paper it's described in algorithm 6 and 7.
--
    #{ Collocation Finder
    #////////////////////////////////////////////////////////////

    def _is_potential_collocation(self, aug_tok1, aug_tok2):
        """
        Returns True if the pair of tokens may form a collocation given
        log-likelihood statistics.
--
                and aug_tok1.is_non_punct
                and aug_tok2.is_non_punct)

    def _find_collocations(self):
        """
        Generates likely collocations and their log-likelihood.
        """
--
    #{ Sentence-Starter Finder
    #////////////////////////////////////////////////////////////

    def _is_potential_sent_starter(self, cur_tok, prev_tok):
        """
        Returns True given a token and the token that preceds it if it
        seems clear that the token is beginning a sentence.
--
             not (prev_tok.is_number or prev_tok.is_initial) and
             cur_tok.is_alpha )

    def _find_sent_starters(self):
        """
        Uses collocation heuristics for each candidate token to
        determine if it frequently starts sentences.
--

                yield typ, ll

    def _get_sentbreak_count(self, tokens):
        """
        Returns the number of sentence breaks marked in a given set of
        augmented tokens.
--
    This approach has been shown to work well for many European
    languages.
    """
    def __init__(self, train_text=None, verbose=False,
            lang_vars=PunktLanguageVars(), token_cls=PunktToken):
        """
        train_text can either be the sole training text for this sentence
--
        if train_text:
            self._params = self.train(train_text, verbose)

    def train(self, train_text, verbose=False):
        """
        Derives parameters from a given training text, or uses the parameters
        given. Repeated calls to this method destroy previous parameters. For
--
    #{ Tokenization
    #////////////////////////////////////////////////////////////

    def tokenize(self, text, realign_boundaries=True):
        """
        Given a text, returns a list of the sentences in that text.
        """
        return list(self.sentences_from_text(text, realign_boundaries))

    def debug_decisions(self, text):
        """
        Classifies candidate periods as sentence breaks, yielding a dict for
        each that may be used to understand why the decision was made.
--
                type2_ortho_contexts=set(self._params._debug_ortho_context(tokens[1].type_no_sentperiod)),
                collocation=(tokens[0].type_no_sentperiod, tokens[1].type_no_sentperiod) in self._params.collocations,

                reason=self._second_pass_annotation(tokens[0], tokens[1]) or REASON_DEFAULT_DECISION,
                break_decision=tokens[0].sentbreak,
            )

    def span_tokenize(self, text):
        """
        Given a text, returns a list of the (start, end) spans of sentences
        in the text.
        """
        return [(sl.start, sl.stop) for sl in self._slices_from_text(text)]

    def sentences_from_text(self, text, realign_boundaries=True):
        """
        Given a text, generates the sentences in that text by only
        testing candidate sentence breaks. If realign_boundaries is
--
            sents = self._realign_boundaries(sents)
        return sents

    def _slices_from_text(self, text):
        last_break = 0
        for match in self._lang_vars.period_context_re().finditer(text):
            context = match.group() + match.group('after_tok')
--
                    last_break = match.end()
        yield slice(last_break, len(text))

    def _realign_boundaries(self, sents):
        """
        Attempts to realign punctuation that falls after the period but
        should otherwise be included in the same sentence.
--
                if s1:
                    yield s1

    def text_contains_sentbreak(self, text):
        """
        Returns True if the given text includes a sentence break.
        """
--
                found = True
        return False

    def sentences_from_text_legacy(self, text):
        """
        Given a text, generates the sentences in that text. Annotates all
        tokens, rather than just those with possible sentence breaks. Should
--
        tokens = self._annotate_tokens(self._tokenize_words(text))
        return self._build_sentence_list(text, tokens)

    def sentences_from_tokens(self, tokens):
        """
        Given a sequence of tokens, generates lists of tokens, each list
        corresponding to a sentence.
--
        if sentence:
            yield sentence

    def _annotate_tokens(self, tokens):
        """
        Given a set of tokens augmented with markers for line-start and
        paragraph-start, returns an iterator through those tokens with full
--

        return tokens

    def _build_sentence_list(self, text, tokens):
        """
        Given the original text and the list of augmented word tokens,
        construct and return a tokenized list of sentence strings.
--
            yield sentence

    # [XX] TESTING
    def dump(self, tokens):
        print('writing to /tmp/punkt.new...')
        out = open('/tmp/punkt.new', 'w')
        for aug_tok in tokens:
--
    #{ Annotation Procedures
    #////////////////////////////////////////////////////////////

    def _annotate_second_pass(self, tokens):
        """
        Performs a token-based classification (section 4) over the given
        tokens, making use of the orthographic heuristic (4.1.1), collocation
--
            self._second_pass_annotation(t1, t2)
            yield t1

    def _second_pass_annotation(self, aug_tok1, aug_tok2):
        """
        Performs token-based classification over a pair of contiguous tokens
        updating the first.
--

        return

    def _ortho_heuristic(self, aug_tok):
        """
        Decide whether the given token is the first token in a sentence.
        """
--
    orthographic heuristic suggests is a sentence starter? %(type2_ortho_heuristic)s
    orthographic contexts in training: %(type2_ortho_contexts)s
'''
def format_debug_decision(d):
    return DEBUG_DECISION_FMT % d

def demo(text, tok_cls=PunktSentenceTokenizer, train_cls=PunktTrainer):
    """Builds a punkt model and applies it to the same text"""
    cleanup = lambda s: re.compile(r'(?:\r|^\s+)', re.MULTILINE).sub('', s).replace('\n', ' ')
    trainer = train_cls()
--
    CONTRACTIONS4 = [re.compile(r"(?i)\b(whad)(dd)(ya)\b"),
                     re.compile(r"(?i)\b(wha)(t)(cha)\b")]

    def tokenize(self, text):
        #starting quotes
        text = re.sub(r'^\"', r'``', text)
        text = re.sub(r'(``)', r' \1 ', text)
--
class TokenizerI(object):
    """
    A processing interface for tokenizing a string.
    Subclasses must define ``tokenize()`` or ``batch_tokenize()`` (or both).
    """
    def tokenize(self, s):
        """
        Return a tokenized copy of *s*.

--
        else:
            raise NotImplementedError()

    def span_tokenize(self, s):
        """
        Identify the tokens using integer offsets ``(start_i, end_i)``,
        where ``s[start_i:end_i]`` is the corresponding token.
--
        """
        raise NotImplementedError()

    def batch_tokenize(self, strings):
        """
        Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:

--
        """
        return [self.tokenize(s) for s in strings]

    def batch_span_tokenize(self, strings):
        """
        Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:

--

class StringTokenizer(TokenizerI):
    """A tokenizer that divides a string into substrings by splitting
    on the specified string (defined in subclasses).
    """

    def tokenize(self, s):
        return s.split(self._string)

    def span_tokenize(self, s):
        for span in string_span_tokenize(s, self._string):
            yield span

--
from nltk.compat import xrange, izip
from nltk.util import LazyConcatenation, LazyMap

def accuracy(reference, test):
    """
    Given a list of reference values and a corresponding list of test
    values, return the fraction of corresponding values that are
--
        raise ValueError("Lists must have the same length.")
    return float(sum(x == y for x, y in izip(reference, test))) / len(test)

def precision(reference, test):
    """
    Given a set of reference values and a set of test values, return
    the fraction of test values that appear in the reference set.
--
    else:
        return float(len(reference.intersection(test)))/len(test)

def recall(reference, test):
    """
    Given a set of reference values and a set of test values, return
    the fraction of reference values that appear in the test set.
--
    else:
        return float(len(reference.intersection(test)))/len(reference)

def f_measure(reference, test, alpha=0.5):
    """
    Given a set of reference values and a set of test values, return
    the f-measure of the test values, when compared against the
    reference values.  The f-measure is the harmonic mean of the
    ``precision`` and ``recall``, weighted by ``alpha``.  In particular,
    given the precision *p* and recall *r* defined by:

    - *p* = card(``reference`` intersection ``test``)/card(``test``)
    - *r* = card(``reference`` intersection ``test``)/card(``reference``)
--
        return 0
    return 1.0/(alpha/p + (1-alpha)/r)

def log_likelihood(reference, test):
    """
    Given a list of reference values and a corresponding list of test
    probability distributions, return the average log likelihood of
--
                            for (val, dist) in izip(reference, test))
    return total_likelihood/len(reference)

def approxrand(a, b, **kwargs):
    """
    Returns an approximate significance level between two lists of
    independently generated test values.
--
    return (significance, c, shuffles)


def demo():
    print('-'*75)
    reference = 'DET NN VB DET JJ NN NN IN DET NN'.split()
    test    = 'DET VB VB DET NN NN NN IN DET NN'.split()
--
An agreement coefficient calculates the amount that annotators agreed on label
assignments beyond what is expected by chance.

In defining the AnnotationTask class, we use naming conventions similar to the
paper's terminology.  There are three types of objects in an annotation task:

    the coders (variables "c" and "C")
--
labels as complete disagreement, and so the AnnotationTask constructor can also
take a distance metric as a final argument.  Distance metrics are simply
functions that take two arguments, and return a value between 0.0 and 1.0
indicating the distance between them.  If not supplied, the default is binary
comparison between the arguments.

The simplest way to initialize an AnnotationTask is with a list of triples,
--
    is the MASI metric, which requires Python sets.
    """

    def __init__(self, data=None, distance=binary_distance):
        """Initialize an empty annotation task.

        """
--
        if data is not None:
            self.load_array(data)

    def __str__(self):
        return "\r\n".join(map(lambda x:"%s\t%s\t%s" %
                               (x['coder'], x['item'].replace('_', "\t"),
                                ",".join(x['labels'])), self.data))

    def load_array(self, array):
        """Load the results of annotation.

        The argument is a list of 3-tuples, each representing a coder's labeling of an item:
--
            self.I.add(item)
            self.data.append({'coder':coder, 'labels':labels, 'item':item})

    def agr(self, cA, cB, i, data=None):
        """Agreement between two coders on a given item

        """
--
                      k1['labels'], k2['labels'], 1.0 - ret)
        return ret

    def Nk(self, k):
        return float(sum(1 for x in self.data if x['labels'] == k))

    def Nik(self, i, k):
        return float(sum(1 for x in self.data if x['item'] == i and x['labels'] == k))

    def Nck(self, c, k):
        return float(sum(1 for x in self.data if x['coder'] == c and x['labels'] == k))

    @deprecated('Use Nk, Nik or Nck instead')
    def N(self, k=None, i=None, c=None):
        """Implements the "n-notation" used in Artstein and Poesio (2007)

        """
--
        log.debug("Count on N[%s,%s,%s]: %d", k, i, c, ret)
        return ret

    def _grouped_data(self, field, data=None):
        data = data or self.data
        return groupby(sorted(data, key=itemgetter(field)), itemgetter(field))

    def Ao(self, cA, cB):
        """Observed agreement between two coders on all items.

        """
--
        log.debug("Observed agreement between %s and %s: %f", cA, cB, ret)
        return ret

    def _pairwise_average(self, function):
        """
        Calculates the average of function results for each coder pair
        """
--
        ret = total / n
        return ret

    def avg_Ao(self):
        """Average observed agreement across all coders and items.

        """
--
        log.debug("Average observed agreement: %f", ret)
        return ret

    def Do_alpha(self):
        """The observed disagreement for the alpha coefficient.

        The alpha coefficient, unlike the other metrics, uses this rather than
--
        log.debug("Observed disagreement: %f", ret)
        return ret

    def Do_Kw_pairwise(self,cA,cB,max_distance=1.0):
        """The observed disagreement for the weighted kappa coefficient.

        """
--
        log.debug("Observed disagreement between %s and %s: %f", cA, cB, ret)
        return ret

    def Do_Kw(self, max_distance=1.0):
        """Averaged over all labelers

        """
--
        return ret

    # Agreement Coefficients
    def S(self):
        """Bennett, Albert and Goldstein 1954

        """
--
        ret = (self.avg_Ao() - Ae) / (1.0 - Ae)
        return ret

    def pi(self):
        """Scott 1955; here, multi-pi.
        Equivalent to K from Siegel and Castellan (1988).

--
        Ae = total / float((len(self.I) * len(self.C)) ** 2)
        return (self.avg_Ao() - Ae) / (1 - Ae)

    def Ae_kappa(self, cA, cB):
        Ae = 0.0
        nitems = float(len(self.I))
        label_freqs = ConditionalFreqDist((x['labels'], x['coder']) for x in self.data)
--
            Ae += (label_freqs[k][cA] / nitems) * (label_freqs[k][cB] / nitems)
        return Ae

    def kappa_pairwise(self, cA, cB):
        """

        """
--
        log.debug("Expected agreement between %s and %s: %f", cA, cB, Ae)
        return ret

    def kappa(self):
        """Cohen 1960
        Averages naively over kappas for each coder pair.

        """
        return self._pairwise_average(self.kappa_pairwise)

    def multi_kappa(self):
        """Davies and Fleiss 1982
        Averages over observed and expected agreements for each coder pair.

--
        Ae = self._pairwise_average(self.Ae_kappa)
        return (self.avg_Ao() - Ae) / (1.0 - Ae)

    def alpha(self):
        """Krippendorff 1980

        """
--
        ret = 1.0 - (self.Do_alpha() / De)
        return ret

    def weighted_kappa_pairwise(self, cA, cB, max_distance=1.0):
        """Cohen 1968

        """
--
        ret = 1.0 - (Do / De)
        return ret

    def weighted_kappa(self, max_distance=1.0):
        """Cohen 1968

        """
--

    # process command-line arguments
    parser = optparse.OptionParser()
    parser.add_option("-d", "--distance", dest="distance", default="binary_distance",
                      help="distance metric to use")
    parser.add_option("-a", "--agreement", dest="agreement", default="kappa",
                      help="agreement coefficient to calculate")
    parser.add_option("-e", "--exclude", dest="exclude", action="append",
                      default=[], help="coder names to exclude (may be specified multiple times)")
    parser.add_option("-i", "--include", dest="include", action="append", default=[],
                      help="coder names to include, same format as exclude")
    parser.add_option("-f", "--file", dest="file",
                      help="file to read labelings from, each line with three columns: 'labeler item labels'")
    parser.add_option("-v", "--verbose", dest="verbose", default='0',
                      help="how much debugging to print on stderr (0-4)")
    parser.add_option("-c", "--columnsep", dest="columnsep", default="\t",
                      help="char/string that separates the three columns in the file, defaults to tab")
    parser.add_option("-l", "--labelsep", dest="labelsep", default=",",
                      help="char/string that separates labels (if labelers can assign more than one), defaults to comma")
    parser.add_option("-p", "--presence", dest="presence", default=None,
                      help="convert each labeling into 1 or 0, based on presence of LABEL")
    parser.add_option("-T", "--thorough", dest="thorough", default=False, action="store_true",
                      help="calculate agreement for every subset of the annotators")
    (options, remainder) = parser.parse_args()

--

class NgramAssocMeasures(object):
    """
    An abstract class defining a collection of generic association measures.
    Each public method returns a score, taking the following arguments::

        score_fn(count_of_ngram,
--

    See ``BigramAssocMeasures`` and ``TrigramAssocMeasures``

    Inheriting classes should define a property _n, and a method _contingency
    which calculates contingency values from marginals in order for all
    association measures defined here to be usable.
    """

    @staticmethod
    def _contingency(*marginals):
        """Calculates values of a contingency table from marginal values."""
        raise NotImplementedError("The contingency table is not available"
                                    "in the general ngram case")

    @staticmethod
    def _marginals(*contingency):
        """Calculates values of contingency table marginals from its values."""
        raise NotImplementedError("The contingency table is not available"
                                    "in the general ngram case")

    @classmethod
    def _expected_values(cls, cont):
        """Calculates expected values for a contingency table."""
        n_all = sum(cont)
        bits = [1 << i for i in range(cls._n)]
--
                   float(n_all ** (cls._n - 1)))

    @staticmethod
    def raw_freq(*marginals):
        """Scores ngrams by their frequency"""
        return float(marginals[NGRAM]) / marginals[TOTAL]

    @classmethod
    def student_t(cls, *marginals):
        """Scores ngrams using Student's t test with independence hypothesis
        for unigrams, as in Manning and Schutze 5.3.1.
        """
--
                (marginals[NGRAM] + _SMALL) ** .5)

    @classmethod
    def chi_sq(cls, *marginals):
        """Scores ngrams using Pearson's chi-square as in Manning and Schutze
        5.3.3.
        """
--
                   for obs, exp in zip(cont, exps))

    @staticmethod
    def mi_like(*marginals, **kwargs):
        """Scores ngrams using a variant of mutual information. The keyword
        argument power sets an exponent (default 3) for the numerator. No
        logarithm of the result is calculated.
        """
        return (marginals[NGRAM] ** kwargs.get('power', 3) /
                float(_product(marginals[UNIGRAMS])))

    @classmethod
    def pmi(cls, *marginals):
        """Scores ngrams by pointwise mutual information, as in Manning and
        Schutze 5.4.
        """
--
                _log2(_product(marginals[UNIGRAMS])))

    @classmethod
    def likelihood_ratio(cls, *marginals):
        """Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.
        """
        cont = cls._contingency(*marginals)
--
                    for obs, exp in zip(cont, cls._expected_values(cont))))

    @classmethod
    def poisson_stirling(cls, *marginals):
        """Scores ngrams using the Poisson-Stirling measure."""
        exp = (_product(marginals[UNIGRAMS]) /
              float(marginals[TOTAL] ** (cls._n - 1)))
        return marginals[NGRAM] * (_log2(marginals[NGRAM] / exp) - 1)

    @classmethod
    def jaccard(cls, *marginals):
        """Scores ngrams using the Jaccard index."""
        cont = cls._contingency(*marginals)
        return float(cont[0]) / sum(cont[:-1])
--
    _n = 2

    @staticmethod
    def _contingency(n_ii, n_ix_xi_tuple, n_xx):
        """Calculates values of a bigram contingency table from marginal values."""
        (n_ix, n_xi) = n_ix_xi_tuple
        n_oi = n_xi - n_ii
--
        return (n_ii, n_oi, n_io, n_xx - n_ii - n_oi - n_io)

    @staticmethod
    def _marginals(n_ii, n_oi, n_io, n_oo):
        """Calculates values of contingency table marginals from its values."""
        return (n_ii, (n_oi + n_ii, n_io + n_ii), n_oo + n_oi + n_io + n_ii)

    @staticmethod
    def _expected_values(cont):
        """Calculates expected values for a contingency table."""
        n_xx = sum(cont)
        # For each contingency table cell
--
            yield (cont[i] + cont[i ^ 1]) * (cont[i] + cont[i ^ 2]) / float(n_xx)

    @classmethod
    def phi_sq(cls, *marginals):
        """Scores bigrams using phi-square, the square of the Pearson correlation
        coefficient.
        """
--
                ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))

    @classmethod
    def chi_sq(cls, n_ii, n_ix_xi_tuple, n_xx):
        """Scores bigrams using chi-square, i.e. phi-sq multiplied by the number
        of bigrams, as in Manning and Schutze 5.3.3.
        """
--
        return n_xx * cls.phi_sq(n_ii, (n_ix, n_xi), n_xx)

    @classmethod
    def fisher(cls, *marginals):
        """Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less
        sensitive to small counts than PMI or Chi Sq, but also more expensive
        to compute. Requires scipy.
--
        return pvalue

    @staticmethod
    def dice(n_ii, n_ix_xi_tuple, n_xx):
        """Scores bigrams using Dice's coefficient."""
        (n_ix, n_xi) = n_ix_xi_tuple
        return 2 * float(n_ii) / (n_ix + n_xi)
--
    _n = 3

    @staticmethod
    def _contingency(n_iii, n_iix_tuple, n_ixx_tuple,
                 n_xxx):
        """Calculates values of a trigram contingency table (or cube) from marginal
        values.
--
                n_iio, n_oio, n_ioo, n_ooo)

    @staticmethod
    def _marginals(*contingency):
        """Calculates values of contingency table marginals from its values."""
        n_iii, n_oii, n_ioi, n_ooi, n_iio, n_oio, n_ioo, n_ooo = contingency
        return (n_iii,
--
    measures are contingency table values rather than marginals.
    """

    def __init__(self, measures):
        """Constructs a ContingencyMeasures given a NgramAssocMeasures class"""
        self.__class__.__name__ = 'Contingency' + measures.__class__.__name__
        for k in dir(measures):
--
            setattr(self, k, v)

    @staticmethod
    def _make_contingency_fn(measures, old_fn):
        """From an association measure function, produces a new function which
        accepts contingency table values as its arguments.
        """
        def res(*contingency):
            return old_fn(*measures._marginals(*contingency))
        res.__doc__ = old_fn.__doc__
        res.__name__ = old_fn.__name__
--
    correspond to incorrect values.
    """

    def __init__(self, reference, test, sort_by_count=False):
        """
        Construct a new confusion matrix from a list of reference
        values and a corresponding list of test values.
--
        if sort_by_count:
            ref_fdist = FreqDist(reference)
            test_fdist = FreqDist(test)
            def key(v): return -(ref_fdist[v]+test_fdist[v])
            values = sorted(set(reference+test), key=key)
        else:
            values = sorted(set(reference+test))
--
        #: The number of correct (on-diagonal) values in the matrix.
        self._correct = sum(confusion[i][i] for i in range(len(values)))

    def __getitem__(self, li_lj_tuple):
        """
        :return: The number of times that value ``li`` was expected and
        value ``lj`` was given.
--
        j = self._indices[lj]
        return self._confusion[i][j]

    def __repr__(self):
        return '<ConfusionMatrix: %s/%s correct>' % (self._correct,
                                                     self._total)

    def __str__(self):
        return self.pp()

    def pp(self, show_percents=False, values_in_chart=True,
           truncate=None, sort_by_count=False):
        """
        :return: A multi-line string representation of this confusion matrix.
--

        return s

    def key(self):
        values = self._values
        str = 'Value key:\n'
        indexlen = len(repr(len(values)-1))
--

        return str

def demo():
    reference = 'DET NN VB DET JJ NN NN IN DET NN'.split()
    test    = 'DET VB VB DET NN NN NN IN DET NN'.split()
    print('Reference =', reference)
--

from nltk.compat import xrange

def windowdiff(seg1, seg2, k, boundary="1", weighted=False):
    """
    Compute the windowdiff score for a pair of segmentations.  A
    segmentation is any sequence over a vocabulary of two items
--

# Generalized Hamming Distance

def _init_mat(nrows, ncols, ins_cost, del_cost):
    mat = np.empty((nrows, ncols))
    mat[0, :] = ins_cost * np.arange(ncols)
    mat[:, 0] = del_cost * np.arange(nrows)
    return mat


def _ghd_aux(mat, rowv, colv, ins_cost, del_cost, shift_cost_coeff):
    for i, rowi in enumerate(rowv):
        for j, colj in enumerate(colv):
            shift_cost = shift_cost_coeff * abs(rowi - colj) + mat[i, j]
--
            mat[i + 1, j + 1] = min(tcost, shift_cost)


def ghd(ref, hyp, ins_cost=2.0, del_cost=2.0, shift_cost_coeff=1.0, boundary='1'):
    """
    Compute the Generalized Hamming Distance for a reference and a hypothetical
    segmentation, corresponding to the cost related to the transformation
--

# Beeferman's Pk text segmentation evaluation metric

def pk(ref, hyp, k=None, boundary='1'):
    """
    Compute the Pk metric for a pair of segmentations A segmentation
    is any sequence over a vocabulary of two items (e.g. "0", "1"),
--


# skip doctests if numpy is not installed
def setup_module(module):
    from nose import SkipTest
    try:
        import numpy
--
Tools for comparing ranked lists.
"""

def _rank_dists(ranks1, ranks2):
    """Finds the difference between the values in ranks1 and ranks2 for keys
    present in both dicts. If the arguments are not dicts, they are converted
    from (key, rank) sequences.
--
            pass


def spearman_correlation(ranks1, ranks2):
    """Returns the Spearman correlation coefficient for two rankings, which
    should be dicts or sequences of (key, rank). The coefficient ranges from
    -1.0 (ranks are opposite) to 1.0 (ranks are identical), and is only
--
    try:
        return 1 - (6 * float(res) / (n * (n*n - 1)))
    except ZeroDivisionError:
        # Result is undefined if only one item is ranked
        return 0.0


def ranks_from_sequence(seq):
    """Given a sequence, yields each element with an increasing rank, suitable
    for use as an argument to ``spearman_correlation``.
    """
    return ((k, i) for i, k in enumerate(seq))


def ranks_from_scores(scores, rank_gap=1e-15):
    """Given a sequence of (key, score) tuples, yields each key with an
    increasing rank, tying with previous key's rank if the difference between
    their scores is less than rank_gap. Suitable for use as an argument to
--
from __future__ import print_function


def _edit_dist_init(len1, len2):
    lev = []
    for i in range(len1):
        lev.append([0] * len2)  # initialize 2-D array to zero
--
    return lev


def _edit_dist_step(lev, i, j, s1, s2, transpositions=False):
    c1 = s1[i - 1]
    c2 = s2[j - 1]

--
    c = lev[i - 1][j - 1] + (c1 != c2)

    # transposition
    d = c + 1  # never picked by default
    if transpositions and i > 1 and j > 1:
        if s1[i - 2] == c2 and s2[j - 2] == c1:
            d = lev[i - 2][j - 2] + 1
--
    lev[i][j] = min(a, b, c, d)


def edit_distance(s1, s2, transpositions=False):
    """
    Calculate the Levenshtein edit-distance between two strings.
    The edit distance is the number of characters that need to be
--
    been done in other orders, but at least three steps are needed.

    This also optionally allows transposition edits (e.g., "ab" -> "ba"),
    though this is disabled by default.

    :param s1, s2: The strings to be analysed
    :param transpositions: Whether to allow transposition edits
--
    return lev[len1][len2]


def binary_distance(label1, label2):
    """Simple equality test.

    0.0 if the labels are identical, 1.0 if they are different.
--
    return 0.0 if label1 == label2 else 1.0


def jaccard_distance(label1, label2):
    """Distance metric comparing set-similarity.

    """
    return (len(label1.union(label2)) - len(label1.intersection(label2)))/float(len(label1.union(label2)))


def masi_distance(label1, label2):
    """Distance metric that takes into account partial agreement when multiple
    labels are assigned.

--
    return 1 - (len_intersection / float(len_union)) * m


def interval_distance(label1,label2):
    """Krippendorff'1 interval distance metric

    >>> from nltk.metrics import interval_distance
--
        print("non-numeric labels not supported with interval distance")


def presence(label):
    """Higher-order function to test presence of a given label

    """
    return lambda x,y: 1.0*((label in x) == (label in y))


def fractional_presence(label):
    return lambda x,y:abs((float(1.0/len(x)) - float(1.0/len(y))))*(label in x and label in y) or 0.0*(label not in x and label not in y) or abs((float(1.0/len(x))))*(label in x and label not in y) or ((float(1.0/len(y))))*(label not in x and label in y)


def custom_distance(file):
    data = {}
    for l in open(file):
        labelA, labelB, dist = l.strip().split("\t")
--
    return lambda x,y:data[frozenset([x,y])]


def demo():
    edit_distance_examples = [("rain", "shine"), ("abcdef", "acbdef"), ("language", "lnaguaeg"), ("language", "lnaugage"), ("language", "lngauage")]
    for s1, s2 in edit_distance_examples:
        print("Edit distance between '%s' and '%s':" % (s1, s2), edit_distance(s1, s2))
    for s1, s2 in edit_distance_examples:
--
from __future__ import unicode_literals

import re
from collections import defaultdict

from nltk.ccg.api import PrimitiveCategory, Direction, CCGVar, FunctionalCategory
from nltk.compat import python_2_unicode_compatible
--
# Separates the next application operator from the remainder
reApp = re.compile(r'''([\\/])([.,]?)([.,]?)(.*)''')

# Parses the definition of the category of either a word or a family
reLex = re.compile(r'''([A-Za-z_]+)\s*(::|[-=]+>)\s*(.+)''')

# Strips comments from a line
--
    families - Families of categories
    entries - A mapping of words to possible categories
    '''
    def __init__(self,start,primitives,families,entries):
        self._start = PrimitiveCategory(start)
        self._primitives = primitives
        self._families = families
        self._entries = entries

    # Returns all the possible categories for a word
    def categories(self,word):
        return self._entries[word]

    # Returns the target category for the parser
    def start(self):
        return self._start

    # String representation of the lexicon
    # Used for debugging
    def __str__(self):
        st = ""
        first = True
        for ident in self._entries:
--

# Separates the contents matching the first set of brackets
# from the rest of the input.
def matchBrackets(string):
    rest = string[1:]
    inside = "("

--

# Separates the string for the next portion of the category
# from the rest of the string
def nextCategory(string):
    if string.startswith('('):
        return matchBrackets(string)
    return reNextPrim.match(string).groups()

# Parses an application operator
def parseApplication(app):
    return Direction(app[0],app[1:])

# Parses the subscripts for a primitive category
def parseSubscripts(subscr):
    if subscr:
        return subscr[1:-1].split(',')
    return []

# Parse a primitive category
def parsePrimitiveCategory(chunks,primitives,families,var):
    # If the primitive is the special category 'var',
    # replace it with the correct CCGVar
    if chunks[0] == "var":
--
    raise AssertionError('String \'' + catstr + '\' is neither a family nor primitive category.')

# parseCategory drops the 'var' from the tuple
def parseCategory(line,primitives,families):
    return augParseCategory(line,primitives,families)[0]

# Parses a string representing a category, and returns
# a tuple with (possibly) the CCG variable for the category
def augParseCategory(line,primitives,families,var = None):
    (str,rest) = nextCategory(line)

    if str.startswith('('):
--
    return (res,var)

# Takes an input string, and converts it into a lexicon for CCGs.
def parseLexicon(lex_str):
    primitives = []
    families = {}
    entries = defaultdict(list)
    for line in lex_str.splitlines():
        # Strip comments and leading/trailing whitespace.
        line = reComm.match(line).groups()[0].strip()
--
            # ie, :- S, N, NP, VP
            primitives = primitives + [ prim.strip() for prim in line[2:].strip().split(',') ]
        else:
            # Either a family definition, or a word definition
            (ident, sep, catstr) = reLex.match(line).groups()
            (cat,var) = augParseCategory(catstr,primitives,families)
            if sep == '::':
                # Family definition
                # ie, Det :: NP/N
                families[ident] = (cat,var)
            else:
                # Word definition
                # ie, which => (N\N)/(S/NP)
                entries[ident].append(cat)
    return CCGLexicon(primitives[0],primitives,families,entries)
--
class UndirectedBinaryCombinator(object):
    """
    Abstract class for representing a binary combinator.
    Merely defines functions for checking if the function and argument
    are able to be combined, and what the resulting category is.

    Note that as no assumptions are made as to direction, the unrestricted
--
    of the combinators; these restrictions must be added in the rule
    class.
    """
    def can_combine(self, function, argument):
        raise NotImplementedError()

    def combine (self,function,argument):
        raise NotImplementedError()

class DirectedBinaryCombinator(object):
--
    the function, and which the argument.
    It then decides whether or not they can be combined.
    """
    def can_combine(self, left, right):
        raise NotImplementedError()

    def combine(self, left, right):
        raise NotImplementedError()

@python_2_unicode_compatible
--
    Takes an undirected combinator, and a predicate which adds constraints
    restricting the cases in which it may apply.
    '''
    def __init__(self, combinator, predicate, suffix=''):
        self._combinator = combinator
        self._predicate = predicate
        self._suffix = suffix

    def can_combine(self, left, right):
        return (self._combinator.can_combine(left,right) and
                  self._predicate(left,right))

    def combine(self, left, right):
        for cat in self._combinator.combine(left,right):
            yield cat

    def __str__(self):
        return ">%s%s" % (self._combinator, self._suffix)

@python_2_unicode_compatible
--
    '''
    The backward equivalent of the ForwardCombinator class.
    '''
    def __init__(self, combinator, predicate, suffix=''):
        self._combinator = combinator
        self._predicate = predicate
        self._suffix = suffix

    def can_combine(self, left, right):
        return (self._combinator.can_combine(right, left) and
                  self._predicate(left,right))
    def combine(self, left, right):
        for cat in self._combinator.combine(right, left):
            yield cat

    def __str__(self):
        return "<%s%s" % (self._combinator, self._suffix)

@python_2_unicode_compatible
--
    And the corresponding backwards application rule
    """

    def can_combine(self, function, argument):
        if not function.is_function():
            return False

        return not function.arg().can_unify(argument) is None

    def combine(self,function,argument):
        if not function.is_function():
            return

--

        yield function.res().substitute(subs)

    def __str__(self):
        return ''


# Predicates for function application.

# Ensures the left functor takes an argument on the right
def forwardOnly(left,right):
    return left.dir().is_forward()

# Ensures the right functor takes an argument on the left
def backwardOnly(left,right):
    return right.dir().is_backward()

# Application combinator instances
--
    X/Y Y/Z -> X/Z (B>)
    And the corresponding backwards and crossed variations.
    """
    def can_combine(self, function, argument):
        # Can only combine two functions, and both functions must
        # allow composition.
        if not (function.is_function() and argument.is_function()):
--
            return not function.arg().can_unify(argument.res()) is None
        return False

    def combine(self, function, argument):
        if not (function.is_function() and argument.is_function()):
            return
        if function.dir().can_compose() and argument.dir().can_compose():
--
                yield FunctionalCategory(function.res().substitute(subs),
                            argument.arg().substitute(subs),argument.dir())

    def __str__(self):
        return 'B'

# Predicates for restricting application of straight composition.
def bothForward(left,right):
    return left.dir().is_forward() and right.dir().is_forward()

def bothBackward(left,right):
    return left.dir().is_backward() and right.dir().is_backward()

# Predicates for crossed composition

def crossedDirs(left,right):
    return left.dir().is_forward() and right.dir().is_backward()

def backwardBxConstraint(left,right):
    # The functors must be crossed inwards
    if not crossedDirs(left, right):
        return False
--
    Y/Z (X\Y)/Z -> X/Z (<Sx)
    And other variations.
    """
    def can_combine(self, function, argument):
        if function.is_primitive() or argument.is_primitive():
            return False

--
            return False
        return (function.res().arg() == argument.res()) and (function.arg() == argument.arg())

    def combine(self,function,argument):
        if self.can_combine(function,argument):
            yield FunctionalCategory(function.res().res(),argument.arg(),argument.dir())

    def __str__(self):
        return 'S'

# Predicate for forward substitution
def forwardSConstraint(left, right):
    if not bothForward(left, right):
        return False
    return left.res().dir().is_forward() and left.arg().is_primitive()

# Predicate for backward crossed substitution
def backwardSxConstraint(left,right):
    if not left.dir().can_cross() and right.dir().can_cross():
        return False
    if not bothForward(left, right):
--

# Retrieves the left-most functional category.
# ie, (N\N)/(S/NP) => N\N
def innermostFunction(categ):
    while categ.res().is_function():
        categ = categ.res()
    return categ
--
    '''
    Undirected combinator for type raising.
    '''
    def can_combine(self,function,arg):
        # The argument must be a function.
        # The restriction that arg.res() must be a function
        # merely reduces redundant type-raising; if arg.res() is
--

        arg = innermostFunction(arg)

        # left, arg_categ are undefined!
        subs = left.can_unify(arg_categ.arg())
        if subs is not None:
            return True
        return False

    def combine(self,function,arg):
        if not (function.is_primitive() and
                arg.is_function() and arg.res().is_function()):
            return
--
                    FunctionalCategory(xcat,function,arg.dir()),
                    -(arg.dir()))

    def __str__(self):
        return 'T'

# Predicates for type-raising
--
# the primary functor.
# The restriction that the variable must be primitive is not
# common to all versions of CCGs; some authors have other restrictions.
def forwardTConstraint(left,right):
    arg = innermostFunction(right)
    return arg.dir().is_backward() and arg.res().is_primitive()

def backwardTConstraint(left,right):
    arg = innermostFunction(left)
    return arg.dir().is_forward() and arg.res().is_primitive()

--

In order to construct a parser, you also need a rule set.
The standard English rules are provided in chart as
``chart.DefaultRuleSet``.

The parser can then be constructed by calling, for example:
``parser = chart.CCGChartParser(<lexicon>, <ruleset>)``
--
Parsing is then performed by running
``parser.nbest_parse(<sentence>.split())``.

While this returns a list of trees, the default representation
of the produced trees is not very enlightening, particularly
given that it uses the same tree class as the CFG parsers.
It is probably better to call:
--
# A number of the properties of the EdgeI interface don't
# transfer well to CCGs, however.
class CCGEdge(EdgeI):
    def __init__(self, span, categ, rule):
        self._span = span
        self._categ = categ
        self._rule = rule
        self._comparison_key = (span, categ, rule)

    # Accessors
    def lhs(self): return self._categ
    def span(self): return self._span
    def start(self): return self._span[0]
    def end(self): return self._span[1]
    def length(self): return self._span[1] - self.span[0]
    def rhs(self): return ()
    def dot(self): return 0
    def is_complete(self): return True
    def is_incomplete(self): return False
    def nextsym(self): return None

    def categ(self): return self._categ
    def rule(self): return self._rule

class CCGLeafEdge(EdgeI):
    '''
    Class representing leaf edges in a CCG derivation.
    '''
    def __init__(self, pos, categ, leaf):
        self._pos = pos
        self._categ = categ
        self._leaf = leaf
        self._comparison_key = (pos, categ, leaf)

    # Accessors
    def lhs(self): return self._categ
    def span(self): return (self._pos, self._pos+1)
    def start(self): return self._pos
    def end(self): return self._pos + 1
    def length(self): return 1
    def rhs(self): return self._leaf
    def dot(self): return 0
    def is_complete(self): return True
    def is_incomplete(self): return False
    def nextsym(self): return None

    def categ(self): return self._categ
    def leaf(self): return self._leaf

@python_2_unicode_compatible
class BinaryCombinatorRule(AbstractChartRule):
--
    Takes the directed combinator to apply.
    '''
    NUMEDGES = 2
    def __init__(self,combinator):
        self._combinator = combinator

    # Apply a combinator
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        # The left & right edges must be touching.
        if not (left_edge.end() == right_edge.start()):
            return
--
                    yield new_edge

    # The representation of the combinator (for printing derivations)
    def __str__(self):
        return "%s" % self._combinator

# Type-raising must be handled slightly differently to the other rules, as the
--
    '''
    NUMEDGES = 2

    def __init__(self):
       self._combinator = ForwardT
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        if not (left_edge.end() == right_edge.start()):
            return

--
            if chart.insert(new_edge,(left_edge,)):
                yield new_edge

    def __str__(self):
        return "%s" % self._combinator

@python_2_unicode_compatible
--
    '''
    NUMEDGES = 2

    def __init__(self):
       self._combinator = BackwardT
    def apply_iter(self, chart, grammar, left_edge, right_edge):
        if not (left_edge.end() == right_edge.start()):
            return

--
            if chart.insert(new_edge,(right_edge,)):
                yield new_edge

    def __str__(self):
        return "%s" % self._combinator


--
TypeRaiseRuleSet = [ForwardTypeRaiseRule(), BackwardTypeRaiseRule()]

# The standard English rule set.
DefaultRuleSet = ApplicationRuleSet + CompositionRuleSet + \
                    SubstitutionRuleSet + TypeRaiseRuleSet

class CCGChartParser(ParserI):
--
    Chart parser for CCGs.
    Based largely on the ChartParser class from NLTK.
    '''
    def __init__(self, lexicon, rules, trace=0):
        self._lexicon = lexicon
        self._rules = rules
        self._trace = trace

    def lexicon(self):
        return self._lexicon

   # Implements the CYK algorithm
    def nbest_parse(self, tokens, n=None):
        tokens = list(tokens)
        chart = CCGChart(list(tokens))
        lex = self._lexicon
--
        return chart.parses(lex.start())[:n]

class CCGChart(Chart):
    def __init__(self, tokens):
        Chart.__init__(self, tokens)

    # Constructs the trees for a given parse. Unfortnunately, the parse trees need to be
    # constructed slightly differently to those in the default Chart class, so it has to
    # be reimplemented
    def _trees(self, edge, complete, memo, tree_class):
        if edge in memo:
            return memo[edge]

--
#--------
# Displaying derivations
#--------
def printCCGDerivation(tree):
    # Get the leaves and initial categories
    leafcats = tree.pos()
    leafstr = ''
--
    printCCGTree(0,tree)

# Prints the sequence of derivation steps.
def printCCGTree(lwidth,tree):
    rwidth = lwidth

    # Is a leaf (word).
--
    bacon => N
    ''')

def demo():
    parser = CCGChartParser(lex, DefaultRuleSet)
    for parse in parser.nbest_parse("I might cook and eat the bacon".split(), 3):
        printCCGDerivation(parse)

--
    '''

    # Returns true if the category is primitive
    def is_primitive(self):
        raise NotImplementedError()

    # Returns true if the category is a function application
    def is_function(self):
        raise NotImplementedError()

    # Returns true if the category is a variable
    def is_var(self):
        raise NotImplementedError()

    # Takes a set of (var, category) substitutions, and replaces every
    # occurrence of the variable with the corresponding category
    def substitute(self,substitutions):
        raise NotImplementedError()

    # Determines whether two categories can be unified.
    #  - Returns None if they cannot be unified
    #  - Returns a list of necessary substitutions if they can.'''
    def can_unify(self,other):
        raise NotImplementedError()

    # Utility functions: comparison, strings and hashing.

    def __str__(self):
        raise NotImplementedError()

    def __eq__(self, other):
        return (self.__class__ is other.__class__ and
                self._comparison_key == other._comparison_key)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, AbstractCCGCategory):
            raise_unorderable_types("<", self, other)
        if self.__class__ is other.__class__:
--
        else:
            return self.__class__.__name__ < other.__class__.__name__

    def __hash__(self):
        try:
            return self._hash
        except AttributeError:
--
    '''
    _maxID = 0

    def __init__(self, prim_only=False):
        """Initialize a variable (selects a new identifier)

        :param prim_only: a boolean that determines whether the variable is restricted to primitives
--
        self._comparison_key = self._id

    @classmethod
    def new_id(cls):
        """A class method allowing generation of unique variable identifiers."""
        cls._maxID = cls._maxID + 1
        return cls._maxID - 1

    def is_primitive(self):
        return False

    def is_function(self):
        return False

    def is_var(self):
        return True

    def substitute(self, substitutions):
        """If there is a substitution corresponding to this variable,
        return the substituted category.
        """
--
                 return cat
        return self

    def can_unify(self, other):
        """ If the variable can be replaced with other
        a substitution is returned.
        """
--
            return [(self,other)]
        return None

    def id(self):
        return self._id

    def __str__(self):
        return "_var" + str(self._id)

@total_ordering
--
    Also contains maintains information as to which combinators
    may be used with the category.
    '''
    def __init__(self,dir,restrictions):
        self._dir = dir
        self._restrs = restrictions
        self._comparison_key = (dir, tuple(restrictions))

    # Testing the application direction
    def is_forward(self):
        return self._dir == '/'
    def is_backward(self):
        return self._dir == '\\'

    def dir(self):
        return self._dir

    def restrs(self):
        """A list of restrictions on the combinators.
        '.' denotes that permuting operations are disallowed
        ',' denotes that function composition is disallowed
--
        """
        return self._restrs

    def is_variable(self):
        return self._restrs == '_'

    # Unification and substitution of variable directions.
    # Used only if type-raising is implemented as a unary rule, as it
    # must inherit restrictions from the argument category.
    def can_unify(self,other):
        if other.is_variable():
            return [('_',self.restrs())]
        elif self.is_variable():
--
                return []
        return None

    def substitute(self,subs):
        if not self.is_variable():
            return self

--
        return self

    # Testing permitted combinators
    def can_compose(self):
        return not ',' in self._restrs

    def can_cross(self):
        return not '.' in self._restrs

    def __eq__(self, other):
        return (self.__class__ is other.__class__ and
                self._comparison_key == other._comparison_key)

    def __ne__(self, other):
        return not self == other

    def __lt__(self, other):
        if not isinstance(other, Direction):
            raise_unorderable_types("<", self, other)
        if self.__class__ is other.__class__:
--
        else:
            return self.__class__.__name__ < other.__class__.__name__

    def __hash__(self):
        try:
            return self._hash
        except AttributeError:
            self._hash = hash(self._comparison_key)
            return self._hash

    def __str__(self):
        r_str = ""
        for r in self._restrs:
            r_str = r_str + "%s" % r
        return "%s%s" % (self._dir, r_str)

    # The negation operator reverses the direction of the application
    def __neg__(self):
        if self._dir == '/':
            return Direction('\\',self._restrs)
        else:
--
    Takes a string representation of the category, and a
    list of strings specifying the morphological subcategories.
    '''
    def __init__(self, categ, restrictions=[]):
        self._categ = categ
        self._restrs = restrictions
        self._comparison_key = (categ, tuple(restrictions))

    def is_primitive(self):
        return True

    def is_function(self):
        return False

    def is_var(self):
        return False

    def restrs(self):
        return self._restrs

    def categ(self):
        return self._categ

    # Substitution does nothing to a primitive category
    def substitute(self,subs):
        return self

    # A primitive can be unified with a class of the same
    # base category, given that the other category shares all
    # of its subclasses, or with a variable.
    def can_unify(self,other):
        if not other.is_primitive():
            return None
        if other.is_var():
--
            return []
        return None

    def __str__(self):
        if self._restrs == []:
            return "%s" % self._categ
        restrictions = "[%s]" % ",".join(unicode_repr(r) for r in self._restrs)
--
    Consists of argument and result categories, together with
    an application direction.
    '''
    def __init__(self, res, arg, dir):
        self._res = res
        self._arg = arg
        self._dir = dir
        self._comparison_key = (arg, dir, res)

    def is_primitive(self):
        return False

    def is_function(self):
        return True

    def is_var(self):
        return False

    # Substitution returns the category consisting of the
    # substitution applied to each of its constituents.
    def substitute(self,subs):
        sub_res = self._res.substitute(subs)
        sub_dir = self._dir.substitute(subs)
        sub_arg = self._arg.substitute(subs)
--

    # A function can unify with another function, so long as its
    # constituents can unify, or with an unrestricted variable.
    def can_unify(self,other):
        if other.is_var():
            return [(other,self)]
        if other.is_function():
--
        return None

    # Constituent accessors
    def arg(self):
        return self._arg

    def res(self):
        return self._res

    def dir(self):
        return self._dir

    def __str__(self):
        return "(%s%s%s)" % (self._res, self._dir, self._arg)


--


# String functions
def decode_string(v, encoding="utf-8"):
    """ Returns the given value as a Unicode string (if possible).
    """
    if isinstance(encoding, basestring):
--
    return unicode(v)


def encode_string(v, encoding="utf-8"):
    """ Returns the given value as a Python byte string (if possible).
    """
    if isinstance(encoding, basestring):
--
encode_utf8 = encode_string


def isnumeric(strg):
    try:
        float(strg)
    except ValueError:
--

class lazydict(dict):

    def load(self):
        # Must be overridden in a subclass.
        # Must load data with dict.__setitem__(self, k, v) instead of lazydict[k] = v.
        pass

    def _lazy(self, method, *args):
        """ If the dictionary is empty, calls lazydict.load().
            Replaces lazydict.method() with dict.method() and calls it.
        """
--
            setattr(self, method, types.MethodType(getattr(dict, method), self))
        return getattr(dict, method)(self, *args)

    def __repr__(self):
        return self._lazy("__repr__")
    def __len__(self):
        return self._lazy("__len__")
    def __iter__(self):
        return self._lazy("__iter__")
    def __contains__(self, *args):
        return self._lazy("__contains__", *args)
    def __getitem__(self, *args):
        return self._lazy("__getitem__", *args)
    def __setitem__(self, *args):
        return self._lazy("__setitem__", *args)
    def setdefault(self, *args):
        return self._lazy("setdefault", *args)
    def get(self, *args, **kwargs):
        return self._lazy("get", *args)
    def items(self):
        return self._lazy("items")
    def keys(self):
        return self._lazy("keys")
    def values(self):
        return self._lazy("values")
    def update(self, *args):
        return self._lazy("update", *args)
    def pop(self, *args):
        return self._lazy("pop", *args)
    def popitem(self, *args):
        return self._lazy("popitem", *args)

class lazylist(list):

    def load(self):
        # Must be overridden in a subclass.
        # Must load data with list.append(self, v) instead of lazylist.append(v).
        pass

    def _lazy(self, method, *args):
        """ If the list is empty, calls lazylist.load().
            Replaces lazylist.method() with list.method() and calls it.
        """
--
            setattr(self, method, types.MethodType(getattr(list, method), self))
        return getattr(list, method)(self, *args)

    def __repr__(self):
        return self._lazy("__repr__")
    def __len__(self):
        return self._lazy("__len__")
    def __iter__(self):
        return self._lazy("__iter__")
    def __contains__(self, *args):
        return self._lazy("__contains__", *args)
    def insert(self, *args):
        return self._lazy("insert", *args)
    def append(self, *args):
        return self._lazy("append", *args)
    def extend(self, *args):
        return self._lazy("extend", *args)
    def remove(self, *args):
        return self._lazy("remove", *args)
    def pop(self, *args):
        return self._lazy("pop", *args)

#--- UNIVERSAL TAGSET ------------------------------------------------------------------------------
# The default part-of-speech tagset used in Pattern is Penn Treebank II.
# However, not all languages are well-suited to Penn Treebank (which was developed for English).
# As more languages are implemented, this is becoming more problematic.
#
--
NOUN, VERB, ADJ, ADV, PRON, DET, PREP, ADP, NUM, CONJ, INTJ, PRT, PUNC, X = \
    "NN", "VB", "JJ", "RB", "PR", "DT", "PP", "PP", "NO", "CJ", "UH", "PT", ".", "X"

def penntreebank2universal(token, tag):
    """ Returns a (token, tag)-tuple with a simplified universal part-of-speech tag.
    """
    if tag.startswith(("NNP-", "NNPS-")):
--

# Handle common abbreviations.
ABBREVIATIONS = abbreviations = set((
    "a.", "adj.", "adv.", "al.", "a.m.", "c.", "cf.", "comp.", "conf.", "def.",
    "ed.", "e.g.", "esp.", "etc.", "ex.", "f.", "fig.", "gen.", "id.", "i.e.",
    "int.", "l.", "m.", "Med.", "Mil.", "Mr.", "n.", "n.q.", "orig.", "pl.",
    "pred.", "pres.", "p.m.", "ref.", "v.", "vs.", "w/"
--
# Handle paragraph line breaks (\n\n marks end of sentence).
EOS = "END-OF-SENTENCE"

def find_tokens(string, punctuation=PUNCTUATION, abbreviations=ABBREVIATIONS, replace=replacements, linebreak=r"\n{2,}"):
    """ Returns a list of sentences. Each sentence is a space-separated string of tokens (words).
        Handles common cases of abbreviations (e.g., etc., ...).
        Punctuation marks are split from other words. Periods (or ?!) mark the end of a sentence.
--
# Named entity rules are used to discover proper nouns (NNP's).


def _read(path, encoding="utf-8", comment=";;;"):
    """ Returns an iterator over the lines in the file at the given path,
        stripping comments and decoding each line to Unicode.
    """
--

class Lexicon(lazydict):

    def __init__(self, path="", morphology=None, context=None, entities=None, NNP="NNP", language=None):
        """ A dictionary of words and their part-of-speech tags.
            For unknown words, rules for word morphology, context and named entities can be used.
        """
--
        self.context    = Context(self, path=context)
        self.entities   = Entities(self, path=entities, tag=NNP)

    def load(self):
        # Arnold NNP x
        dict.update(self, (x.split(" ")[:2] for x in _read(self._path) if x.strip()))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language


--

class Rules:

    def __init__(self, lexicon={}, cmd={}):
        self.lexicon, self.cmd = lexicon, cmd

    def apply(self, x):
        """ Applies the rule to the given token or list of tokens.
        """
        return x

class Morphology(lazylist, Rules):

    def __init__(self, lexicon={}, path=""):
        """ A list of rules based on word morphology (prefix, suffix).
        """
        cmd = ("char", # Word contains x.
--
        self._path = path

    @property
    def path(self):
        return self._path

    def load(self):
        # ["NN", "s", "fhassuf", "1", "NNS", "x"]
        list.extend(self, (x.split() for x in _read(self._path)))

    def apply(self, token, previous=(None, None), next=(None, None)):
        """ Applies lexical rules to the given token, which is a [word, tag] list.
        """
        w = token[0]
--
                token[1] = pos
        return token

    def insert(self, i, tag, affix, cmd="hassuf", tagged=None):
        """ Inserts a new rule that assigns the given tag to words with the given affix,
            e.g., Morphology.append("RB", "-ly").
        """
--
            r = [affix, cmd.lstrip("f"), tag, "x"]
        lazylist.insert(self, i, r)

    def append(self, *args, **kwargs):
        self.insert(len(self)-1, *args, **kwargs)

    def extend(self, rules=[]):
        for r in rules:
            self.append(*r)

--

class Context(lazylist, Rules):

    def __init__(self, lexicon={}, path=""):
        """ A list of rules based on context (preceding and following words).
        """
        cmd = ("prevtag", # Preceding word is tagged x.
--
        self._path = path

    @property
    def path(self):
        return self._path

    def load(self):
        # ["VBD", "VB", "PREVTAG", "TO"]
        list.extend(self, (x.split() for x in _read(self._path)))

    def apply(self, tokens):
        """ Applies contextual rules to the given list of tokens,
            where each token is a [word, tag] list.
        """
--
                    t[i] = [t[i][0], r[1]]
        return t[len(o):-len(o)]

    def insert(self, i, tag1, tag2, cmd="prevtag", x=None, y=None):
        """ Inserts a new rule that updates words with tag1 to tag2,
            given constraints x and y, e.g., Context.append("TO < NN", "VB")
        """
--
            x, tag1 = tag1.split(" > "); cmd="nexttag"
        lazylist.insert(self, i, [tag1, tag2, cmd, x or "", y or ""])

    def append(self, *args, **kwargs):
        self.insert(len(self)-1, *args, **kwargs)

    def extend(self, rules=[]):
        for r in rules:
            self.append(*r)
#--- NAMED ENTITY RECOGNIZER -----------------------------------------------------------------------
--

class Entities(lazydict, Rules):

    def __init__(self, lexicon={}, path="", tag="NNP"):
        """ A dictionary of named entities and their labels.
            For domain names and e-mail adresses, regular expressions are used.
        """
--
        self.tag   = tag

    @property
    def path(self):
        return self._path

    def load(self):
        # ["Alexander", "the", "Great", "PERS"]
        # {"alexander": [["alexander", "the", "great", "pers"], ...]}
        for x in _read(self.path):
            x = [x.lower() for x in x.split()]
            dict.setdefault(self, x[0], []).append(x)

    def apply(self, tokens):
        """ Applies the named entity recognizer to the given list of tokens,
            where each token is a [word, tag] list.
        """
--
            i += 1
        return tokens

    def append(self, entity, name="pers"):
        """ Appends a named entity to the lexicon,
            e.g., Entities.append("Hooloovoo", "PERS")
        """
        e = [s.lower() for s in entity.split(" ") + [name]]
        self.setdefault(e[0], []).append(e)

    def extend(self, entities):
        for entity, name in entities:
            self.append(entity, name)

--

RE_SYNSET = re.compile(r"^[acdnrv][-_][0-9]+$")

def avg(list):
    return sum(list) / float(len(list) or 1)

class Score(tuple):

    def __new__(self, polarity, subjectivity, assessments=[]):
        """ A (polarity, subjectivity)-tuple with an assessments property.
        """
        return tuple.__new__(self, [polarity, subjectivity])

    def __init__(self, polarity, subjectivity, assessments=[]):
        self.assessments = assessments

class Sentiment(lazydict):

    def __init__(self, path="", language=None, synset=None, confidence=None, **kwargs):
        """ A dictionary of words (adjectives) and polarity scores (positive/negative).
            The value for each word is a dictionary of part-of-speech tags.
            The value for each word POS-tag is a tuple with values for
--
        self.modifier    = kwargs.get("modifier" , lambda w: w.endswith("ly"))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language

    @property
    def confidence(self):
        return self._confidence

    def load(self, path=None):
        """ Loads the XML-file (with sentiment annotations) from the given path.
            By default, Sentiment.path is lazily loaded.
        """
        # <word form="great" wordnet_id="a-01123879" pos="JJ" polarity="1.0" subjectivity="1.0" intensity="1.0" />
        # <word form="damnmit" polarity="-0.75" subjectivity="1.0" label="profanity" />
--
                )
                psi = (float(p), float(s), float(i))
                if w:
                    words.setdefault(w, {}).setdefault(pos, []).append(psi)
                if w and label:
                    labels[w] = label
                if synset:
                    synsets.setdefault(synset, []).append(psi)
        self._language = xml.attrib.get("language", self._language)
        # Average scores of all word senses per part-of-speech tag.
        for w in words:
--
        dict.update(self.labeler, labels)
        dict.update(self._synsets, synsets)

    def synset(self, id, pos=ADJECTIVE):
        """ Returns a (polarity, subjectivity)-tuple for the given synset id.
            For example, the adjective "horrible" has id 193480 in WordNet:
            Sentiment.synset(193480, pos="JJ") => (-0.6, 1.0, 1.0).
--
            self.load()
        return tuple(self._synsets.get(id, (0.0, 0.0))[:2])

    def __call__(self, s, negation=True, **kwargs):
        """ Returns a (polarity, subjectivity)-tuple for the given sentence,
            with polarity between -1.0 and 1.0 and subjectivity between 0.0 and 1.0.
            The sentence can be a string, Synset, Text, Sentence, Chunk, Word, Document, Vector.
            An optional weight parameter can be given,
            as a function that takes a list of words and returns a weight.
        """
        def avg(assessments, weighted=lambda w: 1):
            s, n = 0, 0
            for words, score in assessments:
                w = weighted(words)
--
        # to stop assessments() from scanning for preceding negation & modifiers.
        elif hasattr(s, "terms"):
            a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation)
            kwargs.setdefault("weight", lambda w: s.terms[w[0]])
        # A dict of (word, weight)-items.
        elif isinstance(s, dict):
            a = self.assessments(chain(*(((w, None), (None, None)) for w in s)), negation)
            kwargs.setdefault("weight", lambda w: s[w[0]])
        # A list of words.
        elif isinstance(s, list):
            a = self.assessments(((w, None) for w in s), negation)
--
                 subjectivity = avg([(w, s) for w, p, s, x in a], weight),
                  assessments = a)

    def assessments(self, words=[], negation=True):
        """ Returns a list of (chunk, polarity, subjectivity, label)-tuples for the given list of words:
            where chunk is a list of successive words: a known word optionally
            preceded by a modifier ("very good") or a negation ("not good").
--
            a[i] = (w, p * -0.5 if n < 0 else p, s, x)
        return a

    def annotate(self, word, pos=None, polarity=0.0, subjectivity=0.0, intensity=1.0, label=None):
        """ Annotates the given word with polarity, subjectivity and intensity scores,
            and optionally a semantic label (e.g., MOOD for emoticons, IRONY for "(!)").
        """
        w = self.setdefault(word, {})
        w[pos] = w[None] = (polarity, subjectivity, intensity)
        if label:
            self.labeler[word] = label
--
# Unknown words are recognized as numbers if they contain only digits and -,.:/%$
CD = re.compile(r"^[0-9\-\,\.\:\/\%\$]+$")

def _suffix_rules(token, **kwargs):
    """ Default morphological tagging rules for English, based on word suffixes.
    """
    word, pos = token
    if word.endswith("ing"):
--
        pos = "VBP"
    return [word, pos]

def find_tags(tokens, lexicon={}, default=("NN", "NNP", "CD"), language="en", map=None, **kwargs):
    """ Returns a list of [token, tag]-items for the given list of tokens:
        ["The", "cat", "purs"] => [["The", "DT"], ["cat", "NN"], ["purs", "VB"]]
        Words are tagged using the given lexicon of (word, tag)-items.
        Unknown words are tagged NN by default.
        Unknown words that start with a capital letter are tagged NNP (unless language="de").
        Unknown words that consist only of digits and punctuation marks are tagged CD.
        Unknown words are then improved with morphological rules.
--
            and token[0].isupper() \
            and token[0].isalpha() \
            and language != "de":
                tagged[i] = [token, default[1]] # NNP
            elif CD.match(token) is not None:
                tagged[i] = [token, default[2]] # CD
            else:
                tagged[i] = [token, default[0]] # NN
                tagged[i] = f(tagged[i],
                    previous = i > 0 and tagged[i-1] or (None, None),
                        next = i < len(tagged)-1 and tagged[i+1] or (None, None))
--
        tagged = lexicon.context.apply(tagged)
        tagged = lexicon.entities.apply(tagged)
    if map is not None:
        tagged = [list(map(token, tag)) or [token, default[0]] for token, tag in tagged]
    return tagged

#--- PHRASE CHUNKER --------------------------------------------------------------------------------
--
CHUNKS[0].insert(1, CHUNKS[0].pop(3))
CHUNKS[1].insert(1, CHUNKS[1].pop(3))

def find_chunks(tagged, language="en"):
    """ The input is a list of [token, tag]-items.
        The output is a list of [token, tag, chunk]-items:
        The/DT nice/JJ fish/NN is/VBZ dead/JJ ./. =>
--
                chunked[i+1][2] = "B-NP"
    return chunked

def find_prepositions(chunked):
    """ The input is a list of [token, tag, chunk]-items.
        The output is a list of [token, tag, chunk, preposition]-items.
        PP-chunks followed by NP-chunks make up a PNP-chunk.
--
# - the phrases are labeled: SBJ (subject), OBJ (object), LOC (location), ...
# - the phrase start is marked: B (begin), I (inside), O (outside),
# - the past tense "sat" is lemmatized => "sit".
# By default, the English parser uses the Penn Treebank II tagset:
# http://www.clips.ua.ac.be/pages/penn-treebank-tagset
PTB = PENN = "penn"

class Parser:

    def __init__(self, lexicon={}, default=("NN", "NNP", "CD"), language=None):
        """ A simple shallow parser using a Brill-based part-of-speech tagger.
            The given lexicon is a dictionary of known words and their part-of-speech tag.
            The given default tags are used for unknown words.
            Unknown words that start with a capital letter are tagged NNP (except for German).
            Unknown words that contain only digits and punctuation are tagged CD.
            The given language can be used to discern between
            Germanic and Romance languages for phrase chunking.
        """
        self.lexicon  = lexicon
        self.default  = default
        self.language = language

    def find_tokens(self, string, **kwargs):
        """ Returns a list of sentences from the given string.
            Punctuation marks are separated from each word by a space.
        """
--
                    replace = kwargs.get(      "replace", replacements),
                  linebreak = r"\n{2,}")

    def find_tags(self, tokens, **kwargs):
        """ Annotates the given list of tokens with part-of-speech tags.
            Returns a list of tokens, where each token is now a [word, tag]-list.
        """
--
        return find_tags(tokens,
                   language = kwargs.get("language", self.language),
                    lexicon = kwargs.get( "lexicon", self.lexicon),
                    default = kwargs.get( "default", self.default),
                        map = kwargs.get(     "map", None))

    def find_chunks(self, tokens, **kwargs):
        """ Annotates the given list of tokens with chunk tags.
            Several tags can be added, for example chunk + preposition tags.
        """
--
               find_chunks(tokens,
                   language = kwargs.get("language", self.language)))

    def find_prepositions(self, tokens, **kwargs):
        """ Annotates the given list of tokens with prepositional noun phrase tags.
        """
        return find_prepositions(tokens) # See also Parser.find_chunks().

    def find_labels(self, tokens, **kwargs):
        """ Annotates the given list of tokens with verb/predicate tags.
        """
        return find_relations(tokens)

    def find_lemmata(self, tokens, **kwargs):
        """ Annotates the given list of tokens with word lemmata.
        """
        return [token + [token[0].lower()] for token in tokens]

    def parse(self, s, tokenize=True, tags=True, chunks=True, relations=False, lemmata=False, encoding="utf-8", **kwargs):
        """ Takes a string (sentences) and returns a tagged Unicode string (TaggedString).
            Sentences in the output are separated by newlines.
            With tokenize=True, punctuation is split from words and sentences are separated by \n.
--

class TaggedString(unicode):

    def __new__(self, string, tags=["word"], language=None):
        """ Unicode string with tags and language attributes.
            For example: TaggedString("cat/NN/NP", tags=["word", "pos", "chunk"]).
        """
--
        s.language = language
        return s

    def split(self, sep=TOKENS):
        """ Returns a list of sentences, where each sentence is a list of tokens,
            where each token is a list of word + tags.
        """
--

class Spelling(lazydict):

    ALPHA = "abcdefghijklmnopqrstuvwxyz"

    def __init__(self, path=""):
        self._path = path

    def load(self):
        for x in _read(self._path):
            x = x.split()
            dict.__setitem__(self, x[0], int(x[1]))

    @property
    def path(self):
        return self._path

    @property
    def language(self):
        return self._language

    @classmethod
    def train(self, s, path="spelling.txt"):
        """ Counts the words in the given string and saves the probabilities at the given path.
            This can be used to generate a new model for the Spelling() constructor.
        """
--
        f.write(model)
        f.close()

    def _edit1(self, w):
        """ Returns a set of words with edit distance 1 from the given word.
        """
        # Of all spelling errors, 80% is covered by edit distance 1.
--
        )
        return set(delete + transpose + replace + insert)

    def _edit2(self, w):
        """ Returns a set of words with edit distance 2 from the given word
        """
        # Of all spelling errors, 99% is covered by edit distance 2.
        # Only keep candidates that are actually known words (20% speedup).
        return set(e2 for e1 in self._edit1(w) for e2 in self._edit1(e1) if e2 in self)

    def _known(self, words=[]):
        """ Returns the given list of words filtered by known words.
        """
        return set(w for w in words if w in self)

    def suggest(self, w):
        """ Return a list of (word, confidence) spelling corrections for the given word,
            based on the probability of known words with edit distance 1-2 from the given word.
        """
--
# -*- coding: utf-8 -*-
'''Default sentiment analyzers are English for backwards compatibility, so
you can still do

>>> from textblob.sentiments import PatternAnalyzer
--
PUNCTUATION_REGEX = re.compile('[{0}]'.format(re.escape(string.punctuation)))


def strip_punc(s, all=False):
    '''Removes punctuation from a string.

    :param s: The string.
--
        return s.strip().strip(string.punctuation)


def lowerstrip(s, all=False):
    '''Makes text all lowercase and strips punctuation and whitespace.

    :param s: The string.
--
    return strip_punc(s.lower().strip(), all=all)


def tree2str(tree, concat=' '):
    '''Convert a nltk.tree.Tree to a string.

    For example:
--
    return concat.join([word for (word, tag) in tree])


def filter_insignificant(chunk, tag_suffixes=('DT', 'CC', 'PRP$', 'PRP')):
    '''Filter out insignificant (word, tag) tuples from a chunk of text.'''
    good = []
    for word, tag in chunk:
--
# -*- coding: utf-8 -*-
'''Default noun phrase extractors are for English to maintain backwards
compatibility, so you can still do

>>> from textblob.np_extractors import ConllExtractor
--
from textblob.compat import PY2, csv
import json

DEFAULT_ENCODING = 'utf-8'

class BaseFormat(object):

--
    :param f: A filename.
    """

    def __init__(self, fname):
        pass

    def to_iterable(self):
        '''Return an iterable object from the data.'''
        raise NotImplementedError("Must implement a 'to_iterable' method.")

    @staticmethod
    def detect(stream):
        '''Detect the file format given a filename.
        Return True if a stream is this file format.
        '''
--

    delimiter = ","

    def __init__(self, fname):
        super(DelimitedFormat, self).__init__(fname)
        with open(fname, 'r') as fp:
            if PY2:
                reader = csv.reader(fp, delimiter=self.delimiter,
                                    encoding=DEFAULT_ENCODING)
            else:
                reader = csv.reader(fp, delimiter=self.delimiter)
            self.data = [row for row in reader]

    def to_iterable(self):
        '''Return an iterable object from the data.'''
        return self.data

    @staticmethod
    def detect(stream):
        '''Return True if stream is valid.'''
        try:
            csv.Sniffer().sniff(stream, delimiters=",")
--
    delimiter = ","

    @staticmethod
    def detect(stream):
        '''Return True if stream is valid CSV.'''
        try:
            csv.Sniffer().sniff(stream, delimiters=",")
--
    delimiter = "\t"

    @staticmethod
    def detect(stream):
        '''Return True if stream is valid CSV.'''
        try:
            csv.Sniffer().sniff(stream, delimiters="\t")
--
        ]
    """

    def __init__(self, fname):
        super(JSON, self).__init__(fname)
        with open(fname, "r") as fp:
            self.dict = json.load(fp)

    def to_iterable(self):
        '''Return an iterable object from the JSON data.'''
        return [(d['text'], d['label']) for d in self.dict]

    @staticmethod
    def detect(stream):
        '''Return True if stream is valid JSON.'''
        try:
            json.loads(stream)
--
    'tsv': TSV
}

def detect(filename, max_read=1024):
    '''Attempt to detect a file's format, trying each of the supported
    formats. Return the format class that was detected. If no format is
    detected, return ``None``.
--
    dataset or model that the user does not have on their system.
    '''

    def __init__(self, message=MISSING_CORPUS_MESSAGE, *args, **kwargs):
        super(MissingCorpusException, self).__init__(message, *args, **kwargs)

class DeprecationError(TextBlobException):
--
### Basic feature extractors ###


def _get_words_from_dataset(dataset):
    '''Return a set of all words in a dataset.

    :param dataset: A list of tuples of the form ``(words, label)`` where
--
    return set(all_words)


def basic_extractor(document, train_set):
    '''A basic document feature extractor that returns a dict indicating
    what words in ``train_set`` are contained in ``document``.

--
    return features


def contains_extractor(document):
    '''A basic document feature extractor that returns a dict of words that
    the document contains.
    '''
--
    .. versionadded:: 0.6.0
    '''

    def __init__(self, train_set, feature_extractor=basic_extractor, format=None):
        self.feature_extractor = feature_extractor
        if isinstance(train_set, basestring):  # train_set is a filename
            self.train_set = self._read_data(train_set, format)
--
            self.train_set = train_set
        self.train_features = None

    def _read_data(self, dataset, format=None):
        '''Reads a data file and returns and iterable that can be used
        as testing or training data.
        '''
--
        return format_class(dataset).to_iterable()

    @cached_property
    def classifier(self):
        '''The classifier object.'''
        raise NotImplementedError('Must implement the "classifier" property.')

    def classify(self, text):
        '''Classifies a string of text.'''
        raise NotImplementedError('Must implement a "classify" method.')

    def train(self, labeled_featureset):
        '''Trains the classifier.'''
        raise NotImplementedError('Must implement a "train" method.')

    def labels(self):
        '''Returns an iterable containing the possible labels.'''
        raise NotImplementedError('Must implement a "labels" method.')

    def extract_features(self, text):
        '''Extracts features from a body of text.

        :rtype: dictionary of features
--

    nltk_class = None  # This must be a class within nltk.classify

    def __init__(self, train_set,
                 feature_extractor=basic_extractor, format=None):
        super(NLTKClassifier, self).__init__(train_set, feature_extractor, format)
        self.train_features = [(self.extract_features(d), c) for d, c in self.train_set]

    def __repr__(self):
        class_name = self.__class__.__name__
        return "<{cls} trained on {n} instances>".format(cls=class_name,
                                                        n=len(self.train_set))

    @cached_property
    def classifier(self):
        '''The classifier.'''
        try:
            return self.train()
        except AttributeError:  # nltk_class has not been defined
            raise ValueError("NLTKClassifier must have a nltk_class"
                            " variable that is not None.")

    def train(self, *args, **kwargs):
        '''Train the classifier with a labeled feature set and return
        the classifier. Takes the same arguments as the wrapped NLTK class.
        This method is implicitly called when calling ``classify`` or
--
            raise ValueError("NLTKClassifier must have a nltk_class"
                            " variable that is not None.")

    def labels(self):
        '''Return an iterable of possible labels.'''
        return self.classifier.labels()

    def classify(self, text):
        '''Classifies the text.

        :param text: A string of text.
--
        text_features = self.extract_features(text)
        return self.classifier.classify(text_features)

    def accuracy(self, test_set, format=None):
        '''Compute the accuracy on a test set.

        :param test_set: A list of tuples of the form ``(text, label)``, or a
--
        test_features = [(self.extract_features(d), c) for d, c in test_data]
        return nltk.classify.accuracy(self.classifier, test_features)

    def update(self, new_data, *args, **kwargs):
        '''Update the classifier with new training data and re-trains the
        classifier.

--
        try:
            self.classifier = self.nltk_class.train(self.train_features,
                                                    *args, **kwargs)
        except AttributeError:  # Descendant has not defined nltk_class
            raise ValueError("NLTKClassifier must have a nltk_class"
                            " variable that is not None.")
        return True
--

    nltk_class = nltk.classify.NaiveBayesClassifier

    def prob_classify(self, text):
        '''Return the label probability distribution for classifying a string
        of text.

--
        text_features = self.extract_features(text)
        return self.classifier.prob_classify(text_features)

    def informative_features(self, *args, **kwargs):
        '''Return the most informative features as a list of tuples of the
        form ``(feature_name, feature_value)``.

--
        '''
        return self.classifier.most_informative_features(*args, **kwargs)

    def show_informative_features(self, *args, **kwargs):
        '''Displays a listing of the most informative features for this
        classifier.

--

    nltk_class = nltk.classify.decisiontree.DecisionTreeClassifier

    def pprint(self, *args, **kwargs):
        '''Return a string containing a pretty-printed version of this decision
        tree. Each line in the string corresponds to a single decision tree node
        or leaf, and indentation is used to display the structure of the tree.
--
        '''
        return self.classifier.pp(*args, **kwargs)

    def pseudocode(self, *args, **kwargs):
        '''Return a string representation of this decision tree that expresses
        the decisions it makes as a nested set of pseudocode if statements.

--

    nltk_class = nltk.classify.PositiveNaiveBayesClassifier

    def __init__(self, positive_set, unlabeled_set,
                feature_extractor=contains_extractor,
                positive_prob_prior=0.5):
        self.feature_extractor = feature_extractor
--
                                    for d in self.unlabeled_set]
        self.positive_prob_prior = positive_prob_prior

    def __repr__(self):
        class_name = self.__class__.__name__
        return "<{cls} trained on {n_pos} labeled and {n_unlabeled} unlabeled instances>"\
                        .format(cls=class_name, n_pos=len(self.positive_set),
                                n_unlabeled=len(self.unlabeled_set))

    def train(self, *args, **kwargs):
        '''Train the classifier with a labeled and unlabeled feature sets and return
        the classifier. Takes the same arguments as the wrapped NLTK class.
        This method is implicitly called when calling ``classify`` or
--
                                                self.positive_prob_prior)
        return self.classifier

    def update(self, new_positive_data=None,
               new_unlabeled_data=None, positive_prob_prior=0.5,
               *args, **kwargs):
        '''Update the classifier with new data and re-trains the
--
for prop in pass_throughs:
    globals()[prop]=getattr(csv, prop)

def _stringify(s, encoding, errors):
    if s is None:
        return ''
    if isinstance(s, unicode):
--
        s=str(s)
    return s

def _stringify_list(l, encoding, errors='strict'):
    try:
        return [_stringify(s, encoding, errors) for s in iter(l)]
    except TypeError as e:
        raise csv.Error(str(e))

def _unicodify(s, encoding):
    if s is None:
        return None
    if isinstance(s, (unicode, int, float)):
--
    >>> row[1] == u'ñ'
    True
    """
    def __init__(self, f, dialect=csv.excel, encoding='utf-8', errors='strict',
                 *args, **kwds):
        self.encoding = encoding
        self.writer = csv.writer(f, dialect, *args, **kwds)
        self.encoding_errors = errors

    def writerow(self, row):
        self.writer.writerow(_stringify_list(row, self.encoding, self.encoding_errors))

    def writerows(self, rows):
        for row in rows:
          self.writerow(row)

    @property
    def dialect(self):
        return self.writer.dialect
writer = UnicodeWriter

class UnicodeReader(object):
    def __init__(self, f, dialect=None, encoding='utf-8', errors='strict',
                 **kwds):
        format_params = ['delimiter', 'doublequote', 'escapechar', 'lineterminator', 'quotechar', 'quoting', 'skipinitialspace']
        if dialect is None:
--
        self.encoding = encoding
        self.encoding_errors = errors

    def next(self):
        row = self.reader.next()
        encoding = self.encoding
        encoding_errors = self.encoding_errors
--
        return [(value if isinstance(value, float_) else
                 unicode_(value, encoding, encoding_errors)) for value in row]

    def __iter__(self):
        return self

    @property
    def dialect(self):
        return self.reader.dialect

    @property
    def line_num(self):
        return self.reader.line_num
reader = UnicodeReader

--
    >>> r.next() == {'a': u'\xc3\xa9', u'ñ':'2', 'r': [u'\xc3\xae']}
    True
    """
    def __init__(self, csvfile, fieldnames, restval='', extrasaction='raise', dialect='excel', encoding='utf-8', errors='strict', *args, **kwds):
        self.encoding = encoding
        csv.DictWriter.__init__(self, csvfile, fieldnames, restval, extrasaction, dialect, *args, **kwds)
        self.writer = UnicodeWriter(csvfile, dialect, encoding=encoding, errors=errors, *args, **kwds)
        self.encoding_errors = errors

    def writeheader(self):
        fieldnames = _stringify_list(self.fieldnames, self.encoding, self.encoding_errors)
        header = dict(zip(self.fieldnames, self.fieldnames))
        self.writerow(header)
--
    >>> print r.next() == {'name': u'Willam ø. Unicoder', 'place': u'éSpandland'}
    True
    """
    def __init__(self, csvfile, fieldnames=None, restkey=None, restval=None,
                 dialect='excel', encoding='utf-8', errors='strict', *args,
                 **kwds):
        if fieldnames is not None:
--
                                   self.fieldnames]
        self.unicode_restkey = _unicodify(restkey, encoding)

    def next(self):
        row = csv.DictReader.next(self)
        result = dict((uni_key, row[str_key]) for (str_key, uni_key) in
                      izip(self.fieldnames, self.unicode_fieldnames))
--
    izip = izip
    import unicodecsv as csv

    def implements_to_string(cls):
        '''Class decorator that renames __str__ to __unicode__ and
        modifies __str__ that returns utf-8.
        '''
--
    implements_to_string = lambda x: x


def with_metaclass(meta, *bases):
    '''Defines a metaclass.

    Creates a dummy class with a dummy metaclass. When subclassed, the dummy
    metaclass is used, which has a constructor that instantiates a
--
    class metaclass(meta):
        __call__ = type.__call__
        __init__ = type.__init__
        def __new__(cls, name, this_bases, d):
            if this_bases is None:
                return type.__new__(cls, name, (), d)
            return meta(name, bases, d)
--
import sys
import json
import string as pystring
from collections import defaultdict

from textblob.packages import nltk
from textblob.decorators import cached_property, requires_nltk_corpus
--

    translator = Translator()

    def __new__(cls, string, pos_tag=None):
        '''Return a new instance of the class. It is necessary to override
        this method in order to handle the extra pos_tag argument in the
        constructor.
        '''
        return super(Word, cls).__new__(cls, string)

    def __init__(self, string, pos_tag=None):
        self.string = string
        self.pos_tag = pos_tag

    def __repr__(self):
        return repr(self.string)

    def __str__(self):
        return self.string

    def singularize(self):
        '''Return the singular version of the word as a string.'''
        return Word(_singularize(self.string))

    def pluralize(self):
        '''Return the plural version of the word as a string.'''
        return Word(_pluralize(self.string))

    def translate(self, from_lang=None, to="en"):
        '''Translate the word to another language using Google's
        Translate API.

--
        return self.translator.translate(self.string,
                                        from_lang=from_lang, to_lang=to)

    def detect_language(self):
        '''Detect the word's language using Google's Translate API.

        .. versionadded:: 0.5.0
        '''
        return self.translator.detect(self.string)

    def spellcheck(self):
        '''Return a list of (word, confidence) tuples of spelling corrections.

        Based on: Peter Norvig, "How to Write a Spelling Corrector"
--
        '''
        return suggest(self.string)

    def correct(self):
        '''Correct the spelling of the word. Returns the word with the highest
        confidence using the spelling corrector.

--

    @cached_property
    @requires_nltk_corpus
    def lemma(self):
        '''Return the lemma for a word using WordNet's morphy function.'''
        lemmatizer = nltk.stem.WordNetLemmatizer()
        return lemmatizer.lemmatize(self.string)

    @cached_property
    def synsets(self):
        '''The list of Synset objects for this Word.

        :rtype: list of Synsets
--
        return self.get_synsets(pos=None)

    @cached_property
    def definitions(self):
        '''The list of definitions for this word. Each definition corresponds
        to a synset.

        .. versionadded:: 0.7.0
        '''
        return self.define(pos=None)

    def get_synsets(self, pos=None):
        '''Return a list of Synset objects for this word.

        :param pos: A part-of-speech tag to filter upon. If ``None``, all
--
        '''
        return _wordnet.synsets(self.string, pos)

    def define(self, pos=None):
        '''Return a list of definitions for this word. Each definition
        corresponds to a synset for this word.

        :param pos: A part-of-speech tag to filter upon. If ``None``, definitions
            for all parts of speech will be loaded.

        .. versionadded:: 0.7.0
        '''
        return [syn.definition for syn in self.get_synsets(pos=pos)]


class WordList(list):

    '''A list-like collection of words.'''

    def __init__(self, collection):
        '''Initialize a WordList. Takes a collection of strings as
        its only argument.
        '''
        self._collection = [Word(w) for w in collection]
        super(WordList, self).__init__(self._collection)

    def __str__(self):
        return str(self._collection)

    def __repr__(self):
        '''Returns a string representation for debugging.'''
        class_name = self.__class__.__name__
        # String representation of words
--
        else:
            return '{cls}({lst})'.format(cls=class_name, lst=strings)

    def __getitem__(self, key):
        '''Returns a string at the given index.'''
        if isinstance(key, slice):
            return self.__class__(self._collection[key])
        else:
            return self._collection[key]

    def __getslice__(self, i, j):
        # This is included for Python 2.* compatibility
        return self.__class__(self._collection[i:j])

    def __iter__(self):
        return iter(self._collection)

    def count(self, strg, case_sensitive=False, *args, **kwargs):
        """Get the count of a word or phrase `s` within this WordList.

        :param strg: The string to count.
--
                    **kwargs)
        return self._collection.count(strg, *args, **kwargs)

    def append(self, obj):
        '''Append an object to end. If the object is a string, appends a
        ``Word`` object.
        '''
--
        else:
            return self._collection.append(obj)

    def extend(self, iterable):
        '''Extend WordList by appending alements from ``iterable``. If an element
        is a string, appends a ``Word`` object.
        '''
--
            for e in iterable]
        return self

    def upper(self):
        '''Return a new WordList with each word upper-cased.'''
        return self.__class__([word.upper() for word in self])

    def lower(self):
        '''Return a new WordList with each word lower-cased.'''
        return self.__class__([word.lower() for word in self])

    def singularize(self):
        '''Return the single version of each word in this WordList.'''
        return self.__class__([word.singularize() for word in self])

    def pluralize(self):
        '''Return the plural version of each word in this WordList.'''
        return self.__class__([word.pluralize() for word in self])

    def lemmatize(self):
        '''Return the lemma of each word in this WordList.'''
        return self.__class__([word.lemma for word in self])


def _validated_param(obj, name, base_class, default, base_class_name=None):
    '''Validates a parameter passed to __init__. Makes sure that obj is
    the correct class. Return obj if it's not None or falls back to default

    :param obj: The object passed in.
    :param name: The name of the parameter.
    :param base_class: The class that obj must inherit from.
    :param default: The default object to fall back upon if obj is None.
    '''
    base_class_name = base_class_name if base_class_name else base_class.__name__
    if obj is not None and not isinstance(obj, base_class):
        raise ValueError("{name} must be an instance of {cls}"
                         .format(name=name, cls=base_class_name))
    return obj or default


def _initialize_models(obj, tokenizer, pos_tagger,
                       np_extractor, analyzer, parser, classifier):
    """Common initialization between BaseBlob and Blobber classes."""
    # tokenizer may be a textblob or an NLTK tokenizer
    obj.tokenizer = _validated_param(tokenizer, "tokenizer",
                                    base_class=(BaseTokenizer, nltk.tokenize.api.TokenizerI),
                                    default=BaseBlob.tokenizer,
                                    base_class_name="BaseTokenizer")
    obj.np_extractor = _validated_param(np_extractor, "np_extractor",
                                        base_class=BaseNPExtractor,
                                        default=BaseBlob.np_extractor)
    obj.pos_tagger = _validated_param(pos_tagger, "pos_tagger",
                                        BaseTagger, BaseBlob.pos_tagger)
    obj.analyzer = _validated_param(analyzer, "analyzer",
--

    :param text: A string.
    :param tokenizer: (optional) A tokenizer instance. If ``None``,
        defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``,
        defaults to :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``,
        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
    :param parser: A parser. If ``None``, defaults to
        :class:`PatternParser <textblob.en.parsers.PatternParser>`.
    :param classifier: A classifier.

--
    analyzer = PatternAnalyzer()
    parser = PatternParser()

    def __init__(self, text, tokenizer=None,
                pos_tagger=None, np_extractor=None, analyzer=None,
                parser=None, classifier=None, clean_html=False):
        if not isinstance(text, basestring):
--
                           parser, classifier)

    @cached_property
    def words(self):
        '''Return a list of word tokens. This excludes punctuation characters.
        If you want to include punctuation characters, access the ``tokens``
        property.
--
        return WordList(WordTokenizer().itokenize(self.raw, include_punc=False))

    @cached_property
    def tokens(self):
        '''Return a list of tokens, using this blob's tokenizer object
        (defaults to :class:`WordTokenizer <textblob.tokenizers.WordTokenizer>`).
        '''
        return WordList(self.tokenizer.tokenize(self.raw))

    def tokenize(self, tokenizer=None):
        '''Return a list of tokens, using ``tokenizer``.

        :param tokenizer: (optional) A tokenizer object. If None, defaults to
            this blob's default tokenizer.
        '''
        t = tokenizer if tokenizer is not None else self.tokenizer
        return WordList(t.tokenize(self.raw))

    def parse(self, parser=None):
        '''Parse the text.

        :param parser: (optional) A parser instance. If ``None``, defaults to
            this blob's default parser.

        .. versionadded:: 0.6.0
        '''
        p = parser if parser is not None else self.parser
        return p.parse(self.raw)

    def classify(self):
        '''Classify the blob using the blob's ``classifier``.'''
        if self.classifier is None:
            raise NameError("This blob has no classifier. Train one first!")
        return self.classifier.classify(self.raw)

    @cached_property
    def sentiment(self):
        '''Return a tuple of form (polarity, subjectivity ) where polarity
        is a float within the range [-1.0, 1.0] and subjectivity is a float
        within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is
--
        return self.analyzer.analyze(self.raw)

    @cached_property
    def polarity(self):
        '''Return the polarity score as a float within the range [-1.0, 1.0]

        :rtype: float
--
        return PatternAnalyzer().analyze(self.raw)[0]

    @cached_property
    def subjectivity(self):
        '''Return the subjectivity score as a float within the range [0.0, 1.0]
        where 0.0 is very objective and 1.0 is very subjective.

--
        return PatternAnalyzer().analyze(self.raw)[1]

    @cached_property
    def noun_phrases(self):
        '''Returns a list of noun phrases for this blob.'''
        return WordList([phrase.strip().lower()
                        for phrase in self.np_extractor.extract(self.raw)
                        if len(phrase) > 1])

    @cached_property
    def pos_tags(self):
        '''Returns an list of tuples of the form (word, POS tag).

        Example:
--
    tags = pos_tags

    @cached_property
    def word_counts(self):
        '''Dictionary of word frequencies in this text.
        '''
        counts = defaultdict(int)
        stripped_words = [lowerstrip(word) for word in self.words]
        for word in stripped_words:
            counts[word] += 1
        return counts

    @cached_property
    def np_counts(self):
        '''Dictionary of noun phrase frequencies in this text.
        '''
        counts = defaultdict(int)
        for phrase in self.noun_phrases:
            counts[phrase] += 1
        return counts

    def ngrams(self, n=3):
        '''Return a list of n-grams (tuples of n successive words) for this
        blob.
        '''
--
                            for i in range(len(self.words) - n + 1)]
        return grams

    def translate(self, from_lang=None, to="en"):
        '''Translate the blob to another language.
        Uses the Google Translate API. Returns a new TextBlob.

--
        return self.__class__(self.translator.translate(self.raw,
                        from_lang=from_lang, to_lang=to))

    def detect_language(self):
        '''Detect the blob's language using the Google Translate API.

        Requires an internet connection.
--
        '''
        return self.translator.detect(self.raw)

    def correct(self):
        '''Attempt to correct the spelling of a blob.

        .. versionadded:: 0.6.0
--
                ret = ' '.join([ret, word])
        return self.__class__(ret)

    def _cmpkey(self):
        '''Key used by ComparableMixin to implement all rich comparison
        operators.
        '''
        return self.raw

    def _strkey(self):
        '''Key used by StringlikeMixin to implement string methods.'''
        return self.raw

    def __hash__(self):
        return hash(self._cmpkey())

    def __add__(self, other):
        '''Concatenates two text objects the same way Python strings are
        concatenated.

--
            raise TypeError('Operands must be either strings or {0} objects'
                .format(self.__class__.__name__))

    def split(self, sep=None, maxsplit=sys.maxsize):
        """Behaves like the built-in str.split() except returns a
        WordList.
        """
--
    containing sentences). Inherits from :class:`BaseBlob <BaseBlob>`.

    :param text: A string.
    :param tokenizer: (optional) A tokenizer instance. If ``None``, defaults to
        :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``, defaults to
        :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``, defaults to
        :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
    :param classifier: (optional) A classifier.
    """

    @cached_property
    def sentences(self):
        '''Return list of :class:`Sentence <Sentence>` objects.'''
        return self._create_sentence_objects()

    @cached_property
    def words(self):
        '''Return a list of word tokens. This excludes punctuation characters.
        If you want to include punctuation characters, access the ``tokens``
        property.
--
        return WordList(words)

    @property
    def raw_sentences(self):
        '''List of strings, the raw sentences in the blob.'''
        return [sentence.raw for sentence in self.sentences]

    @property
    def serialized(self):
        '''Returns a list of each sentence's dict representation.'''
        return [sentence.dict for sentence in self.sentences]

    def to_json(self, *args, **kwargs):
        '''Return a json representation (str) of this blob.
        Takes the same arguments as json.dumps.

--
        return json.dumps(self.serialized, *args, **kwargs)

    @property
    def json(self):
        '''The json representation of this blob.

        .. versionchanged:: 0.5.1
--
        '''
        return self.to_json()

    def _create_sentence_objects(self):
        '''Returns a list of Sentence objects given
        a list of sentence strings. Attempts to handle sentences that
        have more than one punctuation mark at the end of the sentence.
--

    :param sentence: A string, the raw sentence.
    :param start_index: An int, the index where this sentence begins
                        in a TextBlob. If not given, defaults to 0.
    :param end_index: An int, the index where this sentence ends in
                        a TextBlob. If not given, defaults to the
                        length of the sentence - 1.
    '''

    def __init__(self, sentence, start_index=0, end_index=None, *args, **kwargs):
        super(Sentence, self).__init__(sentence, *args, **kwargs)
        self.start = self.start_index = start_index
        self.end = self.end_index = end_index or len(sentence) - 1

    @property
    def dict(self):
        '''The dict representation of this sentence.'''
        return {
            'raw': self.raw,
--
        True

    :param tokenizer: (optional) A tokenizer instance. If ``None``,
        defaults to :class:`WordTokenizer() <textblob.tokenizers.WordTokenizer>`.
    :param np_extractor: (optional) An NPExtractor instance. If ``None``,
        defaults to :class:`FastNPExtractor() <textblob.en.np_extractors.FastNPExtractor>`.
    :param pos_tagger: (optional) A Tagger instance. If ``None``,
        defaults to :class:`PatternTagger <textblob.en.taggers.PatternTagger>`.
    :param analyzer: (optional) A sentiment analyzer. If ``None``,
        defaults to :class:`PatternAnalyzer <textblob.en.sentiments.PatternAnalyzer>`.
    :param parser: A parser. If ``None``, defaults to
        :class:`PatternParser <textblob.en.parsers.PatternParser>`.
    :param classifier: A classifier.

--
    analyzer = PatternAnalyzer()
    parser = PatternParser()

    def __init__(self, tokenizer=None, pos_tagger=None, np_extractor=None,
                analyzer=None, parser=None, classifier=None):
        _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,
                            parser, classifier)

    def __call__(self, text):
        '''Return a new TextBlob object with this Blobber's ``np_extractor``,
        ``pos_tagger``, ``tokenizer``, ``analyzer``, and ``classifier``.

--
                        parser=self.parser,
                        classifier=self.classifier)

    def __repr__(self):
        classifier_name = self.classifier.__class__.__name__ + "()" if self.classifier else "None"
        return ("Blobber(tokenizer={0}(), pos_tagger={1}(), "
                    "np_extractor={2}(), analyzer={3}(), parser={4}(), classifier={5})")\
--
# -*- coding: utf-8 -*-
'''Make word inflection default to English. This allows for backwards
compatibility so you can still import text.inflect.

    >>> from textblob.inflect import singularize
--

#--- ENGLISH PARSER --------------------------------------------------------------------------------

def find_lemmata(tokens):
    """ Annotates the tokens with lemmata for plural nouns and conjugated verbs,
        where each token is a [word, part-of-speech] list.
    """
--

class Parser(_Parser):

    def find_lemmata(self, tokens, **kwargs):
        return find_lemmata(tokens)

    def find_tags(self, tokens, **kwargs):
        if kwargs.get("tagset") in (PENN, None):
            kwargs.setdefault("map", lambda token, tag: (token, tag))
        if kwargs.get("tagset") == UNIVERSAL:
            kwargs.setdefault("map", lambda token, tag: penntreebank2universal(token, tag))
        return _Parser.find_tags(self, tokens, **kwargs)

class Sentiment(_Sentiment):

    def load(self, path=None):
        _Sentiment.load(self, path)
        # Map "terrible" to adverb "terribly" (+1% accuracy)
        if not path:
--
)
parser = Parser(
     lexicon = lexicon,
     default = ("NN", "NNP", "CD"),
    language = "en"
)

--
)


def tokenize(s, *args, **kwargs):
    """ Returns a list of sentences, where punctuation marks have been split from words.
    """
    return parser.find_tokens(text_type(s), *args, **kwargs)

def parse(s, *args, **kwargs):
    """ Returns a tagged Unicode string.
    """
    return parser.parse(unicode(s), *args, **kwargs)

def parsetree(s, *args, **kwargs):
    """ Returns a parsed Text from the given string.
    """
    return Text(parse(unicode(s), *args, **kwargs))

def split(s, token=[WORD, POS, CHUNK, PNP]):
    """ Returns a parsed Text from the given parsed string.
    """
    return Text(text_type(s), token)

def tag(s, tokenize=True, encoding="utf-8"):
    """ Returns a list of (token, tag)-tuples from the given string.
    """
    tags = []
--
            tags.append((token[0], token[1]))
    return tags

def suggest(w):
    """ Returns a list of (word, confidence)-tuples of spelling corrections.
    """
    return spelling.suggest(w)

def polarity(s, **kwargs):
    """ Returns the sentence polarity (positive/negative) between -1.0 and 1.0.
    """
    return sentiment(unicode(s), **kwargs)[0]

def subjectivity(s, **kwargs):
    """ Returns the sentence subjectivity (objective/subjective) between 0.0 and 1.0.
    """
    return sentiment(unicode(s), **kwargs)[1]

def positive(s, threshold=0.1, **kwargs):
    """ Returns True if the given sentence has a positive sentiment (polarity >= threshold).
    """
    return polarity(unicode(s), **kwargs) >= threshold
--
    http://www.clips.ua.ac.be/pages/pattern-en#parser
    '''

    def parse(self, text):
        '''Parses the text.'''
        return pattern_parse(text)
--

    kind = CONTINUOUS

    def analyze(self, text):
        """Return the sentiment as a tuple of the form:
        ``(polarity, subjectivity)``
        """
--

    kind = DISCRETE

    def __init__(self):
        super(NaiveBayesAnalyzer, self).__init__()
        self._classifier = None

    @requires_nltk_corpus
    def train(self):
        '''Train the Naive Bayes classifier on the movie review corpus.'''
        super(NaiveBayesAnalyzer, self).train()
        neg_ids = nltk.corpus.movie_reviews.fileids('neg')
--
        train_data = neg_feats + pos_feats
        self._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)

    def _extract_feats(self, words):
        return dict([(word, True) for word in words])

    def analyze(self, text):
        """Return the sentiment as a tuple of the form:
        ``(classification, pos_probability, neg_probability)``
        """
--

class ChunkParser(nltk.ChunkParserI):

    def __init__(self):
        self._trained = False

    @requires_nltk_corpus
    def train(self):
        '''Train the Chunker on the ConLL-2000 corpus.'''
        train_data = [[(t, c) for _, t, c in nltk.chunk.tree2conlltags(sent)]
                      for sent in
--
        self.tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)
        self._trained = True

    def parse(self, sentence):
        '''Return the parse tree for the sentence.'''
        if not self._trained:
            self.train()
--
    # POS suffixes that will be ignored
    INSIGNIFICANT_SUFFIXES = ['DT', 'CC', 'PRP$', 'PRP']

    def __init__(self, parser=None):
        self.parser = ChunkParser() if not parser else parser

    def extract(self, text):
        '''Return a list of noun phrases (strings) for body of text.'''
        sentences = nltk.tokenize.sent_tokenize(text)
        noun_phrases = []
--
            noun_phrases.extend(nps)
        return noun_phrases

    def _parse_sentence(self, sentence):
        '''Tag and parse a sentence (a plain, untagged string).'''
        tagged = self.POS_TAGGER.tag(sentence)
        return self.parser.parse(tagged)
--
        ('JJ', 'NN'): 'NNI',
        }

    def __init__(self):
        self._trained = False

    @requires_nltk_corpus
    def train(self):
        train_data = nltk.corpus.brown.tagged_sents(categories='news')
        regexp_tagger = nltk.RegexpTagger([
            (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),
--
        return None


    def _tokenize_sentence(self, sentence):
        '''Split the sentence into single words/tokens'''
        tokens = nltk.word_tokenize(sentence)
        return tokens

    def extract(self, sentence):
        '''Return a list of noun phrases (strings) for body of text.'''
        if not self._trained:
            self.train()
--

### Utility methods ###

def _normalize_tags(chunk):
    '''Normalize the corpus tags.
    ("NN", "NN-PL", "NNS") -> "NN"
    '''
--
    return ret


def _is_match(tagged_phrase, cfg):
    '''Return whether or not a tagged phrases matches a context-free grammar.
    '''
    copy = list(tagged_phrase)  # A copy of the list
--
# Each rule consists of:
# suffix, inflection, category and classic flag.
plural_rules = [
    # 0) Indefinite articles and demonstratives.
    [["^a$|^an$", "some", None, False],
     ["^this$", "these", None, False],
     ["^that$", "those", None, False],
--
        "adjutant", "brigadier", "lieutenant", "major", "quartermaster"],
}

def pluralize(word, pos=NOUN, custom={}, classical=True):
    """ Returns the plural of a given word.
        For example: child -> children.
        Handles nouns and adjectives, using classical inflection by default
        (e.g. where "matrix" pluralizes to "matrices" instead of "matrixes").
        The custom dictionary is for user-defined replacements.
    """

    if word in custom:
--
            "our": "my",
}

def singularize(word, pos=NOUN, custom={}):

    if word in list(custom.keys()):
        return custom[word]
--
import random
import logging
import os.path
from collections import defaultdict
import pickle

import textblob
--
    (http://www.clips.ua.ac.be/pattern).
    '''

    def tag(self, sentence, tokenize=True):
        '''Tag a string `sentence`.'''
        return pattern_tag(sentence, tokenize)

--
    '''

    @requires_nltk_corpus
    def tag(self, sentence, tokenize=True):
        '''Tag a string `sentence`.'''
        if tokenize:
            sentence = nltk.tokenize.word_tokenize(sentence)
--
        Install the ``textblob-aptagger`` extension instead.
    '''

    def __init__(self, load=True):
        raise DeprecationError("PerceptronTagger is deprecated. Use "
                        " the textblob-aptagger extension instead")

    def tag(self, sentence, tokenize=True):
        pass
--
'''Default taggers to the English taggers for backwards incompatiblity, so you
can still do

>>> from textblob.taggers import NLTKTagger
--
    Credit to Marcel Hellkamp, author of bottle.py.
    '''

    def __init__(self, func):
        self.__doc__ = getattr(func, '__doc__')
        self.func = func

    def __get__(self, obj, cls):
        if obj is None:
            return self
        value = obj.__dict__[self.func.__name__] = self.func(obj)
        return value


def requires_nltk_corpus(func):
    '''Wraps a function that requires an NLTK corpus. If the corpus isn't found,
    raise a MissingCorpusException.
    '''
    def decorated(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except LookupError as err:
--
    'wordnet' # Required for lemmatization and Wordnet
]

def main():
    for each in REQUIRED_CORPORA:
        print('Downloading "{0}"'.format(each))
        nltk.download(each)
--
import shutil
import re

def usage():
    print """
Usage:
    %s <work_dir> <search_regex> <replace_with> <glob_pattern> [backup]
    """ % (os.path.basename(sys.argv[0]))

def find_replace(directory, search_pattern, replacement, glob_pattern, create_backup=False):
    for path, dirs, files in os.walk(os.path.abspath(directory)):
        for filename in fnmatch.filter(files, glob_pattern):
            pardir = os.path.normpath(os.path.join(path, '..'))
--
                     'what', 'which', 'who', 'whom','when', 'where', 'why', 'how',
                     # demonstrative pronouns
                     'this', 'that', 'these', 'those',
                     # definite and indefinite article
                     'a', 'an', 'the',
                     # coordinating conjunctions
                     'and', 'but','or', 'so', 'yet', 'nor',
--
import os
essays = [[line.strip() for line in open(os.path.join("/home/ahmed/alltxt/02whole.txt")).readlines() if len(line.strip()) > 1] for essay in range(1, 21)]

def main():
    print essays
if __name__ == '__main__':
    main()
--

class NPExtractor(object):

    def __init__(self, sentence):
        self.sentence = sentence

    # Split the sentence into singlw words/tokens
    def tokenize_sentence(self, sentence):
        tokens = nltk.word_tokenize(sentence)
        return tokens

    # Normalize brown corpus' tags ("NN", "NN-PL", "NNS" > "NN")
    def normalize_tags(self, tagged):
        n_tagged = []
        for t in tagged:
            if t[1] == "NP-TL" or t[1] == "NP":
--
        return n_tagged

    # Extract the main topics from the sentence
    def extract(self):

        tokens = self.tokenize_sentence(self.sentence)
        tags = self.normalize_tags(bigram_tagger.tag(tokens))
--


# Main method, just run "python np_extractor.py"
def main():

    sentence = "Swayy is a beautiful new dashboard for discovering and curating online content."
    np_extractor = NPExtractor(sentence)
--
class SummaryTool(object):

    # Naive method for splitting a text into sentences
    def split_content_to_sentences(self, content):
        content = content.replace("\n", ". ")
        return content.split(". ")

    # Naive method for splitting a text into paragraphs
    def split_content_to_paragraphs(self, content):
        return content.split("\n\n")

    # Caculate the intersection between 2 sentences
    def sentences_intersection(self, sent1, sent2):

        # split the sentence into words/tokens
        s1 = set(sent1.split(" "))
--

    # Format a sentence - remove all non-alphbetic chars from the sentence
    # We'll use the formatted sentence as a key in our sentences dictionary
    def format_sentence(self, sentence):
        sentence = re.sub(r'\W+', '', sentence)
        return sentence

    # Convert the content into a dictionary <K, V>
    # k = The formatted sentence
    # V = The rank of the sentence
    def get_senteces_ranks(self, content):

        # Split the content into sentences
        sentences = self.split_content_to_sentences(content)
--
        return sentences_dic

    # Return the best sentence in a paragraph
    def get_best_sentence(self, paragraph, sentences_dic):

        # Split the paragraph into sentences
        sentences = self.split_content_to_sentences(paragraph)
--
        return best_sentence

    # Build the summary
    def get_summary(self, title, content, sentences_dic):

        # Split the content into paragraphs
        paragraphs = self.split_content_to_paragraphs(content)
--


# Main method, just run "python summary_tool.py"
def main():

    # Demo
    # Content from: "http://thenextweb.com/apps/2013/03/21/swayy-discover-curate-content/"
--
import os
###############

def regexp_answer(strQuestion, listQuestion, articleFile):
	"""Takes a question list tagged and tokenized and attempts to return a true/false value for easy questions."""
	
	fullStr = ""
--
def _should_include_path(path, includes, excludes):
    """Return True iff the given path should be included."""
    from os.path import basename
    from fnmatch import fnmatch
--
            return False
    return True

def _walk(top, topdown=True, onerror=None, follow_symlinks=False):
    """A version of `os.walk()` with a couple differences regarding symlinks.

    1. follow_symlinks=False (the default): A symlink to a dir is
       returned as a *non*-dir. In `os.walk()`, a symlink to a dir is
       returned in the *dirs* list, but it is not recursed into.
    2. follow_symlinks=True: A symlink to a dir is returned in the
--
        yield top, dirs, nondirs

_NOT_SPECIFIED = ("NOT", "SPECIFIED")
def _paths_from_path_patterns(path_patterns, files=True, dirs="never",
                              recursive=True, includes=None, excludes=None,
                              skip_dupe_dirs=False,
                              follow_symlinks=False,
--

        "path_patterns" is a list of paths optionally using the '*', '?' and
            '[seq]' glob patterns.
        "files" is boolean (default True) indicating if file paths
            should be yielded
        "dirs" is string indicating under what conditions dirs are
            yielded. It must be one of:
              never             (default) never yield dirs
              always            yield all dirs matching given patterns
              if-not-recursive  only yield dirs for invocations when
                                recursive=False
            See use cases below for more details.
        "recursive" is boolean (default True) indicating if paths should
            be recursively yielded under given dirs.
        "includes" is a list of file patterns to include in recursive
            searches.
--
            descending into a dir that has already been yielded. Note
            that this currently does not dereference symlinks.
        "follow_symlinks" is a boolean indicating whether to follow
            symlinks (default False). To guard against infinite loops
            with circular dir symlinks, only dir symlinks to *deeper*
            dirs are followed.
        "on_error" is an error callback called when a given path pattern
            matches nothing:
                on_error(PATH_PATTERN)
            If not specified, the default is look for a "log" global and
            call:
                log.error("`%s': No such file or directory")
            Specify None to do nothing.
--
from curses import *

@wrapper
def main(win):
    def print_all_colors(attr):
        for c in range(0, curses.COLORS):
            init_pair(c, c, -1)
            win.addstr(str(c) + ' ', color_pair(c) | attr)
    use_default_colors()
    win.addstr("available colors: %d\n\n" % curses.COLORS)
    print_all_colors(0)
    win.addstr("\n\n")
--
    print("Policy rules")
    print("============")
    print("")
    for Def in dir(policy_rules):
      Fun = getattr(policy_rules, Def)
      if callable(Fun) and Def.startswith("policy_"):
        print("%s: %s" % (Def, getattr(Fun, "__doc__")))
    sys.exit(0)

  try:
--
    sys.exit(-1)

  Founds = []
  for Def in dir(policy_rules):
    Fun = getattr(policy_rules, Def)
    if callable(Fun) and Def.startswith("policy_"):
      for RegExp in Fun():
        for Match in RegExp.finditer(FileContent):
          Line = FileContent.count("\n", 0, Match.start(0)) + 1
          Policy = Def.replace("policy_", "").replace("_", " ").capitalize()
          Pattern = RegExp.pattern
          Text = Match.group(0).replace(os.linesep, " ")
          Founds.append((Line, Policy, Pattern, Text))
--

  doskip = False
  while True:
    nd = raw_input("how many top %s (up to %d) to download for %s? [empty default = all]: " % (desc, maxn, title))
    if nd=="": ndi = maxn
    else: ndi = int(nd)
    if ndi==0:
--
from HTMLParser import HTMLParser

class MLStripper(HTMLParser):
    def __init__(self):
        self.reset()
        self.fed = []
    def handle_data(self, d):
        self.fed.append(d)
    def get_data(self):
        return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    return s.get_data()

def fix_docs(docs):
    '''
    Processes a bunch of docs by stripping HTML, and replacing URLS etc...
    '''
--
singluar_prop_nouns = ('he', 'she', 'i', 'him', 'me', 'myself', 'it')


def check_node_agreement(tree_one, tree_two):

    # First determine which node is the noun node
    if tree_one.node in noun_tags and tree_two.node in noun_tags:
--
        return False


def select_best_noun_verb(tree_one, tree_two):
    # Sometimes neither node directly pointed to in the nsubj
    # dependency information is a verb, in which case we need to
    # go searching for a "nearby verb"
--
    return noun_tree, verb_tree


def find_commanding_verb_tree(tree, steps=0):
    log("looking for verb at root: %s" % (tree.node,), 3)
    if tree.node in verb_tags:
        return (tree, steps)
--
            return find_commanding_verb_tree(parent_node, steps + 1)


def is_pronoun_first_person(tree):
    prop_noun = tree.leaves()[0].lower()
    return prop_noun == "i"


def is_pronoun_singluar(tree):
    prop_noun = tree.leaves()[0].lower()
    return prop_noun in singluar_prop_nouns


def is_sentence_root(tree):
    if not tree.node in tree_utils.semi_tree_roots:
        return False
    else:
--
        return "NP" in child_nodes and "VP" in child_nodes


def shallowest_noun_in_tree(tree):
    tree.subtrees(lambda x: x.node == "NN" or x.node == "NNS")


def node_in_tree(tree, value):
    subtrees = list(tree.subtrees(lambda x: value in x))
    return subtrees[0] if len(subtrees) > 0 else None


def parse(text, use_cache=True):
    num_agrees = 0
    num_not_agrees = 0
    num_unsure = 0
--
start_pers_pro_weight = 1000


def _possible_sentences_in_line(line, min_sentence_len=3):
    # The simplest thing here is to defer to the paper.  If it looks like they've
    # added punctuation already, lets just use that
    has_abbr = sum([1 if len(word) > 0 and word[-1] == "." and word.count(".") > 1 else 0 for word in line.split(" ")])
    if not has_abbr and line.count(". ") > 0:
--
    return sentences


def is_possible_sentence(tree):
    """Perform some basic filtering to remove unlikely constructs, like
    starting a setnence with because"""
    leaf_trees = tree.subtrees(lambda x: x.height() == 2)
--
            return False


def boost_for_sentence_tree(tree):
    weight = 1

    first_np = list(tree.subtrees(lambda x: x.node == "NP"))[0]
--
    return weight


def prod(nums):
    total = 1
    for num in nums:
        total *= num
    return total


def contains_any_invalid_setences(sentences, invalid_sentences):
    for sentence in sentences:
        if sentence in invalid_sentences:
            return True
    return False


def parse(text, use_cache=True, include_prob=False):
    lines = text.split("\n")
    sentences = []
    for line in lines:
--
    return sentences


def parse_sentences(line, use_cache=True, include_prob=False):

    log("Working on: %s" % (line,), 2)

--
import re
from PorterStemmer import PorterStemmer

def cleantext(text, ps):
	text = text.replace('mr .', 'mr')
	text = text.replace('u . s .', 'us')
	sentences = text.split()
--
		else: text = text + " " + ps.stem( word )
	return text

def process2( nameFileIn, nameFileOut):
 ps = PorterStemmer() # It initializes stemmer
 reader = open(nameFileIn, 'r' )
 fi = open( nameFileOut , "w")
--
import os

def read_file(name):
  print 'printing contents of file ' + name
  f = None
  #We use try...finally to ensure that the file is closed even if there
--
    if f != None:
      f.close()

def dir_list(dir_name, subdir, *args):
    '''Return a list of file names in directory 'dir_name'
    If 'subdir' is True, recursively access subdirectories under 'dir_name'.
    Additional arguments, if any, are file extensions to add to the list.
--
            fileList += dir_list(dirfile, subdir, *args)
    return fileList

def combine_files(fileList, fn):
    f = open(fn, 'w')
    for file in fileList:
        print 'Writing file %s' % file
--
stext = clipboard.get_selection()


def getwords(text, splitchars=' \t|!?.;:,"_'):
    words_iter = re.finditer("([%s]+)" % "".join([("^" + c) for c in splitchars]),text)
    for word in words_iter:
        yield word.group()
--

#s = ','.join(wctext)

retCode, choice = dialog.list_menu_multi(options, title="Choose one or more values", message="Choose one or more values", defaults=[])

time.sleep(1.0)

--
python << EOL
import vim, StringIO,sys
def PyExecAppend(line1,line2):
  r = vim.current.buffer.range(int(line1),int(line2))
  redirected = StringIO.StringIO()
  sys.stdout = redirected
--
  r.append(output[:-1]) # the -1 is to remove the final blank line
  redirected.close()

def PyExecReplace(line1,line2):
  r = vim.current.buffer.range(int(line1),int(line2))
  redirected = StringIO.StringIO()
  sys.stdout = redirected
--
import sys
import optparse

def process_command_line(argv):
    """
    Return a 2-tuple: (settings object, args list).
    `argv` is a list of arguments, or `None` for ``sys.argv[1:]``.
--
        formatter=optparse.TitledHelpFormatter(width=78),
        add_help_option=None)

    # define options here:
    parser.add_option(      # customized description; put --help last
        '-h', '--help', action='help',
        help='Show this help message and exit.')
--

    return settings, args

def main(argv=None):
    settings, args = process_command_line(argv)
    # application code here, like:
    # run(settings, args)
